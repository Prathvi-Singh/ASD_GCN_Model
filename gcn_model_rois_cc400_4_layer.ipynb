{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5d23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from nilearn import connectome\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "# Reading and computing the input data\n",
    "\n",
    "# Selected pipeline\n",
    "pipeline = 'cpac'\n",
    "\n",
    "# Input data variables\n",
    "root_folder = '../ABIDE/'\n",
    "data_folder = os.path.join(root_folder, 'ABIDE_pcp/cpac/filt_noglobal')\n",
    "phenotype = os.path.join(root_folder, 'ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv')\n",
    "\n",
    "\n",
    "def fetch_filenames(subject_IDs, file_type):\n",
    "\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        file_type    : must be one of the available file types\n",
    "\n",
    "    returns:\n",
    "\n",
    "        filenames    : list of filetypes (same length as subject_list)\n",
    "    \"\"\"\n",
    "\n",
    "    import glob\n",
    "\n",
    "    # Specify file mappings for the possible file types\n",
    "    filemapping = {'func_preproc': '_func_preproc.nii.gz',\n",
    "                   'rois_ho': '_rois_ho.1D'}\n",
    "\n",
    "    # The list to be filled\n",
    "    filenames = []\n",
    "\n",
    "    # Fill list with requested file paths\n",
    "    for i in range(len(subject_IDs)):\n",
    "        os.chdir(data_folder)  # os.path.join(data_folder, subject_IDs[i]))\n",
    "        try:\n",
    "            filenames.append(glob.glob('*' + subject_IDs[i] + filemapping[file_type])[0])\n",
    "        except IndexError:\n",
    "            # Return N/A if subject ID is not found\n",
    "            filenames.append('N/A')\n",
    "\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# Get timeseries arrays for list of subjects\n",
    "def get_timeseries(subject_list, atlas_name):\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        atlas_name   : the atlas based on which the timeseries are generated e.g. aal, cc200\n",
    "\n",
    "    returns:\n",
    "        time_series  : list of timeseries arrays, each of shape (timepoints x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    timeseries = []\n",
    "    for i in range(len(subject_list)):\n",
    "        subject_folder = os.path.join(data_folder, subject_list[i])\n",
    "        ro_file = [f for f in os.listdir(subject_folder) if f.endswith('_rois_' + atlas_name + '.1D')]\n",
    "        fl = os.path.join(subject_folder, ro_file[0])\n",
    "        print(\"Reading timeseries file %s\" %fl)\n",
    "        timeseries.append(np.loadtxt(fl, skiprows=0))\n",
    "\n",
    "    return timeseries\n",
    "\n",
    "\n",
    "# Compute connectivity matrices\n",
    "def subject_connectivity(timeseries, subject, atlas_name, kind, save=True, save_path=data_folder):\n",
    "    \"\"\"\n",
    "        timeseries   : timeseries table for subject (timepoints x regions)\n",
    "        subject      : the subject ID\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        save         : save the connectivity matrix to a file\n",
    "        save_path    : specify path to save the matrix if different from subject folder\n",
    "\n",
    "    returns:\n",
    "        connectivity : connectivity matrix (regions x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Estimating %s matrix for subject %s\" % (kind, subject))\n",
    "\n",
    "    if kind in ['tangent', 'partial correlation', 'correlation']:\n",
    "        conn_measure = connectome.ConnectivityMeasure(kind=kind)\n",
    "        connectivity = conn_measure.fit_transform([timeseries])[0]\n",
    "\n",
    "    if save:\n",
    "        subject_file = os.path.join(save_path, subject,\n",
    "                                    subject + '_' + atlas_name + '_' + kind.replace(' ', '_') + '.mat')\n",
    "        sio.savemat(subject_file, {'connectivity': connectivity})\n",
    "\n",
    "    return connectivity\n",
    "\n",
    "\n",
    "# Get the list of subject IDs\n",
    "def get_ids(num_subjects=None):\n",
    "    \"\"\"\n",
    "\n",
    "    return:\n",
    "        subject_IDs    : list of all subject IDs\n",
    "    \"\"\"\n",
    "\n",
    "    subject_IDs = np.genfromtxt(os.path.join(data_folder, 'subject_IDs.txt'), dtype=str)\n",
    "\n",
    "    if num_subjects is not None:\n",
    "        subject_IDs = subject_IDs[:num_subjects]\n",
    "\n",
    "    return subject_IDs\n",
    "\n",
    "\n",
    "# Get phenotype values for a list of subjects\n",
    "def get_subject_score(subject_list, score):\n",
    "    scores_dict = {}\n",
    "\n",
    "    with open(phenotype) as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            if row['SUB_ID'] in subject_list:\n",
    "                scores_dict[row['SUB_ID']] = row[score]\n",
    "\n",
    "    return scores_dict\n",
    "\n",
    "\n",
    "# Dimensionality reduction step for the feature vector using a ridge classifier\n",
    "def feature_selection(matrix, labels, train_ind, fnum):\n",
    "    \"\"\"\n",
    "        matrix       : feature matrix (num_subjects x num_features)\n",
    "        labels       : ground truth labels (num_subjects x 1)\n",
    "        train_ind    : indices of the training samples\n",
    "        fnum         : size of the feature vector after feature selection\n",
    "\n",
    "    return:\n",
    "        x_data      : feature matrix of lower dimension (num_subjects x fnum)\n",
    "    \"\"\"\n",
    "\n",
    "    estimator = RidgeClassifier()\n",
    "    selector = RFE(estimator, n_features_to_select=fnum, step=100, verbose=1)\n",
    "\n",
    "    featureX = matrix[train_ind, :]\n",
    "    featureY = labels[train_ind]\n",
    "    selector = selector.fit(featureX, featureY.ravel())\n",
    "    x_data = selector.transform(matrix)\n",
    "\n",
    "    print(\"Number of labeled samples %d\" % len(train_ind))\n",
    "    print(\"Number of features selected %d\" % x_data.shape[1])\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "# Make sure each site is represented in the training set when selecting a subset of the training set\n",
    "def site_percentage(train_ind, perc, subject_list):\n",
    "    \"\"\"\n",
    "        train_ind    : indices of the training samples\n",
    "        perc         : percentage of training set used\n",
    "        subject_list : list of subject IDs\n",
    "\n",
    "    return:\n",
    "        labeled_indices      : indices of the subset of training samples\n",
    "    \"\"\"\n",
    "\n",
    "    train_list = subject_list[train_ind]\n",
    "    sites = get_subject_score(train_list, score='SITE_ID')\n",
    "    unique = np.unique(list(sites.values())).tolist()\n",
    "    site = np.array([unique.index(sites[train_list[x]]) for x in range(len(train_list))])\n",
    "\n",
    "    labeled_indices = []\n",
    "\n",
    "    for i in np.unique(site):\n",
    "        id_in_site = np.argwhere(site == i).flatten()\n",
    "\n",
    "        num_nodes = len(id_in_site)\n",
    "        labeled_num = int(round(perc * num_nodes))\n",
    "        labeled_indices.extend(train_ind[id_in_site[:labeled_num]])\n",
    "\n",
    "    return labeled_indices\n",
    "\n",
    "\n",
    "# Load precomputed fMRI connectivity networks\n",
    "def get_networks(subject_list, kind, atlas_name=\"aal\", variable='connectivity'):\n",
    "    \"\"\"\n",
    "        subject_list : list of subject IDs\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        variable     : variable name in the .mat file that has been used to save the precomputed networks\n",
    "\n",
    "\n",
    "    return:\n",
    "        matrix      : feature matrix of connectivity networks (num_subjects x network_size)\n",
    "    \"\"\"\n",
    "\n",
    "    all_networks = []\n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_cc400_2_a/matrix_rois_cc400_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_cc400_2_a/matrix_rois_cc400_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks.append(matrix)\n",
    "            \n",
    "            \n",
    "    # all_networks=np.array(all_networks)\n",
    "\n",
    "    idx = np.triu_indices_from(all_networks[0], 1)\n",
    "    norm_networks = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks]\n",
    "    vec_networks = [mat[idx] for mat in norm_networks]\n",
    "    matrix = np.vstack(vec_networks)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# Construct the adjacency matrix of the population from phenotypic scores\n",
    "def create_affinity_graph_from_scores(scores, pd_dict):\n",
    "    num_nodes = len(pd_dict[scores[0]]) \n",
    "    graph = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for l in scores:\n",
    "        label_dict = pd_dict[l]\n",
    "\n",
    "        if l in ['AGE_AT_SCAN', 'FIQ']:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    try:\n",
    "                        val = abs(float(label_dict[k]) - float(label_dict[j]))\n",
    "                        if val < 2:\n",
    "                            graph[k, j] += 1\n",
    "                            graph[j, k] += 1\n",
    "                    except ValueError:  # missing label\n",
    "                        pass\n",
    "\n",
    "        else:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    if label_dict[k] == label_dict[j]:\n",
    "                        graph[k, j] += 1\n",
    "                        graph[j, k] += 1\n",
    "\n",
    "    return graph\n",
    "\n",
    "def get_static_affinity_adj(features, pd_dict):\n",
    "    pd_affinity = create_affinity_graph_from_scores(['SEX', 'SITE_ID'], pd_dict) \n",
    "    distv = distance.pdist(features, metric='correlation') \n",
    "    dist = distance.squareform(distv)  \n",
    "    sigma = np.mean(dist)\n",
    "    feature_sim = np.exp(- dist ** 2 / (2 * sigma ** 2))\n",
    "    adj = pd_affinity * feature_sim  \n",
    "\n",
    "    return adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e2f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_3116\\3838381434.py:8: DeprecationWarning: Please use `eigsh` from the `scipy.sparse.linalg` namespace, the `scipy.sparse.linalg.eigen` namespace is deprecated.\n",
      "  from scipy.sparse.linalg.eigen import eigsh\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.spatial import distance\n",
    "from scipy.sparse.linalg.eigen import eigsh\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def get_train_test_masks(labels, idx_train, idx_val, idx_test):\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def load_data(subject_IDs, params): \n",
    "    \n",
    "    # labels\n",
    "    num_classes = 2\n",
    "    num_nodes = len(subject_IDs)\n",
    "    \n",
    "    # , y\n",
    "    y_data = np.zeros([num_nodes, num_classes])\n",
    "    y = np.zeros([num_nodes, 1])\n",
    "    \n",
    "    labels = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "    features = get_networks(subject_IDs, kind=params['connectivity'], atlas_name=params['atlas'])\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        y_data[i, int(labels[subject_IDs[i]]) - 1] = 1 # (871,2)\n",
    "        y[i] = int(labels[subject_IDs[i]]) # (871,)\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    cv_splits = list(skf.split(features, np.squeeze(y)))\n",
    "    train = cv_splits[params['folds']][0]\n",
    "    test = cv_splits[params['folds']][1]\n",
    "    val = test\n",
    "    \n",
    "    print('Number of train sample:{}' .format(len(train)))\n",
    "        \n",
    "    y_train, y_val, y_test, train_mask, val_mask, test_mask = get_train_test_masks(y_data, train, val, test)\n",
    "    \n",
    "    y_data = torch.LongTensor(np.where(y_data)[1])\n",
    "    y = torch.LongTensor(y)\n",
    "    y_train = torch.LongTensor(y_train[1])\n",
    "    y_val = torch.LongTensor(y_val[1])\n",
    "    y_test = torch.LongTensor(y_test[1])\n",
    "    \n",
    "    train = torch.LongTensor(train)\n",
    "    val = torch.LongTensor(val)\n",
    "    test = torch.LongTensor(test)\n",
    "    train_mask = torch.LongTensor(train_mask)\n",
    "    val_mask = torch.LongTensor(val_mask)\n",
    "    test_mask = torch.LongTensor(test_mask)\n",
    "    \n",
    "    # Eigenvector\n",
    "    labeled_ind = site_percentage(train, params['num_training'], subject_IDs)\n",
    "    x_data = feature_selection(features, y, labeled_ind, params['num_features'])\n",
    "    features = preprocess_features(sp.coo_matrix(x_data).tolil())\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    \n",
    "    # Adjacency matrix\n",
    "    graph = create_affinity_graph_from_scores(['SEX', 'SITE_ID'], subject_IDs)\n",
    "    distv = distance.pdist(x_data, metric='correlation')\n",
    "    dist = distance.squareform(distv)\n",
    "    sigma = np.mean(dist)\n",
    "    sparse_graph = np.exp(- dist ** 2 / (2 * sigma ** 2))\n",
    "    final_graph = graph * sparse_graph\n",
    "\n",
    "    return final_graph, features, y, y_data, y_train, y_val, y_test, train, val, test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        coords = torch.from_numpy(coords)\n",
    "        values = torch.from_numpy(values)\n",
    "        shape = torch.tensor(shape)\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return adj_normalized\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(sp.eye(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return t_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7980a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# from utils import preprocess_features\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "class dataloader():\n",
    "    def __init__(self): \n",
    "        self.pd_dict = {}\n",
    "        self.node_ftr_dim = 2000\n",
    "        self.num_classes = 2 \n",
    "\n",
    "    def load_data(self, params, connectivity='correlation', atlas='cc400'):\n",
    "        ''' load multimodal data from ABIDE\n",
    "        return: imaging features (raw), labels, non-image data\n",
    "        '''\n",
    "        subject_IDs = get_ids()\n",
    "        labels = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "        num_nodes = len(subject_IDs)\n",
    "\n",
    "        sites = get_subject_score(subject_IDs, score='SITE_ID')\n",
    "        unique = np.unique(list(sites.values())).tolist()\n",
    "        ages = get_subject_score(subject_IDs, score='AGE_AT_SCAN')\n",
    "        genders = get_subject_score(subject_IDs, score='SEX') \n",
    "\n",
    "        y_onehot = np.zeros([num_nodes, self.num_classes])\n",
    "        y = np.zeros([num_nodes])\n",
    "        site = np.zeros([num_nodes], dtype=int)\n",
    "        age = np.zeros([num_nodes], dtype=np.float32)\n",
    "        gender = np.zeros([num_nodes], dtype=int)\n",
    "        for i in range(num_nodes):\n",
    "            y_onehot[i, int(labels[subject_IDs[i]])-1] = 1\n",
    "            y[i] = int(labels[subject_IDs[i]])\n",
    "            site[i] = unique.index(sites[subject_IDs[i]])\n",
    "            age[i] = float(ages[subject_IDs[i]])\n",
    "            gender[i] = genders[subject_IDs[i]]\n",
    "        \n",
    "        self.y = y -1  \n",
    "\n",
    "        self.raw_features = get_networks(subject_IDs, kind=connectivity, atlas_name=atlas)\n",
    "\n",
    "        phonetic_data = np.zeros([num_nodes, 3], dtype=np.float32)\n",
    "        phonetic_data[:,0] = site \n",
    "        phonetic_data[:,1] = gender \n",
    "        phonetic_data[:,2] = age \n",
    "\n",
    "        self.pd_dict['SITE_ID'] = np.copy(phonetic_data[:,0])\n",
    "        self.pd_dict['SEX'] = np.copy(phonetic_data[:,1])\n",
    "        self.pd_dict['AGE_AT_SCAN'] = np.copy(phonetic_data[:,2]) \n",
    "        \n",
    "        return self.raw_features, self.y, phonetic_data\n",
    "\n",
    "    def data_split(self, n_folds):\n",
    "        # split data by k-fold CV\n",
    "        skf = StratifiedKFold(n_splits=n_folds)\n",
    "        cv_splits = list(skf.split(self.raw_features, self.y))\n",
    "        return cv_splits \n",
    "\n",
    "    def get_node_features(self, train_ind):\n",
    "        '''preprocess node features for wl-deepgcn\n",
    "        '''\n",
    "        node_ftr = feature_selection(self.raw_features, self.y, train_ind, self.node_ftr_dim)\n",
    "        self.node_ftr = preprocess_features(node_ftr) \n",
    "        return self.node_ftr\n",
    "\n",
    "    def get_WL_inputs(self, nonimg):\n",
    "        '''get WL inputs for wl-deepgcn \n",
    "        '''\n",
    "        # construct edge network inputs \n",
    "        n = self.node_ftr.shape[0] \n",
    "        num_edge = n*(1+n)//2 - n  # n*(n-1)//2,HO=6105\n",
    "        pd_ftr_dim = nonimg.shape[1]\n",
    "        edge_index = np.zeros([2, num_edge], dtype=np.int64) \n",
    "        edgenet_input = np.zeros([num_edge, 2*pd_ftr_dim], dtype=np.float32)  \n",
    "        aff_score = np.zeros(num_edge, dtype=np.float32)\n",
    "        # static affinity score used to pre-prune edges \n",
    "        aff_adj = get_static_affinity_adj(self.node_ftr, self.pd_dict)  \n",
    "        flatten_ind = 0 \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                edge_index[:,flatten_ind] = [i,j]\n",
    "                edgenet_input[flatten_ind]  = np.concatenate((nonimg[i], nonimg[j]))\n",
    "                aff_score[flatten_ind] = aff_adj[i][j]  \n",
    "                flatten_ind +=1\n",
    "\n",
    "        assert flatten_ind == num_edge, \"Error in computing edge input\"\n",
    "        \n",
    "        keep_ind = np.where(aff_score > 1.1)[0]  \n",
    "        edge_index = edge_index[:, keep_ind]\n",
    "        edgenet_input = edgenet_input[keep_ind]\n",
    "\n",
    "        return edge_index, edgenet_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc69f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn import Linear as Lin, Sequential as Seq\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class WL(torch.nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.3):\n",
    "        super(WL, self).__init__()\n",
    "        h1=256\n",
    "        h2=128\n",
    "        self.parser =nn.Sequential(\n",
    "                nn.Linear(input_dim, h1, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.BatchNorm1d(h1),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(h1, h2, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.BatchNorm1d(h2),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(h2, h2, bias=True),\n",
    "                )\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-8)\n",
    "        self.input_dim = input_dim\n",
    "        self.model_init()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.elu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:,0:self.input_dim]\n",
    "        x2 = x[:,self.input_dim:]\n",
    "        h1 = self.parser(x1) \n",
    "        h2 = self.parser(x2) \n",
    "        p = (self.cos(h1,h2) + 1)*0.5\n",
    "        return p\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Lin):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db8243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear as Lin, Sequential as Seq\n",
    "import torch_geometric as tg\n",
    "# from wl import WL\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, nhid):\n",
    "        super(MLP,self).__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "            torch.nn.Linear(input_dim,nhid))\n",
    "        \n",
    "    def forward(self, features):\n",
    "        output = self.cls(features)\n",
    "        return output\n",
    "            \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, nhid, num_classes, ngl, dropout, edge_dropout, edgenet_input_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        K=3   \n",
    "        hidden = [nhid for i in range(ngl)] \n",
    "        self.dropout = dropout\n",
    "        self.edge_dropout = edge_dropout \n",
    "        bias = False \n",
    "        self.relu = torch.nn.ReLU(inplace=True) \n",
    "        self.ngl = ngl \n",
    "        self.gconv = nn.ModuleList()\n",
    "        for i in range(ngl):\n",
    "            in_channels = input_dim if i==0  else hidden[i-1]\n",
    "            self.gconv.append(tg.nn.ChebConv(in_channels, hidden[i], K, normalization='sym', bias=bias)) \n",
    "          \n",
    "        self.cls = nn.Sequential(\n",
    "                torch.nn.Linear(16, 128),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(128), \n",
    "                torch.nn.Linear(128, num_classes))\n",
    "\n",
    "        self.edge_net = WL(input_dim=edgenet_input_dim//2, dropout=dropout)\n",
    "        self.model_init()\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Lin):\n",
    "                torch.nn.init.kaiming_normal_(m.weight) # He init\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n",
    "    def forward(self, features, edge_index, edgenet_input, enforce_edropout=False): \n",
    "        if self.edge_dropout>0:\n",
    "            if enforce_edropout or self.training:\n",
    "                one_mask = torch.ones([edgenet_input.shape[0],1])\n",
    "                self.drop_mask = F.dropout(one_mask, self.edge_dropout, True)\n",
    "                self.bool_mask = torch.squeeze(self.drop_mask.type(torch.bool))\n",
    "                edge_index = edge_index[:, self.bool_mask] \n",
    "                edgenet_input = edgenet_input[self.bool_mask] # Weights\n",
    "            \n",
    "        edge_weight = torch.squeeze(self.edge_net(edgenet_input))\n",
    "        \n",
    "\n",
    "        # GCN residual connection\n",
    "        # input layer\n",
    "        features = F.dropout(features, self.dropout, self.training)\n",
    "        x = self.relu(self.gconv[0](features, edge_index, edge_weight)) \n",
    "        x_temp = x\n",
    "        \n",
    "        # hidden layers\n",
    "        for i in range(1, self.ngl - 1): # self.ngl→7\n",
    "            x = F.dropout(x_temp, self.dropout, self.training)\n",
    "            x = self.relu(self.gconv[i](x, edge_index, edge_weight)) \n",
    "            x_temp = x_temp + x # ([871,64])\n",
    "\n",
    "        # output layer\n",
    "        x = F.dropout(x_temp, self.dropout, self.training)\n",
    "        x = self.relu(self.gconv[self.ngl - 1](x, edge_index, edge_weight))\n",
    "        x_temp = x_temp + x\n",
    "\n",
    "        output = x # Final output is not cumulative\n",
    "        output = self.cls(output) \n",
    "        \n",
    "        return output, edge_weight\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5856d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassSpecificity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def torchmetrics_accuracy(preds, labels):\n",
    "    acc = torchmetrics.functional.accuracy(preds, labels,task=\"multiclass\", num_classes=2)\n",
    "    return acc\n",
    "\n",
    "def torchmetrics_spef(preds, labels):\n",
    "    metric = MulticlassSpecificity(num_classes=2)\n",
    "    spef = metric(preds, labels)\n",
    "    return spef\n",
    "\n",
    "def torchmetrics_auc(preds, labels):\n",
    "    auc = torchmetrics.functional.auroc(preds, labels, task=\"multiclass\", num_classes=2)\n",
    "    return auc\n",
    "\n",
    "def confusion_matrix(preds, labels):\n",
    "    conf_matrix = torch.zeros(2, 2)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    for p, t in zip(preds, labels):\n",
    "        conf_matrix[t, p] += 1 \n",
    "    return conf_matrix\n",
    "def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Input\n",
    "    - cm : computer the value of confusion matrix\n",
    "    - normalize : True: %, False: 123\n",
    "    \"\"\"\n",
    "    classes = ['0:ASD','1:TC']\n",
    "    if normalize:\n",
    "        cm = cm.numpy()\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else '.0f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def correct_num(preds, labels):\n",
    "    \"\"\"Accuracy, auc with masking.Acc of the masked samples\"\"\"\n",
    "    correct_prediction = np.equal(np.argmax(preds, 1), labels).astype(np.float32)\n",
    "    return np.sum(correct_prediction)\n",
    "\n",
    "def prf(preds, labels, is_logit=True):\n",
    "    ''' input: logits, labels  ''' \n",
    "    pred_lab= np.argmax(preds, 1)\n",
    "    p,r,f,s  = precision_recall_fscore_support(labels, pred_lab, average='binary')\n",
    "    return [p,r,f]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae068d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "no_cuda: False\n",
      "seed: 46\n",
      "epochs: 200\n",
      "lr: 0.001\n",
      "weight_decay: 5e-05\n",
      "hidden: 16\n",
      "dropout: 0.2\n",
      "atlas: cc400\n",
      "num_features: 2000\n",
      "folds: 10\n",
      "connectivity: correlation\n",
      "max_degree: 3\n",
      "ngl: 4\n",
      "edropout: 0.3\n",
      "train: 1\n",
      "ckpt_path: ../folds/rois_cc400_pth_4_layer\n",
      "early_stopping: True\n",
      "early_stopping_patience: 20\n",
      "cuda: False\n",
      "  Loading dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_3116\\2141194207.py:217: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the 1-fold Training, Validation, and Test Sets:900,100,112\n",
      " Starting the 1-1 Fold:：\n",
      "Fitting estimator with 76636 features.\n",
      "Fitting estimator with 76536 features.\n",
      "Fitting estimator with 76436 features.\n",
      "Fitting estimator with 76336 features.\n",
      "Fitting estimator with 76236 features.\n",
      "Fitting estimator with 76136 features.\n",
      "Fitting estimator with 76036 features.\n",
      "Fitting estimator with 75936 features.\n",
      "Fitting estimator with 75836 features.\n",
      "Fitting estimator with 75736 features.\n",
      "Fitting estimator with 75636 features.\n",
      "Fitting estimator with 75536 features.\n",
      "Fitting estimator with 75436 features.\n",
      "Fitting estimator with 75336 features.\n",
      "Fitting estimator with 75236 features.\n",
      "Fitting estimator with 75136 features.\n",
      "Fitting estimator with 75036 features.\n",
      "Fitting estimator with 74936 features.\n",
      "Fitting estimator with 74836 features.\n",
      "Fitting estimator with 74736 features.\n",
      "Fitting estimator with 74636 features.\n",
      "Fitting estimator with 74536 features.\n",
      "Fitting estimator with 74436 features.\n",
      "Fitting estimator with 74336 features.\n",
      "Fitting estimator with 74236 features.\n",
      "Fitting estimator with 74136 features.\n",
      "Fitting estimator with 74036 features.\n",
      "Fitting estimator with 73936 features.\n",
      "Fitting estimator with 73836 features.\n",
      "Fitting estimator with 73736 features.\n",
      "Fitting estimator with 73636 features.\n",
      "Fitting estimator with 73536 features.\n",
      "Fitting estimator with 73436 features.\n",
      "Fitting estimator with 73336 features.\n",
      "Fitting estimator with 73236 features.\n",
      "Fitting estimator with 73136 features.\n",
      "Fitting estimator with 73036 features.\n",
      "Fitting estimator with 72936 features.\n",
      "Fitting estimator with 72836 features.\n",
      "Fitting estimator with 72736 features.\n",
      "Fitting estimator with 72636 features.\n",
      "Fitting estimator with 72536 features.\n",
      "Fitting estimator with 72436 features.\n",
      "Fitting estimator with 72336 features.\n",
      "Fitting estimator with 72236 features.\n",
      "Fitting estimator with 72136 features.\n",
      "Fitting estimator with 72036 features.\n",
      "Fitting estimator with 71936 features.\n",
      "Fitting estimator with 71836 features.\n",
      "Fitting estimator with 71736 features.\n",
      "Fitting estimator with 71636 features.\n",
      "Fitting estimator with 71536 features.\n",
      "Fitting estimator with 71436 features.\n",
      "Fitting estimator with 71336 features.\n",
      "Fitting estimator with 71236 features.\n",
      "Fitting estimator with 71136 features.\n",
      "Fitting estimator with 71036 features.\n",
      "Fitting estimator with 70936 features.\n",
      "Fitting estimator with 70836 features.\n",
      "Fitting estimator with 70736 features.\n",
      "Fitting estimator with 70636 features.\n",
      "Fitting estimator with 70536 features.\n",
      "Fitting estimator with 70436 features.\n",
      "Fitting estimator with 70336 features.\n",
      "Fitting estimator with 70236 features.\n",
      "Fitting estimator with 70136 features.\n",
      "Fitting estimator with 70036 features.\n",
      "Fitting estimator with 69936 features.\n",
      "Fitting estimator with 69836 features.\n",
      "Fitting estimator with 69736 features.\n",
      "Fitting estimator with 69636 features.\n",
      "Fitting estimator with 69536 features.\n",
      "Fitting estimator with 69436 features.\n",
      "Fitting estimator with 69336 features.\n",
      "Fitting estimator with 69236 features.\n",
      "Fitting estimator with 69136 features.\n",
      "Fitting estimator with 69036 features.\n",
      "Fitting estimator with 68936 features.\n",
      "Fitting estimator with 68836 features.\n",
      "Fitting estimator with 68736 features.\n",
      "Fitting estimator with 68636 features.\n",
      "Fitting estimator with 68536 features.\n",
      "Fitting estimator with 68436 features.\n",
      "Fitting estimator with 68336 features.\n",
      "Fitting estimator with 68236 features.\n",
      "Fitting estimator with 68136 features.\n",
      "Fitting estimator with 68036 features.\n",
      "Fitting estimator with 67936 features.\n",
      "Fitting estimator with 67836 features.\n",
      "Fitting estimator with 67736 features.\n",
      "Fitting estimator with 67636 features.\n",
      "Fitting estimator with 67536 features.\n",
      "Fitting estimator with 67436 features.\n",
      "Fitting estimator with 67336 features.\n",
      "Fitting estimator with 67236 features.\n",
      "Fitting estimator with 67136 features.\n",
      "Fitting estimator with 67036 features.\n",
      "Fitting estimator with 66936 features.\n",
      "Fitting estimator with 66836 features.\n",
      "Fitting estimator with 66736 features.\n",
      "Fitting estimator with 66636 features.\n",
      "Fitting estimator with 66536 features.\n",
      "Fitting estimator with 66436 features.\n",
      "Fitting estimator with 66336 features.\n",
      "Fitting estimator with 66236 features.\n",
      "Fitting estimator with 66136 features.\n",
      "Fitting estimator with 66036 features.\n",
      "Fitting estimator with 65936 features.\n",
      "Fitting estimator with 65836 features.\n",
      "Fitting estimator with 65736 features.\n",
      "Fitting estimator with 65636 features.\n",
      "Fitting estimator with 65536 features.\n",
      "Fitting estimator with 65436 features.\n",
      "Fitting estimator with 65336 features.\n",
      "Fitting estimator with 65236 features.\n",
      "Fitting estimator with 65136 features.\n",
      "Fitting estimator with 65036 features.\n",
      "Fitting estimator with 64936 features.\n",
      "Fitting estimator with 64836 features.\n",
      "Fitting estimator with 64736 features.\n",
      "Fitting estimator with 64636 features.\n",
      "Fitting estimator with 64536 features.\n",
      "Fitting estimator with 64436 features.\n",
      "Fitting estimator with 64336 features.\n",
      "Fitting estimator with 64236 features.\n",
      "Fitting estimator with 64136 features.\n",
      "Fitting estimator with 64036 features.\n",
      "Fitting estimator with 63936 features.\n",
      "Fitting estimator with 63836 features.\n",
      "Fitting estimator with 63736 features.\n",
      "Fitting estimator with 63636 features.\n",
      "Fitting estimator with 63536 features.\n",
      "Fitting estimator with 63436 features.\n",
      "Fitting estimator with 63336 features.\n",
      "Fitting estimator with 63236 features.\n",
      "Fitting estimator with 63136 features.\n",
      "Fitting estimator with 63036 features.\n",
      "Fitting estimator with 62936 features.\n",
      "Fitting estimator with 62836 features.\n",
      "Fitting estimator with 62736 features.\n",
      "Fitting estimator with 62636 features.\n",
      "Fitting estimator with 62536 features.\n",
      "Fitting estimator with 62436 features.\n",
      "Fitting estimator with 62336 features.\n",
      "Fitting estimator with 62236 features.\n",
      "Fitting estimator with 62136 features.\n",
      "Fitting estimator with 62036 features.\n",
      "Fitting estimator with 61936 features.\n",
      "Fitting estimator with 61836 features.\n",
      "Fitting estimator with 61736 features.\n",
      "Fitting estimator with 61636 features.\n",
      "Fitting estimator with 61536 features.\n",
      "Fitting estimator with 61436 features.\n",
      "Fitting estimator with 61336 features.\n",
      "Fitting estimator with 61236 features.\n",
      "Fitting estimator with 61136 features.\n",
      "Fitting estimator with 61036 features.\n",
      "Fitting estimator with 60936 features.\n",
      "Fitting estimator with 60836 features.\n",
      "Fitting estimator with 60736 features.\n",
      "Fitting estimator with 60636 features.\n",
      "Fitting estimator with 60536 features.\n",
      "Fitting estimator with 60436 features.\n",
      "Fitting estimator with 60336 features.\n",
      "Fitting estimator with 60236 features.\n",
      "Fitting estimator with 60136 features.\n",
      "Fitting estimator with 60036 features.\n",
      "Fitting estimator with 59936 features.\n",
      "Fitting estimator with 59836 features.\n",
      "Fitting estimator with 59736 features.\n",
      "Fitting estimator with 59636 features.\n",
      "Fitting estimator with 59536 features.\n",
      "Fitting estimator with 59436 features.\n",
      "Fitting estimator with 59336 features.\n",
      "Fitting estimator with 59236 features.\n",
      "Fitting estimator with 59136 features.\n",
      "Fitting estimator with 59036 features.\n",
      "Fitting estimator with 58936 features.\n",
      "Fitting estimator with 58836 features.\n",
      "Fitting estimator with 58736 features.\n",
      "Fitting estimator with 58636 features.\n",
      "Fitting estimator with 58536 features.\n",
      "Fitting estimator with 58436 features.\n",
      "Fitting estimator with 58336 features.\n",
      "Fitting estimator with 58236 features.\n",
      "Fitting estimator with 58136 features.\n",
      "Fitting estimator with 58036 features.\n",
      "Fitting estimator with 57936 features.\n",
      "Fitting estimator with 57836 features.\n",
      "Fitting estimator with 57736 features.\n",
      "Fitting estimator with 57636 features.\n",
      "Fitting estimator with 57536 features.\n",
      "Fitting estimator with 57436 features.\n",
      "Fitting estimator with 57336 features.\n",
      "Fitting estimator with 57236 features.\n",
      "Fitting estimator with 57136 features.\n",
      "Fitting estimator with 57036 features.\n",
      "Fitting estimator with 56936 features.\n",
      "Fitting estimator with 56836 features.\n",
      "Fitting estimator with 56736 features.\n",
      "Fitting estimator with 56636 features.\n",
      "Fitting estimator with 56536 features.\n",
      "Fitting estimator with 56436 features.\n",
      "Fitting estimator with 56336 features.\n",
      "Fitting estimator with 56236 features.\n",
      "Fitting estimator with 56136 features.\n",
      "Fitting estimator with 56036 features.\n",
      "Fitting estimator with 55936 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 55836 features.\n",
      "Fitting estimator with 55736 features.\n",
      "Fitting estimator with 55636 features.\n",
      "Fitting estimator with 55536 features.\n",
      "Fitting estimator with 55436 features.\n",
      "Fitting estimator with 55336 features.\n",
      "Fitting estimator with 55236 features.\n",
      "Fitting estimator with 55136 features.\n",
      "Fitting estimator with 55036 features.\n",
      "Fitting estimator with 54936 features.\n",
      "Fitting estimator with 54836 features.\n",
      "Fitting estimator with 54736 features.\n",
      "Fitting estimator with 54636 features.\n",
      "Fitting estimator with 54536 features.\n",
      "Fitting estimator with 54436 features.\n",
      "Fitting estimator with 54336 features.\n",
      "Fitting estimator with 54236 features.\n",
      "Fitting estimator with 54136 features.\n",
      "Fitting estimator with 54036 features.\n",
      "Fitting estimator with 53936 features.\n",
      "Fitting estimator with 53836 features.\n",
      "Fitting estimator with 53736 features.\n",
      "Fitting estimator with 53636 features.\n",
      "Fitting estimator with 53536 features.\n",
      "Fitting estimator with 53436 features.\n",
      "Fitting estimator with 53336 features.\n",
      "Fitting estimator with 53236 features.\n",
      "Fitting estimator with 53136 features.\n",
      "Fitting estimator with 53036 features.\n",
      "Fitting estimator with 52936 features.\n",
      "Fitting estimator with 52836 features.\n",
      "Fitting estimator with 52736 features.\n",
      "Fitting estimator with 52636 features.\n",
      "Fitting estimator with 52536 features.\n",
      "Fitting estimator with 52436 features.\n",
      "Fitting estimator with 52336 features.\n",
      "Fitting estimator with 52236 features.\n",
      "Fitting estimator with 52136 features.\n",
      "Fitting estimator with 52036 features.\n",
      "Fitting estimator with 51936 features.\n",
      "Fitting estimator with 51836 features.\n",
      "Fitting estimator with 51736 features.\n",
      "Fitting estimator with 51636 features.\n",
      "Fitting estimator with 51536 features.\n",
      "Fitting estimator with 51436 features.\n",
      "Fitting estimator with 51336 features.\n",
      "Fitting estimator with 51236 features.\n",
      "Fitting estimator with 51136 features.\n",
      "Fitting estimator with 51036 features.\n",
      "Fitting estimator with 50936 features.\n",
      "Fitting estimator with 50836 features.\n",
      "Fitting estimator with 50736 features.\n",
      "Fitting estimator with 50636 features.\n",
      "Fitting estimator with 50536 features.\n",
      "Fitting estimator with 50436 features.\n",
      "Fitting estimator with 50336 features.\n",
      "Fitting estimator with 50236 features.\n",
      "Fitting estimator with 50136 features.\n",
      "Fitting estimator with 50036 features.\n",
      "Fitting estimator with 49936 features.\n",
      "Fitting estimator with 49836 features.\n",
      "Fitting estimator with 49736 features.\n",
      "Fitting estimator with 49636 features.\n",
      "Fitting estimator with 49536 features.\n",
      "Fitting estimator with 49436 features.\n",
      "Fitting estimator with 49336 features.\n",
      "Fitting estimator with 49236 features.\n",
      "Fitting estimator with 49136 features.\n",
      "Fitting estimator with 49036 features.\n",
      "Fitting estimator with 48936 features.\n",
      "Fitting estimator with 48836 features.\n",
      "Fitting estimator with 48736 features.\n",
      "Fitting estimator with 48636 features.\n",
      "Fitting estimator with 48536 features.\n",
      "Fitting estimator with 48436 features.\n",
      "Fitting estimator with 48336 features.\n",
      "Fitting estimator with 48236 features.\n",
      "Fitting estimator with 48136 features.\n",
      "Fitting estimator with 48036 features.\n",
      "Fitting estimator with 47936 features.\n",
      "Fitting estimator with 47836 features.\n",
      "Fitting estimator with 47736 features.\n",
      "Fitting estimator with 47636 features.\n",
      "Fitting estimator with 47536 features.\n",
      "Fitting estimator with 47436 features.\n",
      "Fitting estimator with 47336 features.\n",
      "Fitting estimator with 47236 features.\n",
      "Fitting estimator with 47136 features.\n",
      "Fitting estimator with 47036 features.\n",
      "Fitting estimator with 46936 features.\n",
      "Fitting estimator with 46836 features.\n",
      "Fitting estimator with 46736 features.\n",
      "Fitting estimator with 46636 features.\n",
      "Fitting estimator with 46536 features.\n",
      "Fitting estimator with 46436 features.\n",
      "Fitting estimator with 46336 features.\n",
      "Fitting estimator with 46236 features.\n",
      "Fitting estimator with 46136 features.\n",
      "Fitting estimator with 46036 features.\n",
      "Fitting estimator with 45936 features.\n",
      "Fitting estimator with 45836 features.\n",
      "Fitting estimator with 45736 features.\n",
      "Fitting estimator with 45636 features.\n",
      "Fitting estimator with 45536 features.\n",
      "Fitting estimator with 45436 features.\n",
      "Fitting estimator with 45336 features.\n",
      "Fitting estimator with 45236 features.\n",
      "Fitting estimator with 45136 features.\n",
      "Fitting estimator with 45036 features.\n",
      "Fitting estimator with 44936 features.\n",
      "Fitting estimator with 44836 features.\n",
      "Fitting estimator with 44736 features.\n",
      "Fitting estimator with 44636 features.\n",
      "Fitting estimator with 44536 features.\n",
      "Fitting estimator with 44436 features.\n",
      "Fitting estimator with 44336 features.\n",
      "Fitting estimator with 44236 features.\n",
      "Fitting estimator with 44136 features.\n",
      "Fitting estimator with 44036 features.\n",
      "Fitting estimator with 43936 features.\n",
      "Fitting estimator with 43836 features.\n",
      "Fitting estimator with 43736 features.\n",
      "Fitting estimator with 43636 features.\n",
      "Fitting estimator with 43536 features.\n",
      "Fitting estimator with 43436 features.\n",
      "Fitting estimator with 43336 features.\n",
      "Fitting estimator with 43236 features.\n",
      "Fitting estimator with 43136 features.\n",
      "Fitting estimator with 43036 features.\n",
      "Fitting estimator with 42936 features.\n",
      "Fitting estimator with 42836 features.\n",
      "Fitting estimator with 42736 features.\n",
      "Fitting estimator with 42636 features.\n",
      "Fitting estimator with 42536 features.\n",
      "Fitting estimator with 42436 features.\n",
      "Fitting estimator with 42336 features.\n",
      "Fitting estimator with 42236 features.\n",
      "Fitting estimator with 42136 features.\n",
      "Fitting estimator with 42036 features.\n",
      "Fitting estimator with 41936 features.\n",
      "Fitting estimator with 41836 features.\n",
      "Fitting estimator with 41736 features.\n",
      "Fitting estimator with 41636 features.\n",
      "Fitting estimator with 41536 features.\n",
      "Fitting estimator with 41436 features.\n",
      "Fitting estimator with 41336 features.\n",
      "Fitting estimator with 41236 features.\n",
      "Fitting estimator with 41136 features.\n",
      "Fitting estimator with 41036 features.\n",
      "Fitting estimator with 40936 features.\n",
      "Fitting estimator with 40836 features.\n",
      "Fitting estimator with 40736 features.\n",
      "Fitting estimator with 40636 features.\n",
      "Fitting estimator with 40536 features.\n",
      "Fitting estimator with 40436 features.\n",
      "Fitting estimator with 40336 features.\n",
      "Fitting estimator with 40236 features.\n",
      "Fitting estimator with 40136 features.\n",
      "Fitting estimator with 40036 features.\n",
      "Fitting estimator with 39936 features.\n",
      "Fitting estimator with 39836 features.\n",
      "Fitting estimator with 39736 features.\n",
      "Fitting estimator with 39636 features.\n",
      "Fitting estimator with 39536 features.\n",
      "Fitting estimator with 39436 features.\n",
      "Fitting estimator with 39336 features.\n",
      "Fitting estimator with 39236 features.\n",
      "Fitting estimator with 39136 features.\n",
      "Fitting estimator with 39036 features.\n",
      "Fitting estimator with 38936 features.\n",
      "Fitting estimator with 38836 features.\n",
      "Fitting estimator with 38736 features.\n",
      "Fitting estimator with 38636 features.\n",
      "Fitting estimator with 38536 features.\n",
      "Fitting estimator with 38436 features.\n",
      "Fitting estimator with 38336 features.\n",
      "Fitting estimator with 38236 features.\n",
      "Fitting estimator with 38136 features.\n",
      "Fitting estimator with 38036 features.\n",
      "Fitting estimator with 37936 features.\n",
      "Fitting estimator with 37836 features.\n",
      "Fitting estimator with 37736 features.\n",
      "Fitting estimator with 37636 features.\n",
      "Fitting estimator with 37536 features.\n",
      "Fitting estimator with 37436 features.\n",
      "Fitting estimator with 37336 features.\n",
      "Fitting estimator with 37236 features.\n",
      "Fitting estimator with 37136 features.\n",
      "Fitting estimator with 37036 features.\n",
      "Fitting estimator with 36936 features.\n",
      "Fitting estimator with 36836 features.\n",
      "Fitting estimator with 36736 features.\n",
      "Fitting estimator with 36636 features.\n",
      "Fitting estimator with 36536 features.\n",
      "Fitting estimator with 36436 features.\n",
      "Fitting estimator with 36336 features.\n",
      "Fitting estimator with 36236 features.\n",
      "Fitting estimator with 36136 features.\n",
      "Fitting estimator with 36036 features.\n",
      "Fitting estimator with 35936 features.\n",
      "Fitting estimator with 35836 features.\n",
      "Fitting estimator with 35736 features.\n",
      "Fitting estimator with 35636 features.\n",
      "Fitting estimator with 35536 features.\n",
      "Fitting estimator with 35436 features.\n",
      "Fitting estimator with 35336 features.\n",
      "Fitting estimator with 35236 features.\n",
      "Fitting estimator with 35136 features.\n",
      "Fitting estimator with 35036 features.\n",
      "Fitting estimator with 34936 features.\n",
      "Fitting estimator with 34836 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 34736 features.\n",
      "Fitting estimator with 34636 features.\n",
      "Fitting estimator with 34536 features.\n",
      "Fitting estimator with 34436 features.\n",
      "Fitting estimator with 34336 features.\n",
      "Fitting estimator with 34236 features.\n",
      "Fitting estimator with 34136 features.\n",
      "Fitting estimator with 34036 features.\n",
      "Fitting estimator with 33936 features.\n",
      "Fitting estimator with 33836 features.\n",
      "Fitting estimator with 33736 features.\n",
      "Fitting estimator with 33636 features.\n",
      "Fitting estimator with 33536 features.\n",
      "Fitting estimator with 33436 features.\n",
      "Fitting estimator with 33336 features.\n",
      "Fitting estimator with 33236 features.\n",
      "Fitting estimator with 33136 features.\n",
      "Fitting estimator with 33036 features.\n",
      "Fitting estimator with 32936 features.\n",
      "Fitting estimator with 32836 features.\n",
      "Fitting estimator with 32736 features.\n",
      "Fitting estimator with 32636 features.\n",
      "Fitting estimator with 32536 features.\n",
      "Fitting estimator with 32436 features.\n",
      "Fitting estimator with 32336 features.\n",
      "Fitting estimator with 32236 features.\n",
      "Fitting estimator with 32136 features.\n",
      "Fitting estimator with 32036 features.\n",
      "Fitting estimator with 31936 features.\n",
      "Fitting estimator with 31836 features.\n",
      "Fitting estimator with 31736 features.\n",
      "Fitting estimator with 31636 features.\n",
      "Fitting estimator with 31536 features.\n",
      "Fitting estimator with 31436 features.\n",
      "Fitting estimator with 31336 features.\n",
      "Fitting estimator with 31236 features.\n",
      "Fitting estimator with 31136 features.\n",
      "Fitting estimator with 31036 features.\n",
      "Fitting estimator with 30936 features.\n",
      "Fitting estimator with 30836 features.\n",
      "Fitting estimator with 30736 features.\n",
      "Fitting estimator with 30636 features.\n",
      "Fitting estimator with 30536 features.\n",
      "Fitting estimator with 30436 features.\n",
      "Fitting estimator with 30336 features.\n",
      "Fitting estimator with 30236 features.\n",
      "Fitting estimator with 30136 features.\n",
      "Fitting estimator with 30036 features.\n",
      "Fitting estimator with 29936 features.\n",
      "Fitting estimator with 29836 features.\n",
      "Fitting estimator with 29736 features.\n",
      "Fitting estimator with 29636 features.\n",
      "Fitting estimator with 29536 features.\n",
      "Fitting estimator with 29436 features.\n",
      "Fitting estimator with 29336 features.\n",
      "Fitting estimator with 29236 features.\n",
      "Fitting estimator with 29136 features.\n",
      "Fitting estimator with 29036 features.\n",
      "Fitting estimator with 28936 features.\n",
      "Fitting estimator with 28836 features.\n",
      "Fitting estimator with 28736 features.\n",
      "Fitting estimator with 28636 features.\n",
      "Fitting estimator with 28536 features.\n",
      "Fitting estimator with 28436 features.\n",
      "Fitting estimator with 28336 features.\n",
      "Fitting estimator with 28236 features.\n",
      "Fitting estimator with 28136 features.\n",
      "Fitting estimator with 28036 features.\n",
      "Fitting estimator with 27936 features.\n",
      "Fitting estimator with 27836 features.\n",
      "Fitting estimator with 27736 features.\n",
      "Fitting estimator with 27636 features.\n",
      "Fitting estimator with 27536 features.\n",
      "Fitting estimator with 27436 features.\n",
      "Fitting estimator with 27336 features.\n",
      "Fitting estimator with 27236 features.\n",
      "Fitting estimator with 27136 features.\n",
      "Fitting estimator with 27036 features.\n",
      "Fitting estimator with 26936 features.\n",
      "Fitting estimator with 26836 features.\n",
      "Fitting estimator with 26736 features.\n",
      "Fitting estimator with 26636 features.\n",
      "Fitting estimator with 26536 features.\n",
      "Fitting estimator with 26436 features.\n",
      "Fitting estimator with 26336 features.\n",
      "Fitting estimator with 26236 features.\n",
      "Fitting estimator with 26136 features.\n",
      "Fitting estimator with 26036 features.\n",
      "Fitting estimator with 25936 features.\n",
      "Fitting estimator with 25836 features.\n",
      "Fitting estimator with 25736 features.\n",
      "Fitting estimator with 25636 features.\n",
      "Fitting estimator with 25536 features.\n",
      "Fitting estimator with 25436 features.\n",
      "Fitting estimator with 25336 features.\n",
      "Fitting estimator with 25236 features.\n",
      "Fitting estimator with 25136 features.\n",
      "Fitting estimator with 25036 features.\n",
      "Fitting estimator with 24936 features.\n",
      "Fitting estimator with 24836 features.\n",
      "Fitting estimator with 24736 features.\n",
      "Fitting estimator with 24636 features.\n",
      "Fitting estimator with 24536 features.\n",
      "Fitting estimator with 24436 features.\n",
      "Fitting estimator with 24336 features.\n",
      "Fitting estimator with 24236 features.\n",
      "Fitting estimator with 24136 features.\n",
      "Fitting estimator with 24036 features.\n",
      "Fitting estimator with 23936 features.\n",
      "Fitting estimator with 23836 features.\n",
      "Fitting estimator with 23736 features.\n",
      "Fitting estimator with 23636 features.\n",
      "Fitting estimator with 23536 features.\n",
      "Fitting estimator with 23436 features.\n",
      "Fitting estimator with 23336 features.\n",
      "Fitting estimator with 23236 features.\n",
      "Fitting estimator with 23136 features.\n",
      "Fitting estimator with 23036 features.\n",
      "Fitting estimator with 22936 features.\n",
      "Fitting estimator with 22836 features.\n",
      "Fitting estimator with 22736 features.\n",
      "Fitting estimator with 22636 features.\n",
      "Fitting estimator with 22536 features.\n",
      "Fitting estimator with 22436 features.\n",
      "Fitting estimator with 22336 features.\n",
      "Fitting estimator with 22236 features.\n",
      "Fitting estimator with 22136 features.\n",
      "Fitting estimator with 22036 features.\n",
      "Fitting estimator with 21936 features.\n",
      "Fitting estimator with 21836 features.\n",
      "Fitting estimator with 21736 features.\n",
      "Fitting estimator with 21636 features.\n",
      "Fitting estimator with 21536 features.\n",
      "Fitting estimator with 21436 features.\n",
      "Fitting estimator with 21336 features.\n",
      "Fitting estimator with 21236 features.\n",
      "Fitting estimator with 21136 features.\n",
      "Fitting estimator with 21036 features.\n",
      "Fitting estimator with 20936 features.\n",
      "Fitting estimator with 20836 features.\n",
      "Fitting estimator with 20736 features.\n",
      "Fitting estimator with 20636 features.\n",
      "Fitting estimator with 20536 features.\n",
      "Fitting estimator with 20436 features.\n",
      "Fitting estimator with 20336 features.\n",
      "Fitting estimator with 20236 features.\n",
      "Fitting estimator with 20136 features.\n",
      "Fitting estimator with 20036 features.\n",
      "Fitting estimator with 19936 features.\n",
      "Fitting estimator with 19836 features.\n",
      "Fitting estimator with 19736 features.\n",
      "Fitting estimator with 19636 features.\n",
      "Fitting estimator with 19536 features.\n",
      "Fitting estimator with 19436 features.\n",
      "Fitting estimator with 19336 features.\n",
      "Fitting estimator with 19236 features.\n",
      "Fitting estimator with 19136 features.\n",
      "Fitting estimator with 19036 features.\n",
      "Fitting estimator with 18936 features.\n",
      "Fitting estimator with 18836 features.\n",
      "Fitting estimator with 18736 features.\n",
      "Fitting estimator with 18636 features.\n",
      "Fitting estimator with 18536 features.\n",
      "Fitting estimator with 18436 features.\n",
      "Fitting estimator with 18336 features.\n",
      "Fitting estimator with 18236 features.\n",
      "Fitting estimator with 18136 features.\n",
      "Fitting estimator with 18036 features.\n",
      "Fitting estimator with 17936 features.\n",
      "Fitting estimator with 17836 features.\n",
      "Fitting estimator with 17736 features.\n",
      "Fitting estimator with 17636 features.\n",
      "Fitting estimator with 17536 features.\n",
      "Fitting estimator with 17436 features.\n",
      "Fitting estimator with 17336 features.\n",
      "Fitting estimator with 17236 features.\n",
      "Fitting estimator with 17136 features.\n",
      "Fitting estimator with 17036 features.\n",
      "Fitting estimator with 16936 features.\n",
      "Fitting estimator with 16836 features.\n",
      "Fitting estimator with 16736 features.\n",
      "Fitting estimator with 16636 features.\n",
      "Fitting estimator with 16536 features.\n",
      "Fitting estimator with 16436 features.\n",
      "Fitting estimator with 16336 features.\n",
      "Fitting estimator with 16236 features.\n",
      "Fitting estimator with 16136 features.\n",
      "Fitting estimator with 16036 features.\n",
      "Fitting estimator with 15936 features.\n",
      "Fitting estimator with 15836 features.\n",
      "Fitting estimator with 15736 features.\n",
      "Fitting estimator with 15636 features.\n",
      "Fitting estimator with 15536 features.\n",
      "Fitting estimator with 15436 features.\n",
      "Fitting estimator with 15336 features.\n",
      "Fitting estimator with 15236 features.\n",
      "Fitting estimator with 15136 features.\n",
      "Fitting estimator with 15036 features.\n",
      "Fitting estimator with 14936 features.\n",
      "Fitting estimator with 14836 features.\n",
      "Fitting estimator with 14736 features.\n",
      "Fitting estimator with 14636 features.\n",
      "Fitting estimator with 14536 features.\n",
      "Fitting estimator with 14436 features.\n",
      "Fitting estimator with 14336 features.\n",
      "Fitting estimator with 14236 features.\n",
      "Fitting estimator with 14136 features.\n",
      "Fitting estimator with 14036 features.\n",
      "Fitting estimator with 13936 features.\n",
      "Fitting estimator with 13836 features.\n",
      "Fitting estimator with 13736 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 13636 features.\n",
      "Fitting estimator with 13536 features.\n",
      "Fitting estimator with 13436 features.\n",
      "Fitting estimator with 13336 features.\n",
      "Fitting estimator with 13236 features.\n",
      "Fitting estimator with 13136 features.\n",
      "Fitting estimator with 13036 features.\n",
      "Fitting estimator with 12936 features.\n",
      "Fitting estimator with 12836 features.\n",
      "Fitting estimator with 12736 features.\n",
      "Fitting estimator with 12636 features.\n",
      "Fitting estimator with 12536 features.\n",
      "Fitting estimator with 12436 features.\n",
      "Fitting estimator with 12336 features.\n",
      "Fitting estimator with 12236 features.\n",
      "Fitting estimator with 12136 features.\n",
      "Fitting estimator with 12036 features.\n",
      "Fitting estimator with 11936 features.\n",
      "Fitting estimator with 11836 features.\n",
      "Fitting estimator with 11736 features.\n",
      "Fitting estimator with 11636 features.\n",
      "Fitting estimator with 11536 features.\n",
      "Fitting estimator with 11436 features.\n",
      "Fitting estimator with 11336 features.\n",
      "Fitting estimator with 11236 features.\n",
      "Fitting estimator with 11136 features.\n",
      "Fitting estimator with 11036 features.\n",
      "Fitting estimator with 10936 features.\n",
      "Fitting estimator with 10836 features.\n",
      "Fitting estimator with 10736 features.\n",
      "Fitting estimator with 10636 features.\n",
      "Fitting estimator with 10536 features.\n",
      "Fitting estimator with 10436 features.\n",
      "Fitting estimator with 10336 features.\n",
      "Fitting estimator with 10236 features.\n",
      "Fitting estimator with 10136 features.\n",
      "Fitting estimator with 10036 features.\n",
      "Fitting estimator with 9936 features.\n",
      "Fitting estimator with 9836 features.\n",
      "Fitting estimator with 9736 features.\n",
      "Fitting estimator with 9636 features.\n",
      "Fitting estimator with 9536 features.\n",
      "Fitting estimator with 9436 features.\n",
      "Fitting estimator with 9336 features.\n",
      "Fitting estimator with 9236 features.\n",
      "Fitting estimator with 9136 features.\n",
      "Fitting estimator with 9036 features.\n",
      "Fitting estimator with 8936 features.\n",
      "Fitting estimator with 8836 features.\n",
      "Fitting estimator with 8736 features.\n",
      "Fitting estimator with 8636 features.\n",
      "Fitting estimator with 8536 features.\n",
      "Fitting estimator with 8436 features.\n",
      "Fitting estimator with 8336 features.\n",
      "Fitting estimator with 8236 features.\n",
      "Fitting estimator with 8136 features.\n",
      "Fitting estimator with 8036 features.\n",
      "Fitting estimator with 7936 features.\n",
      "Fitting estimator with 7836 features.\n",
      "Fitting estimator with 7736 features.\n",
      "Fitting estimator with 7636 features.\n",
      "Fitting estimator with 7536 features.\n",
      "Fitting estimator with 7436 features.\n",
      "Fitting estimator with 7336 features.\n",
      "Fitting estimator with 7236 features.\n",
      "Fitting estimator with 7136 features.\n",
      "Fitting estimator with 7036 features.\n",
      "Fitting estimator with 6936 features.\n",
      "Fitting estimator with 6836 features.\n",
      "Fitting estimator with 6736 features.\n",
      "Fitting estimator with 6636 features.\n",
      "Fitting estimator with 6536 features.\n",
      "Fitting estimator with 6436 features.\n",
      "Fitting estimator with 6336 features.\n",
      "Fitting estimator with 6236 features.\n",
      "Fitting estimator with 6136 features.\n",
      "Fitting estimator with 6036 features.\n",
      "Fitting estimator with 5936 features.\n",
      "Fitting estimator with 5836 features.\n",
      "Fitting estimator with 5736 features.\n",
      "Fitting estimator with 5636 features.\n",
      "Fitting estimator with 5536 features.\n",
      "Fitting estimator with 5436 features.\n",
      "Fitting estimator with 5336 features.\n",
      "Fitting estimator with 5236 features.\n",
      "Fitting estimator with 5136 features.\n",
      "Fitting estimator with 5036 features.\n",
      "Fitting estimator with 4936 features.\n",
      "Fitting estimator with 4836 features.\n",
      "Fitting estimator with 4736 features.\n",
      "Fitting estimator with 4636 features.\n",
      "Fitting estimator with 4536 features.\n",
      "Fitting estimator with 4436 features.\n",
      "Fitting estimator with 4336 features.\n",
      "Fitting estimator with 4236 features.\n",
      "Fitting estimator with 4136 features.\n",
      "Fitting estimator with 4036 features.\n",
      "Fitting estimator with 3936 features.\n",
      "Fitting estimator with 3836 features.\n",
      "Fitting estimator with 3736 features.\n",
      "Fitting estimator with 3636 features.\n",
      "Fitting estimator with 3536 features.\n",
      "Fitting estimator with 3436 features.\n",
      "Fitting estimator with 3336 features.\n",
      "Fitting estimator with 3236 features.\n",
      "Fitting estimator with 3136 features.\n",
      "Fitting estimator with 3036 features.\n",
      "Fitting estimator with 2936 features.\n",
      "Fitting estimator with 2836 features.\n",
      "Fitting estimator with 2736 features.\n",
      "Fitting estimator with 2636 features.\n",
      "Fitting estimator with 2536 features.\n",
      "Fitting estimator with 2436 features.\n",
      "Fitting estimator with 2336 features.\n",
      "Fitting estimator with 2236 features.\n",
      "Fitting estimator with 2136 features.\n",
      "Fitting estimator with 2036 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001\n",
      "acc_train:0.4700 pre_train:0.4902 recall_train:0.6473 F1_train:0.5579 AUC_train:0.4541\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.5972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0002\n",
      "acc_train:0.5356 pre_train:0.5439 recall_train:0.6258 F1_train:0.5820 AUC_train:0.5373\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.5568\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0003\n",
      "acc_train:0.5678 pre_train:0.5754 recall_train:0.6237 F1_train:0.5986 AUC_train:0.6190\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.5433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0004\n",
      "acc_train:0.6089 pre_train:0.6336 recall_train:0.5763 F1_train:0.6036 AUC_train:0.6380\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.5472\n",
      "Epoch:0005\n",
      "acc_train:0.6044 pre_train:0.6394 recall_train:0.5376 F1_train:0.5841 AUC_train:0.6309\n",
      "acc_val:0.4800 pre_val:0.2500 recall_val:0.0200 F1_val:0.037037 AUC_val:0.5704\n",
      "Epoch:0006\n",
      "acc_train:0.6367 pre_train:0.6778 recall_train:0.5656 F1_train:0.6166 AUC_train:0.6651\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.0600 F1_val:0.107143 AUC_val:0.5892\n",
      "Epoch:0007\n",
      "acc_train:0.5900 pre_train:0.6250 recall_train:0.5161 F1_train:0.5654 AUC_train:0.6158\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.0600 F1_val:0.107143 AUC_val:0.6140\n",
      "Epoch:0008\n",
      "acc_train:0.6189 pre_train:0.6540 recall_train:0.5570 F1_train:0.6016 AUC_train:0.6576\n",
      "acc_val:0.5700 pre_val:0.7692 recall_val:0.2000 F1_val:0.317460 AUC_val:0.6288\n",
      "Epoch:0009\n",
      "acc_train:0.6222 pre_train:0.6658 recall_train:0.5398 F1_train:0.5962 AUC_train:0.6612\n",
      "acc_val:0.5900 pre_val:0.7368 recall_val:0.2800 F1_val:0.405797 AUC_val:0.6496\n",
      "Epoch:0010\n",
      "acc_train:0.6433 pre_train:0.7011 recall_train:0.5398 F1_train:0.6100 AUC_train:0.6928\n",
      "acc_val:0.6400 pre_val:0.7692 recall_val:0.4000 F1_val:0.526316 AUC_val:0.6528\n",
      "Epoch:0011\n",
      "acc_train:0.6556 pre_train:0.7135 recall_train:0.5570 F1_train:0.6256 AUC_train:0.6900\n",
      "acc_val:0.6400 pre_val:0.7333 recall_val:0.4400 F1_val:0.550000 AUC_val:0.6624\n",
      "Epoch:0012\n",
      "acc_train:0.6367 pre_train:0.6855 recall_train:0.5484 F1_train:0.6093 AUC_train:0.6809\n",
      "acc_val:0.6600 pre_val:0.7500 recall_val:0.4800 F1_val:0.585366 AUC_val:0.6752\n",
      "Epoch:0013\n",
      "acc_train:0.6500 pre_train:0.6838 recall_train:0.6000 F1_train:0.6392 AUC_train:0.6858\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.6756\n",
      "Epoch:0014\n",
      "acc_train:0.6556 pre_train:0.6942 recall_train:0.5957 F1_train:0.6412 AUC_train:0.7184\n",
      "acc_val:0.5500 pre_val:0.5294 recall_val:0.9000 F1_val:0.666667 AUC_val:0.6824\n",
      "Epoch:0015\n",
      "acc_train:0.6522 pre_train:0.6891 recall_train:0.5957 F1_train:0.6390 AUC_train:0.7171\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.9800 F1_val:0.662162 AUC_val:0.6908\n",
      "Epoch:0016\n",
      "acc_train:0.6744 pre_train:0.7129 recall_train:0.6194 F1_train:0.6628 AUC_train:0.7398\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6976\n",
      "Epoch:0017\n",
      "acc_train:0.6878 pre_train:0.7201 recall_train:0.6473 F1_train:0.6818 AUC_train:0.7499\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7016\n",
      "Epoch:0018\n",
      "acc_train:0.6789 pre_train:0.6888 recall_train:0.6903 F1_train:0.6896 AUC_train:0.7414\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7100\n",
      "Epoch:0019\n",
      "acc_train:0.6667 pre_train:0.6846 recall_train:0.6581 F1_train:0.6711 AUC_train:0.7320\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7120\n",
      "Epoch:0020\n",
      "acc_train:0.6833 pre_train:0.7083 recall_train:0.6581 F1_train:0.6823 AUC_train:0.7615\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7196\n",
      "Epoch:0021\n",
      "acc_train:0.6756 pre_train:0.7045 recall_train:0.6409 F1_train:0.6712 AUC_train:0.7716\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7284\n",
      "Epoch:0022\n",
      "acc_train:0.7044 pre_train:0.7319 recall_train:0.6753 F1_train:0.7025 AUC_train:0.7741\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7372\n",
      "Epoch:0023\n",
      "acc_train:0.6900 pre_train:0.7022 recall_train:0.6946 F1_train:0.6984 AUC_train:0.7756\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7320\n",
      "Epoch:0024\n",
      "acc_train:0.7211 pre_train:0.7211 recall_train:0.7505 F1_train:0.7355 AUC_train:0.8061\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7384\n",
      "Epoch:0025\n",
      "acc_train:0.7289 pre_train:0.7472 recall_train:0.7183 F1_train:0.7325 AUC_train:0.8159\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7532\n",
      "Epoch:0026\n",
      "acc_train:0.7678 pre_train:0.7678 recall_train:0.7892 F1_train:0.7784 AUC_train:0.8408\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7704\n",
      "Epoch:0027\n",
      "acc_train:0.7978 pre_train:0.7836 recall_train:0.8409 F1_train:0.8112 AUC_train:0.8601\n",
      "acc_val:0.4900 pre_val:0.4949 recall_val:0.9800 F1_val:0.657718 AUC_val:0.7880\n",
      "Epoch:0028\n",
      "acc_train:0.7767 pre_train:0.7609 recall_train:0.8280 F1_train:0.7930 AUC_train:0.8509\n",
      "acc_val:0.4900 pre_val:0.4948 recall_val:0.9600 F1_val:0.653061 AUC_val:0.7932\n",
      "Epoch:0029\n",
      "acc_train:0.7844 pre_train:0.7782 recall_train:0.8151 F1_train:0.7962 AUC_train:0.8813\n",
      "acc_val:0.5100 pre_val:0.5053 recall_val:0.9600 F1_val:0.662069 AUC_val:0.8024\n",
      "Epoch:0030\n",
      "acc_train:0.8156 pre_train:0.7984 recall_train:0.8602 F1_train:0.8282 AUC_train:0.8893\n",
      "acc_val:0.5600 pre_val:0.5341 recall_val:0.9400 F1_val:0.681159 AUC_val:0.8124\n",
      "Epoch:0031\n",
      "acc_train:0.8267 pre_train:0.8096 recall_train:0.8688 F1_train:0.8382 AUC_train:0.9061\n",
      "acc_val:0.5800 pre_val:0.5465 recall_val:0.9400 F1_val:0.691176 AUC_val:0.8380\n",
      "Epoch:0032\n",
      "acc_train:0.8700 pre_train:0.8580 recall_train:0.8968 F1_train:0.8770 AUC_train:0.9363\n",
      "acc_val:0.5900 pre_val:0.5529 recall_val:0.9400 F1_val:0.696296 AUC_val:0.8328\n",
      "Epoch:0033\n",
      "acc_train:0.8967 pre_train:0.8843 recall_train:0.9204 F1_train:0.9020 AUC_train:0.9529\n",
      "acc_val:0.6000 pre_val:0.5581 recall_val:0.9600 F1_val:0.705882 AUC_val:0.8228\n",
      "Epoch:0034\n",
      "acc_train:0.9033 pre_train:0.8765 recall_train:0.9462 F1_train:0.9100 AUC_train:0.9476\n",
      "acc_val:0.6200 pre_val:0.5732 recall_val:0.9400 F1_val:0.712121 AUC_val:0.8356\n",
      "Epoch:0035\n",
      "acc_train:0.9089 pre_train:0.8792 recall_train:0.9548 F1_train:0.9155 AUC_train:0.9582\n",
      "acc_val:0.6400 pre_val:0.5875 recall_val:0.9400 F1_val:0.723077 AUC_val:0.8336\n",
      "Epoch:0036\n",
      "acc_train:0.9111 pre_train:0.8953 recall_train:0.9376 F1_train:0.9160 AUC_train:0.9561\n",
      "acc_val:0.6400 pre_val:0.5897 recall_val:0.9200 F1_val:0.718750 AUC_val:0.8268\n",
      "Epoch:0037\n",
      "acc_train:0.9189 pre_train:0.8904 recall_train:0.9613 F1_train:0.9245 AUC_train:0.9664\n",
      "acc_val:0.6600 pre_val:0.6053 recall_val:0.9200 F1_val:0.730159 AUC_val:0.8136\n",
      "Epoch:0038\n",
      "acc_train:0.9100 pre_train:0.8840 recall_train:0.9505 F1_train:0.9161 AUC_train:0.9701\n",
      "acc_val:0.6600 pre_val:0.6053 recall_val:0.9200 F1_val:0.730159 AUC_val:0.8236\n",
      "Epoch:0039\n",
      "acc_train:0.9322 pre_train:0.9106 recall_train:0.9634 F1_train:0.9363 AUC_train:0.9781\n",
      "acc_val:0.6900 pre_val:0.6301 recall_val:0.9200 F1_val:0.747967 AUC_val:0.8472\n",
      "Epoch:0040\n",
      "acc_train:0.9222 pre_train:0.8990 recall_train:0.9570 F1_train:0.9271 AUC_train:0.9697\n",
      "acc_val:0.7200 pre_val:0.6571 recall_val:0.9200 F1_val:0.766667 AUC_val:0.8568\n",
      "Epoch:0041\n",
      "acc_train:0.9444 pre_train:0.9226 recall_train:0.9742 F1_train:0.9477 AUC_train:0.9822\n",
      "acc_val:0.7100 pre_val:0.6522 recall_val:0.9000 F1_val:0.756303 AUC_val:0.8628\n",
      "Epoch:0042\n",
      "acc_train:0.9256 pre_train:0.8948 recall_train:0.9699 F1_train:0.9309 AUC_train:0.9802\n",
      "acc_val:0.7300 pre_val:0.6667 recall_val:0.9200 F1_val:0.773109 AUC_val:0.8616\n",
      "Epoch:0043\n",
      "acc_train:0.9267 pre_train:0.9030 recall_train:0.9613 F1_train:0.9313 AUC_train:0.9842\n",
      "acc_val:0.7200 pre_val:0.6618 recall_val:0.9000 F1_val:0.762712 AUC_val:0.8592\n",
      "Epoch:0044\n",
      "acc_train:0.9256 pre_train:0.8933 recall_train:0.9720 F1_train:0.9310 AUC_train:0.9858\n",
      "acc_val:0.7300 pre_val:0.6825 recall_val:0.8600 F1_val:0.761062 AUC_val:0.8532\n",
      "Epoch:0045\n",
      "acc_train:0.9411 pre_train:0.9221 recall_train:0.9677 F1_train:0.9444 AUC_train:0.9824\n",
      "acc_val:0.7600 pre_val:0.7321 recall_val:0.8200 F1_val:0.773585 AUC_val:0.8488\n",
      "Epoch:0046\n",
      "acc_train:0.9344 pre_train:0.9060 recall_train:0.9742 F1_train:0.9389 AUC_train:0.9887\n",
      "acc_val:0.8100 pre_val:0.8039 recall_val:0.8200 F1_val:0.811881 AUC_val:0.8576\n",
      "Epoch:0047\n",
      "acc_train:0.9467 pre_train:0.9317 recall_train:0.9677 F1_train:0.9494 AUC_train:0.9865\n",
      "acc_val:0.8100 pre_val:0.8163 recall_val:0.8000 F1_val:0.808081 AUC_val:0.8684\n",
      "Epoch:0048\n",
      "acc_train:0.9511 pre_train:0.9287 recall_train:0.9806 F1_train:0.9540 AUC_train:0.9865\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8732\n",
      "Epoch:0049\n",
      "acc_train:0.9544 pre_train:0.9309 recall_train:0.9849 F1_train:0.9572 AUC_train:0.9904\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8776\n",
      "Epoch:0050\n",
      "acc_train:0.9522 pre_train:0.9306 recall_train:0.9806 F1_train:0.9550 AUC_train:0.9901\n",
      "acc_val:0.8100 pre_val:0.8039 recall_val:0.8200 F1_val:0.811881 AUC_val:0.8712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0051\n",
      "acc_train:0.9544 pre_train:0.9327 recall_train:0.9828 F1_train:0.9571 AUC_train:0.9919\n",
      "acc_val:0.8000 pre_val:0.7885 recall_val:0.8200 F1_val:0.803922 AUC_val:0.8652\n",
      "Epoch:0052\n",
      "acc_train:0.9400 pre_train:0.9290 recall_train:0.9570 F1_train:0.9428 AUC_train:0.9841\n",
      "acc_val:0.8500 pre_val:0.8723 recall_val:0.8200 F1_val:0.845361 AUC_val:0.8824\n",
      "Epoch:0053\n",
      "acc_train:0.9622 pre_train:0.9518 recall_train:0.9763 F1_train:0.9639 AUC_train:0.9942\n",
      "acc_val:0.8300 pre_val:0.9024 recall_val:0.7400 F1_val:0.813187 AUC_val:0.8884\n",
      "Epoch:0054\n",
      "acc_train:0.9511 pre_train:0.9340 recall_train:0.9742 F1_train:0.9537 AUC_train:0.9884\n",
      "acc_val:0.8400 pre_val:0.9048 recall_val:0.7600 F1_val:0.826087 AUC_val:0.8904\n",
      "Epoch:0055\n",
      "acc_train:0.9489 pre_train:0.9374 recall_train:0.9656 F1_train:0.9513 AUC_train:0.9900\n",
      "acc_val:0.8600 pre_val:0.8750 recall_val:0.8400 F1_val:0.857143 AUC_val:0.8872\n",
      "Epoch:0056\n",
      "acc_train:0.9667 pre_train:0.9485 recall_train:0.9892 F1_train:0.9684 AUC_train:0.9910\n",
      "acc_val:0.7900 pre_val:0.7736 recall_val:0.8200 F1_val:0.796117 AUC_val:0.8820\n",
      "Epoch:0057\n",
      "acc_train:0.9733 pre_train:0.9662 recall_train:0.9828 F1_train:0.9744 AUC_train:0.9945\n",
      "acc_val:0.7700 pre_val:0.7455 recall_val:0.8200 F1_val:0.780952 AUC_val:0.8776\n",
      "Epoch:0058\n",
      "acc_train:0.9622 pre_train:0.9462 recall_train:0.9828 F1_train:0.9641 AUC_train:0.9886\n",
      "acc_val:0.7700 pre_val:0.7455 recall_val:0.8200 F1_val:0.780952 AUC_val:0.8768\n",
      "Epoch:0059\n",
      "acc_train:0.9711 pre_train:0.9563 recall_train:0.9892 F1_train:0.9725 AUC_train:0.9942\n",
      "acc_val:0.8000 pre_val:0.8000 recall_val:0.8000 F1_val:0.800000 AUC_val:0.8824\n",
      "Epoch:0060\n",
      "acc_train:0.9733 pre_train:0.9662 recall_train:0.9828 F1_train:0.9744 AUC_train:0.9959\n",
      "acc_val:0.8400 pre_val:0.8542 recall_val:0.8200 F1_val:0.836735 AUC_val:0.8856\n",
      "Epoch:0061\n",
      "acc_train:0.9678 pre_train:0.9523 recall_train:0.9871 F1_train:0.9694 AUC_train:0.9928\n",
      "acc_val:0.8300 pre_val:0.8367 recall_val:0.8200 F1_val:0.828283 AUC_val:0.8800\n",
      "Epoch:0062\n",
      "acc_train:0.9656 pre_train:0.9597 recall_train:0.9742 F1_train:0.9669 AUC_train:0.9921\n",
      "acc_val:0.8200 pre_val:0.8200 recall_val:0.8200 F1_val:0.820000 AUC_val:0.8808\n",
      "Epoch:0063\n",
      "acc_train:0.9722 pre_train:0.9622 recall_train:0.9849 F1_train:0.9734 AUC_train:0.9926\n",
      "acc_val:0.7900 pre_val:0.7736 recall_val:0.8200 F1_val:0.796117 AUC_val:0.8812\n",
      "Epoch:0064\n",
      "acc_train:0.9756 pre_train:0.9743 recall_train:0.9785 F1_train:0.9764 AUC_train:0.9950\n",
      "acc_val:0.7700 pre_val:0.7455 recall_val:0.8200 F1_val:0.780952 AUC_val:0.8836\n",
      "Epoch:0065\n",
      "acc_train:0.9656 pre_train:0.9502 recall_train:0.9849 F1_train:0.9673 AUC_train:0.9972\n",
      "acc_val:0.8000 pre_val:0.7885 recall_val:0.8200 F1_val:0.803922 AUC_val:0.8872\n",
      "Epoch:0066\n",
      "acc_train:0.9689 pre_train:0.9619 recall_train:0.9785 F1_train:0.9701 AUC_train:0.9961\n",
      "acc_val:0.8300 pre_val:0.8235 recall_val:0.8400 F1_val:0.831683 AUC_val:0.8904\n",
      "Epoch:0067\n",
      "acc_train:0.9733 pre_train:0.9642 recall_train:0.9849 F1_train:0.9745 AUC_train:0.9950\n",
      "acc_val:0.8300 pre_val:0.8235 recall_val:0.8400 F1_val:0.831683 AUC_val:0.8932\n",
      "Epoch:0068\n",
      "acc_train:0.9789 pre_train:0.9685 recall_train:0.9914 F1_train:0.9798 AUC_train:0.9977\n",
      "acc_val:0.8300 pre_val:0.8235 recall_val:0.8400 F1_val:0.831683 AUC_val:0.8932\n",
      "Epoch:0069\n",
      "acc_train:0.9833 pre_train:0.9787 recall_train:0.9892 F1_train:0.9840 AUC_train:0.9984\n",
      "acc_val:0.8500 pre_val:0.8431 recall_val:0.8600 F1_val:0.851485 AUC_val:0.8996\n",
      "Epoch:0070\n",
      "acc_train:0.9744 pre_train:0.9662 recall_train:0.9849 F1_train:0.9755 AUC_train:0.9954\n",
      "acc_val:0.8600 pre_val:0.8750 recall_val:0.8400 F1_val:0.857143 AUC_val:0.8988\n",
      "Epoch:0071\n",
      "acc_train:0.9800 pre_train:0.9725 recall_train:0.9892 F1_train:0.9808 AUC_train:0.9959\n",
      "acc_val:0.8600 pre_val:0.8750 recall_val:0.8400 F1_val:0.857143 AUC_val:0.8992\n",
      "Epoch:0072\n",
      "acc_train:0.9811 pre_train:0.9726 recall_train:0.9914 F1_train:0.9819 AUC_train:0.9953\n",
      "acc_val:0.8300 pre_val:0.8235 recall_val:0.8400 F1_val:0.831683 AUC_val:0.9008\n",
      "Epoch:0073\n",
      "acc_train:0.9767 pre_train:0.9664 recall_train:0.9892 F1_train:0.9777 AUC_train:0.9934\n",
      "acc_val:0.7900 pre_val:0.7544 recall_val:0.8600 F1_val:0.803738 AUC_val:0.8976\n",
      "Epoch:0074\n",
      "acc_train:0.9833 pre_train:0.9767 recall_train:0.9914 F1_train:0.9840 AUC_train:0.9977\n",
      "acc_val:0.8000 pre_val:0.7586 recall_val:0.8800 F1_val:0.814815 AUC_val:0.8852\n",
      "Epoch:0075\n",
      "acc_train:0.9844 pre_train:0.9788 recall_train:0.9914 F1_train:0.9850 AUC_train:0.9973\n",
      "acc_val:0.7700 pre_val:0.7288 recall_val:0.8600 F1_val:0.788991 AUC_val:0.8776\n",
      "Epoch:0076\n",
      "acc_train:0.9800 pre_train:0.9725 recall_train:0.9892 F1_train:0.9808 AUC_train:0.9971\n",
      "acc_val:0.7700 pre_val:0.7288 recall_val:0.8600 F1_val:0.788991 AUC_val:0.8772\n",
      "Epoch:0077\n",
      "acc_train:0.9744 pre_train:0.9682 recall_train:0.9828 F1_train:0.9755 AUC_train:0.9970\n",
      "acc_val:0.7900 pre_val:0.7544 recall_val:0.8600 F1_val:0.803738 AUC_val:0.8900\n",
      "Epoch:0078\n",
      "acc_train:0.9822 pre_train:0.9746 recall_train:0.9914 F1_train:0.9829 AUC_train:0.9983\n",
      "acc_val:0.8300 pre_val:0.8113 recall_val:0.8600 F1_val:0.834951 AUC_val:0.8992\n",
      "Epoch:0079\n",
      "acc_train:0.9822 pre_train:0.9746 recall_train:0.9914 F1_train:0.9829 AUC_train:0.9979\n",
      "acc_val:0.8300 pre_val:0.8113 recall_val:0.8600 F1_val:0.834951 AUC_val:0.8980\n",
      "Epoch:0080\n",
      "acc_train:0.9822 pre_train:0.9706 recall_train:0.9957 F1_train:0.9830 AUC_train:0.9970\n",
      "acc_val:0.8200 pre_val:0.7857 recall_val:0.8800 F1_val:0.830189 AUC_val:0.8992\n",
      "Epoch:0081\n",
      "acc_train:0.9778 pre_train:0.9665 recall_train:0.9914 F1_train:0.9788 AUC_train:0.9976\n",
      "acc_val:0.7700 pre_val:0.7213 recall_val:0.8800 F1_val:0.792793 AUC_val:0.9012\n",
      "Epoch:0082\n",
      "acc_train:0.9867 pre_train:0.9789 recall_train:0.9957 F1_train:0.9872 AUC_train:0.9983\n",
      "acc_val:0.7500 pre_val:0.6984 recall_val:0.8800 F1_val:0.778761 AUC_val:0.8904\n",
      "Epoch:0083\n",
      "acc_train:0.9811 pre_train:0.9686 recall_train:0.9957 F1_train:0.9820 AUC_train:0.9985\n",
      "acc_val:0.7500 pre_val:0.6984 recall_val:0.8800 F1_val:0.778761 AUC_val:0.8808\n",
      "Epoch:0084\n",
      "acc_train:0.9833 pre_train:0.9767 recall_train:0.9914 F1_train:0.9840 AUC_train:0.9975\n",
      "acc_val:0.7900 pre_val:0.7544 recall_val:0.8600 F1_val:0.803738 AUC_val:0.8896\n",
      "Epoch:0085\n",
      "acc_train:0.9889 pre_train:0.9872 recall_train:0.9914 F1_train:0.9893 AUC_train:0.9984\n",
      "acc_val:0.8300 pre_val:0.8235 recall_val:0.8400 F1_val:0.831683 AUC_val:0.8944\n",
      "Epoch:0086\n",
      "acc_train:0.9856 pre_train:0.9809 recall_train:0.9914 F1_train:0.9861 AUC_train:0.9980\n",
      "acc_val:0.8500 pre_val:0.8723 recall_val:0.8200 F1_val:0.845361 AUC_val:0.8952\n",
      "Epoch:0087\n",
      "acc_train:0.9833 pre_train:0.9828 recall_train:0.9849 F1_train:0.9839 AUC_train:0.9982\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8972\n",
      "Epoch:0088\n",
      "acc_train:0.9822 pre_train:0.9787 recall_train:0.9871 F1_train:0.9829 AUC_train:0.9975\n",
      "acc_val:0.8000 pre_val:0.7679 recall_val:0.8600 F1_val:0.811321 AUC_val:0.8936\n",
      "Epoch:0089\n",
      "acc_train:0.9878 pre_train:0.9850 recall_train:0.9914 F1_train:0.9882 AUC_train:0.9984\n",
      "acc_val:0.7900 pre_val:0.7458 recall_val:0.8800 F1_val:0.807339 AUC_val:0.8876\n",
      "Epoch:0090\n",
      "acc_train:0.9867 pre_train:0.9809 recall_train:0.9935 F1_train:0.9872 AUC_train:0.9987\n",
      "acc_val:0.7600 pre_val:0.7031 recall_val:0.9000 F1_val:0.789474 AUC_val:0.8820\n",
      "Epoch:0091\n",
      "acc_train:0.9833 pre_train:0.9767 recall_train:0.9914 F1_train:0.9840 AUC_train:0.9981\n",
      "acc_val:0.7500 pre_val:0.6923 recall_val:0.9000 F1_val:0.782609 AUC_val:0.8836\n",
      "Epoch:0092\n",
      "acc_train:0.9867 pre_train:0.9809 recall_train:0.9935 F1_train:0.9872 AUC_train:0.9992\n",
      "acc_val:0.7700 pre_val:0.7143 recall_val:0.9000 F1_val:0.796460 AUC_val:0.8892\n",
      "Epoch:0093\n",
      "acc_train:0.9878 pre_train:0.9913 recall_train:0.9849 F1_train:0.9881 AUC_train:0.9993\n",
      "acc_val:0.7800 pre_val:0.7333 recall_val:0.8800 F1_val:0.800000 AUC_val:0.8976\n",
      "Epoch:0094\n",
      "acc_train:0.9911 pre_train:0.9872 recall_train:0.9957 F1_train:0.9914 AUC_train:0.9994\n",
      "acc_val:0.8100 pre_val:0.7719 recall_val:0.8800 F1_val:0.822430 AUC_val:0.8940\n",
      "Epoch:0095\n",
      "acc_train:0.9844 pre_train:0.9808 recall_train:0.9892 F1_train:0.9850 AUC_train:0.9989\n",
      "acc_val:0.8000 pre_val:0.7586 recall_val:0.8800 F1_val:0.814815 AUC_val:0.8956\n",
      "Epoch:0096\n",
      "acc_train:0.9878 pre_train:0.9789 recall_train:0.9978 F1_train:0.9883 AUC_train:0.9987\n",
      "acc_val:0.7600 pre_val:0.7097 recall_val:0.8800 F1_val:0.785714 AUC_val:0.8924\n",
      "Epoch:0097\n",
      "acc_train:0.9856 pre_train:0.9768 recall_train:0.9957 F1_train:0.9862 AUC_train:0.9991\n",
      "acc_val:0.7700 pre_val:0.7213 recall_val:0.8800 F1_val:0.792793 AUC_val:0.8864\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0098\n",
      "acc_train:0.9822 pre_train:0.9726 recall_train:0.9935 F1_train:0.9830 AUC_train:0.9972\n",
      "acc_val:0.7400 pre_val:0.6875 recall_val:0.8800 F1_val:0.771930 AUC_val:0.8772\n",
      "Epoch:0099\n",
      "acc_train:0.9933 pre_train:0.9873 recall_train:1.0000 F1_train:0.9936 AUC_train:0.9997\n",
      "acc_val:0.7500 pre_val:0.6984 recall_val:0.8800 F1_val:0.778761 AUC_val:0.8732\n",
      "Early Stopping!!! epoch：98\n",
      " Starting the 1-2 Fold:：\n",
      "Fitting estimator with 76636 features.\n",
      "Fitting estimator with 76536 features.\n",
      "Fitting estimator with 76436 features.\n",
      "Fitting estimator with 76336 features.\n",
      "Fitting estimator with 76236 features.\n",
      "Fitting estimator with 76136 features.\n",
      "Fitting estimator with 76036 features.\n",
      "Fitting estimator with 75936 features.\n",
      "Fitting estimator with 75836 features.\n",
      "Fitting estimator with 75736 features.\n",
      "Fitting estimator with 75636 features.\n",
      "Fitting estimator with 75536 features.\n",
      "Fitting estimator with 75436 features.\n",
      "Fitting estimator with 75336 features.\n",
      "Fitting estimator with 75236 features.\n",
      "Fitting estimator with 75136 features.\n",
      "Fitting estimator with 75036 features.\n",
      "Fitting estimator with 74936 features.\n",
      "Fitting estimator with 74836 features.\n",
      "Fitting estimator with 74736 features.\n",
      "Fitting estimator with 74636 features.\n",
      "Fitting estimator with 74536 features.\n",
      "Fitting estimator with 74436 features.\n",
      "Fitting estimator with 74336 features.\n",
      "Fitting estimator with 74236 features.\n",
      "Fitting estimator with 74136 features.\n",
      "Fitting estimator with 74036 features.\n",
      "Fitting estimator with 73936 features.\n",
      "Fitting estimator with 73836 features.\n",
      "Fitting estimator with 73736 features.\n",
      "Fitting estimator with 73636 features.\n",
      "Fitting estimator with 73536 features.\n",
      "Fitting estimator with 73436 features.\n",
      "Fitting estimator with 73336 features.\n",
      "Fitting estimator with 73236 features.\n",
      "Fitting estimator with 73136 features.\n",
      "Fitting estimator with 73036 features.\n",
      "Fitting estimator with 72936 features.\n",
      "Fitting estimator with 72836 features.\n",
      "Fitting estimator with 72736 features.\n",
      "Fitting estimator with 72636 features.\n",
      "Fitting estimator with 72536 features.\n",
      "Fitting estimator with 72436 features.\n",
      "Fitting estimator with 72336 features.\n",
      "Fitting estimator with 72236 features.\n",
      "Fitting estimator with 72136 features.\n",
      "Fitting estimator with 72036 features.\n",
      "Fitting estimator with 71936 features.\n",
      "Fitting estimator with 71836 features.\n",
      "Fitting estimator with 71736 features.\n",
      "Fitting estimator with 71636 features.\n",
      "Fitting estimator with 71536 features.\n",
      "Fitting estimator with 71436 features.\n",
      "Fitting estimator with 71336 features.\n",
      "Fitting estimator with 71236 features.\n",
      "Fitting estimator with 71136 features.\n",
      "Fitting estimator with 71036 features.\n",
      "Fitting estimator with 70936 features.\n",
      "Fitting estimator with 70836 features.\n",
      "Fitting estimator with 70736 features.\n",
      "Fitting estimator with 70636 features.\n",
      "Fitting estimator with 70536 features.\n",
      "Fitting estimator with 70436 features.\n",
      "Fitting estimator with 70336 features.\n",
      "Fitting estimator with 70236 features.\n",
      "Fitting estimator with 70136 features.\n",
      "Fitting estimator with 70036 features.\n",
      "Fitting estimator with 69936 features.\n",
      "Fitting estimator with 69836 features.\n",
      "Fitting estimator with 69736 features.\n",
      "Fitting estimator with 69636 features.\n",
      "Fitting estimator with 69536 features.\n",
      "Fitting estimator with 69436 features.\n",
      "Fitting estimator with 69336 features.\n",
      "Fitting estimator with 69236 features.\n",
      "Fitting estimator with 69136 features.\n",
      "Fitting estimator with 69036 features.\n",
      "Fitting estimator with 68936 features.\n",
      "Fitting estimator with 68836 features.\n",
      "Fitting estimator with 68736 features.\n",
      "Fitting estimator with 68636 features.\n",
      "Fitting estimator with 68536 features.\n",
      "Fitting estimator with 68436 features.\n",
      "Fitting estimator with 68336 features.\n",
      "Fitting estimator with 68236 features.\n",
      "Fitting estimator with 68136 features.\n",
      "Fitting estimator with 68036 features.\n",
      "Fitting estimator with 67936 features.\n",
      "Fitting estimator with 67836 features.\n",
      "Fitting estimator with 67736 features.\n",
      "Fitting estimator with 67636 features.\n",
      "Fitting estimator with 67536 features.\n",
      "Fitting estimator with 67436 features.\n",
      "Fitting estimator with 67336 features.\n",
      "Fitting estimator with 67236 features.\n",
      "Fitting estimator with 67136 features.\n",
      "Fitting estimator with 67036 features.\n",
      "Fitting estimator with 66936 features.\n",
      "Fitting estimator with 66836 features.\n",
      "Fitting estimator with 66736 features.\n",
      "Fitting estimator with 66636 features.\n",
      "Fitting estimator with 66536 features.\n",
      "Fitting estimator with 66436 features.\n",
      "Fitting estimator with 66336 features.\n",
      "Fitting estimator with 66236 features.\n",
      "Fitting estimator with 66136 features.\n",
      "Fitting estimator with 66036 features.\n",
      "Fitting estimator with 65936 features.\n",
      "Fitting estimator with 65836 features.\n",
      "Fitting estimator with 65736 features.\n",
      "Fitting estimator with 65636 features.\n",
      "Fitting estimator with 65536 features.\n",
      "Fitting estimator with 65436 features.\n",
      "Fitting estimator with 65336 features.\n",
      "Fitting estimator with 65236 features.\n",
      "Fitting estimator with 65136 features.\n",
      "Fitting estimator with 65036 features.\n",
      "Fitting estimator with 64936 features.\n",
      "Fitting estimator with 64836 features.\n",
      "Fitting estimator with 64736 features.\n",
      "Fitting estimator with 64636 features.\n",
      "Fitting estimator with 64536 features.\n",
      "Fitting estimator with 64436 features.\n",
      "Fitting estimator with 64336 features.\n",
      "Fitting estimator with 64236 features.\n",
      "Fitting estimator with 64136 features.\n",
      "Fitting estimator with 64036 features.\n",
      "Fitting estimator with 63936 features.\n",
      "Fitting estimator with 63836 features.\n",
      "Fitting estimator with 63736 features.\n",
      "Fitting estimator with 63636 features.\n",
      "Fitting estimator with 63536 features.\n",
      "Fitting estimator with 63436 features.\n",
      "Fitting estimator with 63336 features.\n",
      "Fitting estimator with 63236 features.\n",
      "Fitting estimator with 63136 features.\n",
      "Fitting estimator with 63036 features.\n",
      "Fitting estimator with 62936 features.\n",
      "Fitting estimator with 62836 features.\n",
      "Fitting estimator with 62736 features.\n",
      "Fitting estimator with 62636 features.\n",
      "Fitting estimator with 62536 features.\n",
      "Fitting estimator with 62436 features.\n",
      "Fitting estimator with 62336 features.\n",
      "Fitting estimator with 62236 features.\n",
      "Fitting estimator with 62136 features.\n",
      "Fitting estimator with 62036 features.\n",
      "Fitting estimator with 61936 features.\n",
      "Fitting estimator with 61836 features.\n",
      "Fitting estimator with 61736 features.\n",
      "Fitting estimator with 61636 features.\n",
      "Fitting estimator with 61536 features.\n",
      "Fitting estimator with 61436 features.\n",
      "Fitting estimator with 61336 features.\n",
      "Fitting estimator with 61236 features.\n",
      "Fitting estimator with 61136 features.\n",
      "Fitting estimator with 61036 features.\n",
      "Fitting estimator with 60936 features.\n",
      "Fitting estimator with 60836 features.\n",
      "Fitting estimator with 60736 features.\n",
      "Fitting estimator with 60636 features.\n",
      "Fitting estimator with 60536 features.\n",
      "Fitting estimator with 60436 features.\n",
      "Fitting estimator with 60336 features.\n",
      "Fitting estimator with 60236 features.\n",
      "Fitting estimator with 60136 features.\n",
      "Fitting estimator with 60036 features.\n",
      "Fitting estimator with 59936 features.\n",
      "Fitting estimator with 59836 features.\n",
      "Fitting estimator with 59736 features.\n",
      "Fitting estimator with 59636 features.\n",
      "Fitting estimator with 59536 features.\n",
      "Fitting estimator with 59436 features.\n",
      "Fitting estimator with 59336 features.\n",
      "Fitting estimator with 59236 features.\n",
      "Fitting estimator with 59136 features.\n",
      "Fitting estimator with 59036 features.\n",
      "Fitting estimator with 58936 features.\n",
      "Fitting estimator with 58836 features.\n",
      "Fitting estimator with 58736 features.\n",
      "Fitting estimator with 58636 features.\n",
      "Fitting estimator with 58536 features.\n",
      "Fitting estimator with 58436 features.\n",
      "Fitting estimator with 58336 features.\n",
      "Fitting estimator with 58236 features.\n",
      "Fitting estimator with 58136 features.\n",
      "Fitting estimator with 58036 features.\n",
      "Fitting estimator with 57936 features.\n",
      "Fitting estimator with 57836 features.\n",
      "Fitting estimator with 57736 features.\n",
      "Fitting estimator with 57636 features.\n",
      "Fitting estimator with 57536 features.\n",
      "Fitting estimator with 57436 features.\n",
      "Fitting estimator with 57336 features.\n",
      "Fitting estimator with 57236 features.\n",
      "Fitting estimator with 57136 features.\n",
      "Fitting estimator with 57036 features.\n",
      "Fitting estimator with 56936 features.\n",
      "Fitting estimator with 56836 features.\n",
      "Fitting estimator with 56736 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 56636 features.\n",
      "Fitting estimator with 56536 features.\n",
      "Fitting estimator with 56436 features.\n",
      "Fitting estimator with 56336 features.\n",
      "Fitting estimator with 56236 features.\n",
      "Fitting estimator with 56136 features.\n",
      "Fitting estimator with 56036 features.\n",
      "Fitting estimator with 55936 features.\n",
      "Fitting estimator with 55836 features.\n",
      "Fitting estimator with 55736 features.\n",
      "Fitting estimator with 55636 features.\n",
      "Fitting estimator with 55536 features.\n",
      "Fitting estimator with 55436 features.\n",
      "Fitting estimator with 55336 features.\n",
      "Fitting estimator with 55236 features.\n",
      "Fitting estimator with 55136 features.\n",
      "Fitting estimator with 55036 features.\n",
      "Fitting estimator with 54936 features.\n",
      "Fitting estimator with 54836 features.\n",
      "Fitting estimator with 54736 features.\n",
      "Fitting estimator with 54636 features.\n",
      "Fitting estimator with 54536 features.\n",
      "Fitting estimator with 54436 features.\n",
      "Fitting estimator with 54336 features.\n",
      "Fitting estimator with 54236 features.\n",
      "Fitting estimator with 54136 features.\n",
      "Fitting estimator with 54036 features.\n",
      "Fitting estimator with 53936 features.\n",
      "Fitting estimator with 53836 features.\n",
      "Fitting estimator with 53736 features.\n",
      "Fitting estimator with 53636 features.\n",
      "Fitting estimator with 53536 features.\n",
      "Fitting estimator with 53436 features.\n",
      "Fitting estimator with 53336 features.\n",
      "Fitting estimator with 53236 features.\n",
      "Fitting estimator with 53136 features.\n",
      "Fitting estimator with 53036 features.\n",
      "Fitting estimator with 52936 features.\n",
      "Fitting estimator with 52836 features.\n",
      "Fitting estimator with 52736 features.\n",
      "Fitting estimator with 52636 features.\n",
      "Fitting estimator with 52536 features.\n",
      "Fitting estimator with 52436 features.\n",
      "Fitting estimator with 52336 features.\n",
      "Fitting estimator with 52236 features.\n",
      "Fitting estimator with 52136 features.\n",
      "Fitting estimator with 52036 features.\n",
      "Fitting estimator with 51936 features.\n",
      "Fitting estimator with 51836 features.\n",
      "Fitting estimator with 51736 features.\n",
      "Fitting estimator with 51636 features.\n",
      "Fitting estimator with 51536 features.\n",
      "Fitting estimator with 51436 features.\n",
      "Fitting estimator with 51336 features.\n",
      "Fitting estimator with 51236 features.\n",
      "Fitting estimator with 51136 features.\n",
      "Fitting estimator with 51036 features.\n",
      "Fitting estimator with 50936 features.\n",
      "Fitting estimator with 50836 features.\n",
      "Fitting estimator with 50736 features.\n",
      "Fitting estimator with 50636 features.\n",
      "Fitting estimator with 50536 features.\n",
      "Fitting estimator with 50436 features.\n",
      "Fitting estimator with 50336 features.\n",
      "Fitting estimator with 50236 features.\n",
      "Fitting estimator with 50136 features.\n",
      "Fitting estimator with 50036 features.\n",
      "Fitting estimator with 49936 features.\n",
      "Fitting estimator with 49836 features.\n",
      "Fitting estimator with 49736 features.\n",
      "Fitting estimator with 49636 features.\n",
      "Fitting estimator with 49536 features.\n",
      "Fitting estimator with 49436 features.\n",
      "Fitting estimator with 49336 features.\n",
      "Fitting estimator with 49236 features.\n",
      "Fitting estimator with 49136 features.\n",
      "Fitting estimator with 49036 features.\n",
      "Fitting estimator with 48936 features.\n",
      "Fitting estimator with 48836 features.\n",
      "Fitting estimator with 48736 features.\n",
      "Fitting estimator with 48636 features.\n",
      "Fitting estimator with 48536 features.\n",
      "Fitting estimator with 48436 features.\n",
      "Fitting estimator with 48336 features.\n",
      "Fitting estimator with 48236 features.\n",
      "Fitting estimator with 48136 features.\n",
      "Fitting estimator with 48036 features.\n",
      "Fitting estimator with 47936 features.\n",
      "Fitting estimator with 47836 features.\n",
      "Fitting estimator with 47736 features.\n",
      "Fitting estimator with 47636 features.\n",
      "Fitting estimator with 47536 features.\n",
      "Fitting estimator with 47436 features.\n",
      "Fitting estimator with 47336 features.\n",
      "Fitting estimator with 47236 features.\n",
      "Fitting estimator with 47136 features.\n",
      "Fitting estimator with 47036 features.\n",
      "Fitting estimator with 46936 features.\n",
      "Fitting estimator with 46836 features.\n",
      "Fitting estimator with 46736 features.\n",
      "Fitting estimator with 46636 features.\n",
      "Fitting estimator with 46536 features.\n",
      "Fitting estimator with 46436 features.\n",
      "Fitting estimator with 46336 features.\n",
      "Fitting estimator with 46236 features.\n",
      "Fitting estimator with 46136 features.\n",
      "Fitting estimator with 46036 features.\n",
      "Fitting estimator with 45936 features.\n",
      "Fitting estimator with 45836 features.\n",
      "Fitting estimator with 45736 features.\n",
      "Fitting estimator with 45636 features.\n",
      "Fitting estimator with 45536 features.\n",
      "Fitting estimator with 45436 features.\n",
      "Fitting estimator with 45336 features.\n",
      "Fitting estimator with 45236 features.\n",
      "Fitting estimator with 45136 features.\n",
      "Fitting estimator with 45036 features.\n",
      "Fitting estimator with 44936 features.\n",
      "Fitting estimator with 44836 features.\n",
      "Fitting estimator with 44736 features.\n",
      "Fitting estimator with 44636 features.\n",
      "Fitting estimator with 44536 features.\n",
      "Fitting estimator with 44436 features.\n",
      "Fitting estimator with 44336 features.\n",
      "Fitting estimator with 44236 features.\n",
      "Fitting estimator with 44136 features.\n",
      "Fitting estimator with 44036 features.\n",
      "Fitting estimator with 43936 features.\n",
      "Fitting estimator with 43836 features.\n",
      "Fitting estimator with 43736 features.\n",
      "Fitting estimator with 43636 features.\n",
      "Fitting estimator with 43536 features.\n",
      "Fitting estimator with 43436 features.\n",
      "Fitting estimator with 43336 features.\n",
      "Fitting estimator with 43236 features.\n",
      "Fitting estimator with 43136 features.\n",
      "Fitting estimator with 43036 features.\n",
      "Fitting estimator with 42936 features.\n",
      "Fitting estimator with 42836 features.\n",
      "Fitting estimator with 42736 features.\n",
      "Fitting estimator with 42636 features.\n",
      "Fitting estimator with 42536 features.\n",
      "Fitting estimator with 42436 features.\n",
      "Fitting estimator with 42336 features.\n",
      "Fitting estimator with 42236 features.\n",
      "Fitting estimator with 42136 features.\n",
      "Fitting estimator with 42036 features.\n",
      "Fitting estimator with 41936 features.\n",
      "Fitting estimator with 41836 features.\n",
      "Fitting estimator with 41736 features.\n",
      "Fitting estimator with 41636 features.\n",
      "Fitting estimator with 41536 features.\n",
      "Fitting estimator with 41436 features.\n",
      "Fitting estimator with 41336 features.\n",
      "Fitting estimator with 41236 features.\n",
      "Fitting estimator with 41136 features.\n",
      "Fitting estimator with 41036 features.\n",
      "Fitting estimator with 40936 features.\n",
      "Fitting estimator with 40836 features.\n",
      "Fitting estimator with 40736 features.\n",
      "Fitting estimator with 40636 features.\n",
      "Fitting estimator with 40536 features.\n",
      "Fitting estimator with 40436 features.\n",
      "Fitting estimator with 40336 features.\n",
      "Fitting estimator with 40236 features.\n",
      "Fitting estimator with 40136 features.\n",
      "Fitting estimator with 40036 features.\n",
      "Fitting estimator with 39936 features.\n",
      "Fitting estimator with 39836 features.\n",
      "Fitting estimator with 39736 features.\n",
      "Fitting estimator with 39636 features.\n",
      "Fitting estimator with 39536 features.\n",
      "Fitting estimator with 39436 features.\n",
      "Fitting estimator with 39336 features.\n",
      "Fitting estimator with 39236 features.\n",
      "Fitting estimator with 39136 features.\n",
      "Fitting estimator with 39036 features.\n",
      "Fitting estimator with 38936 features.\n",
      "Fitting estimator with 38836 features.\n",
      "Fitting estimator with 38736 features.\n",
      "Fitting estimator with 38636 features.\n",
      "Fitting estimator with 38536 features.\n",
      "Fitting estimator with 38436 features.\n",
      "Fitting estimator with 38336 features.\n",
      "Fitting estimator with 38236 features.\n",
      "Fitting estimator with 38136 features.\n",
      "Fitting estimator with 38036 features.\n",
      "Fitting estimator with 37936 features.\n",
      "Fitting estimator with 37836 features.\n",
      "Fitting estimator with 37736 features.\n",
      "Fitting estimator with 37636 features.\n",
      "Fitting estimator with 37536 features.\n",
      "Fitting estimator with 37436 features.\n",
      "Fitting estimator with 37336 features.\n",
      "Fitting estimator with 37236 features.\n",
      "Fitting estimator with 37136 features.\n",
      "Fitting estimator with 37036 features.\n",
      "Fitting estimator with 36936 features.\n",
      "Fitting estimator with 36836 features.\n",
      "Fitting estimator with 36736 features.\n",
      "Fitting estimator with 36636 features.\n",
      "Fitting estimator with 36536 features.\n",
      "Fitting estimator with 36436 features.\n",
      "Fitting estimator with 36336 features.\n",
      "Fitting estimator with 36236 features.\n",
      "Fitting estimator with 36136 features.\n",
      "Fitting estimator with 36036 features.\n",
      "Fitting estimator with 35936 features.\n",
      "Fitting estimator with 35836 features.\n",
      "Fitting estimator with 35736 features.\n",
      "Fitting estimator with 35636 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 35536 features.\n",
      "Fitting estimator with 35436 features.\n",
      "Fitting estimator with 35336 features.\n",
      "Fitting estimator with 35236 features.\n",
      "Fitting estimator with 35136 features.\n",
      "Fitting estimator with 35036 features.\n",
      "Fitting estimator with 34936 features.\n",
      "Fitting estimator with 34836 features.\n",
      "Fitting estimator with 34736 features.\n",
      "Fitting estimator with 34636 features.\n",
      "Fitting estimator with 34536 features.\n",
      "Fitting estimator with 34436 features.\n",
      "Fitting estimator with 34336 features.\n",
      "Fitting estimator with 34236 features.\n",
      "Fitting estimator with 34136 features.\n",
      "Fitting estimator with 34036 features.\n",
      "Fitting estimator with 33936 features.\n",
      "Fitting estimator with 33836 features.\n",
      "Fitting estimator with 33736 features.\n",
      "Fitting estimator with 33636 features.\n",
      "Fitting estimator with 33536 features.\n",
      "Fitting estimator with 33436 features.\n",
      "Fitting estimator with 33336 features.\n",
      "Fitting estimator with 33236 features.\n",
      "Fitting estimator with 33136 features.\n",
      "Fitting estimator with 33036 features.\n",
      "Fitting estimator with 32936 features.\n",
      "Fitting estimator with 32836 features.\n",
      "Fitting estimator with 32736 features.\n",
      "Fitting estimator with 32636 features.\n",
      "Fitting estimator with 32536 features.\n",
      "Fitting estimator with 32436 features.\n",
      "Fitting estimator with 32336 features.\n",
      "Fitting estimator with 32236 features.\n",
      "Fitting estimator with 32136 features.\n",
      "Fitting estimator with 32036 features.\n",
      "Fitting estimator with 31936 features.\n",
      "Fitting estimator with 31836 features.\n",
      "Fitting estimator with 31736 features.\n",
      "Fitting estimator with 31636 features.\n",
      "Fitting estimator with 31536 features.\n",
      "Fitting estimator with 31436 features.\n",
      "Fitting estimator with 31336 features.\n",
      "Fitting estimator with 31236 features.\n",
      "Fitting estimator with 31136 features.\n",
      "Fitting estimator with 31036 features.\n",
      "Fitting estimator with 30936 features.\n",
      "Fitting estimator with 30836 features.\n",
      "Fitting estimator with 30736 features.\n",
      "Fitting estimator with 30636 features.\n",
      "Fitting estimator with 30536 features.\n",
      "Fitting estimator with 30436 features.\n",
      "Fitting estimator with 30336 features.\n",
      "Fitting estimator with 30236 features.\n",
      "Fitting estimator with 30136 features.\n",
      "Fitting estimator with 30036 features.\n",
      "Fitting estimator with 29936 features.\n",
      "Fitting estimator with 29836 features.\n",
      "Fitting estimator with 29736 features.\n",
      "Fitting estimator with 29636 features.\n",
      "Fitting estimator with 29536 features.\n",
      "Fitting estimator with 29436 features.\n",
      "Fitting estimator with 29336 features.\n",
      "Fitting estimator with 29236 features.\n",
      "Fitting estimator with 29136 features.\n",
      "Fitting estimator with 29036 features.\n",
      "Fitting estimator with 28936 features.\n",
      "Fitting estimator with 28836 features.\n",
      "Fitting estimator with 28736 features.\n",
      "Fitting estimator with 28636 features.\n",
      "Fitting estimator with 28536 features.\n",
      "Fitting estimator with 28436 features.\n",
      "Fitting estimator with 28336 features.\n",
      "Fitting estimator with 28236 features.\n",
      "Fitting estimator with 28136 features.\n",
      "Fitting estimator with 28036 features.\n",
      "Fitting estimator with 27936 features.\n",
      "Fitting estimator with 27836 features.\n",
      "Fitting estimator with 27736 features.\n",
      "Fitting estimator with 27636 features.\n",
      "Fitting estimator with 27536 features.\n",
      "Fitting estimator with 27436 features.\n",
      "Fitting estimator with 27336 features.\n",
      "Fitting estimator with 27236 features.\n",
      "Fitting estimator with 27136 features.\n",
      "Fitting estimator with 27036 features.\n",
      "Fitting estimator with 26936 features.\n",
      "Fitting estimator with 26836 features.\n",
      "Fitting estimator with 26736 features.\n",
      "Fitting estimator with 26636 features.\n",
      "Fitting estimator with 26536 features.\n",
      "Fitting estimator with 26436 features.\n",
      "Fitting estimator with 26336 features.\n",
      "Fitting estimator with 26236 features.\n",
      "Fitting estimator with 26136 features.\n",
      "Fitting estimator with 26036 features.\n",
      "Fitting estimator with 25936 features.\n",
      "Fitting estimator with 25836 features.\n",
      "Fitting estimator with 25736 features.\n",
      "Fitting estimator with 25636 features.\n",
      "Fitting estimator with 25536 features.\n",
      "Fitting estimator with 25436 features.\n",
      "Fitting estimator with 25336 features.\n",
      "Fitting estimator with 25236 features.\n",
      "Fitting estimator with 25136 features.\n",
      "Fitting estimator with 25036 features.\n",
      "Fitting estimator with 24936 features.\n",
      "Fitting estimator with 24836 features.\n",
      "Fitting estimator with 24736 features.\n",
      "Fitting estimator with 24636 features.\n",
      "Fitting estimator with 24536 features.\n",
      "Fitting estimator with 24436 features.\n",
      "Fitting estimator with 24336 features.\n",
      "Fitting estimator with 24236 features.\n",
      "Fitting estimator with 24136 features.\n",
      "Fitting estimator with 24036 features.\n",
      "Fitting estimator with 23936 features.\n",
      "Fitting estimator with 23836 features.\n",
      "Fitting estimator with 23736 features.\n",
      "Fitting estimator with 23636 features.\n",
      "Fitting estimator with 23536 features.\n",
      "Fitting estimator with 23436 features.\n",
      "Fitting estimator with 23336 features.\n",
      "Fitting estimator with 23236 features.\n",
      "Fitting estimator with 23136 features.\n",
      "Fitting estimator with 23036 features.\n",
      "Fitting estimator with 22936 features.\n",
      "Fitting estimator with 22836 features.\n",
      "Fitting estimator with 22736 features.\n",
      "Fitting estimator with 22636 features.\n",
      "Fitting estimator with 22536 features.\n",
      "Fitting estimator with 22436 features.\n",
      "Fitting estimator with 22336 features.\n",
      "Fitting estimator with 22236 features.\n",
      "Fitting estimator with 22136 features.\n",
      "Fitting estimator with 22036 features.\n",
      "Fitting estimator with 21936 features.\n",
      "Fitting estimator with 21836 features.\n",
      "Fitting estimator with 21736 features.\n",
      "Fitting estimator with 21636 features.\n",
      "Fitting estimator with 21536 features.\n",
      "Fitting estimator with 21436 features.\n",
      "Fitting estimator with 21336 features.\n",
      "Fitting estimator with 21236 features.\n",
      "Fitting estimator with 21136 features.\n",
      "Fitting estimator with 21036 features.\n",
      "Fitting estimator with 20936 features.\n",
      "Fitting estimator with 20836 features.\n",
      "Fitting estimator with 20736 features.\n",
      "Fitting estimator with 20636 features.\n",
      "Fitting estimator with 20536 features.\n",
      "Fitting estimator with 20436 features.\n",
      "Fitting estimator with 20336 features.\n",
      "Fitting estimator with 20236 features.\n",
      "Fitting estimator with 20136 features.\n",
      "Fitting estimator with 20036 features.\n",
      "Fitting estimator with 19936 features.\n",
      "Fitting estimator with 19836 features.\n",
      "Fitting estimator with 19736 features.\n",
      "Fitting estimator with 19636 features.\n",
      "Fitting estimator with 19536 features.\n",
      "Fitting estimator with 19436 features.\n",
      "Fitting estimator with 19336 features.\n",
      "Fitting estimator with 19236 features.\n",
      "Fitting estimator with 19136 features.\n",
      "Fitting estimator with 19036 features.\n",
      "Fitting estimator with 18936 features.\n",
      "Fitting estimator with 18836 features.\n",
      "Fitting estimator with 18736 features.\n",
      "Fitting estimator with 18636 features.\n",
      "Fitting estimator with 18536 features.\n",
      "Fitting estimator with 18436 features.\n",
      "Fitting estimator with 18336 features.\n",
      "Fitting estimator with 18236 features.\n",
      "Fitting estimator with 18136 features.\n",
      "Fitting estimator with 18036 features.\n",
      "Fitting estimator with 17936 features.\n",
      "Fitting estimator with 17836 features.\n",
      "Fitting estimator with 17736 features.\n",
      "Fitting estimator with 17636 features.\n",
      "Fitting estimator with 17536 features.\n",
      "Fitting estimator with 17436 features.\n",
      "Fitting estimator with 17336 features.\n",
      "Fitting estimator with 17236 features.\n",
      "Fitting estimator with 17136 features.\n",
      "Fitting estimator with 17036 features.\n",
      "Fitting estimator with 16936 features.\n",
      "Fitting estimator with 16836 features.\n",
      "Fitting estimator with 16736 features.\n",
      "Fitting estimator with 16636 features.\n",
      "Fitting estimator with 16536 features.\n",
      "Fitting estimator with 16436 features.\n",
      "Fitting estimator with 16336 features.\n",
      "Fitting estimator with 16236 features.\n",
      "Fitting estimator with 16136 features.\n",
      "Fitting estimator with 16036 features.\n",
      "Fitting estimator with 15936 features.\n",
      "Fitting estimator with 15836 features.\n",
      "Fitting estimator with 15736 features.\n",
      "Fitting estimator with 15636 features.\n",
      "Fitting estimator with 15536 features.\n",
      "Fitting estimator with 15436 features.\n",
      "Fitting estimator with 15336 features.\n",
      "Fitting estimator with 15236 features.\n",
      "Fitting estimator with 15136 features.\n",
      "Fitting estimator with 15036 features.\n",
      "Fitting estimator with 14936 features.\n",
      "Fitting estimator with 14836 features.\n",
      "Fitting estimator with 14736 features.\n",
      "Fitting estimator with 14636 features.\n",
      "Fitting estimator with 14536 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 14436 features.\n",
      "Fitting estimator with 14336 features.\n",
      "Fitting estimator with 14236 features.\n",
      "Fitting estimator with 14136 features.\n",
      "Fitting estimator with 14036 features.\n",
      "Fitting estimator with 13936 features.\n",
      "Fitting estimator with 13836 features.\n",
      "Fitting estimator with 13736 features.\n",
      "Fitting estimator with 13636 features.\n",
      "Fitting estimator with 13536 features.\n",
      "Fitting estimator with 13436 features.\n",
      "Fitting estimator with 13336 features.\n",
      "Fitting estimator with 13236 features.\n",
      "Fitting estimator with 13136 features.\n",
      "Fitting estimator with 13036 features.\n",
      "Fitting estimator with 12936 features.\n",
      "Fitting estimator with 12836 features.\n",
      "Fitting estimator with 12736 features.\n",
      "Fitting estimator with 12636 features.\n",
      "Fitting estimator with 12536 features.\n",
      "Fitting estimator with 12436 features.\n",
      "Fitting estimator with 12336 features.\n",
      "Fitting estimator with 12236 features.\n",
      "Fitting estimator with 12136 features.\n",
      "Fitting estimator with 12036 features.\n",
      "Fitting estimator with 11936 features.\n",
      "Fitting estimator with 11836 features.\n",
      "Fitting estimator with 11736 features.\n",
      "Fitting estimator with 11636 features.\n",
      "Fitting estimator with 11536 features.\n",
      "Fitting estimator with 11436 features.\n",
      "Fitting estimator with 11336 features.\n",
      "Fitting estimator with 11236 features.\n",
      "Fitting estimator with 11136 features.\n",
      "Fitting estimator with 11036 features.\n",
      "Fitting estimator with 10936 features.\n",
      "Fitting estimator with 10836 features.\n",
      "Fitting estimator with 10736 features.\n",
      "Fitting estimator with 10636 features.\n",
      "Fitting estimator with 10536 features.\n",
      "Fitting estimator with 10436 features.\n",
      "Fitting estimator with 10336 features.\n",
      "Fitting estimator with 10236 features.\n",
      "Fitting estimator with 10136 features.\n",
      "Fitting estimator with 10036 features.\n",
      "Fitting estimator with 9936 features.\n",
      "Fitting estimator with 9836 features.\n",
      "Fitting estimator with 9736 features.\n",
      "Fitting estimator with 9636 features.\n",
      "Fitting estimator with 9536 features.\n",
      "Fitting estimator with 9436 features.\n",
      "Fitting estimator with 9336 features.\n",
      "Fitting estimator with 9236 features.\n",
      "Fitting estimator with 9136 features.\n",
      "Fitting estimator with 9036 features.\n",
      "Fitting estimator with 8936 features.\n",
      "Fitting estimator with 8836 features.\n",
      "Fitting estimator with 8736 features.\n",
      "Fitting estimator with 8636 features.\n",
      "Fitting estimator with 8536 features.\n",
      "Fitting estimator with 8436 features.\n",
      "Fitting estimator with 8336 features.\n",
      "Fitting estimator with 8236 features.\n",
      "Fitting estimator with 8136 features.\n",
      "Fitting estimator with 8036 features.\n",
      "Fitting estimator with 7936 features.\n",
      "Fitting estimator with 7836 features.\n",
      "Fitting estimator with 7736 features.\n",
      "Fitting estimator with 7636 features.\n",
      "Fitting estimator with 7536 features.\n",
      "Fitting estimator with 7436 features.\n",
      "Fitting estimator with 7336 features.\n",
      "Fitting estimator with 7236 features.\n",
      "Fitting estimator with 7136 features.\n",
      "Fitting estimator with 7036 features.\n",
      "Fitting estimator with 6936 features.\n",
      "Fitting estimator with 6836 features.\n",
      "Fitting estimator with 6736 features.\n",
      "Fitting estimator with 6636 features.\n",
      "Fitting estimator with 6536 features.\n",
      "Fitting estimator with 6436 features.\n",
      "Fitting estimator with 6336 features.\n",
      "Fitting estimator with 6236 features.\n",
      "Fitting estimator with 6136 features.\n",
      "Fitting estimator with 6036 features.\n",
      "Fitting estimator with 5936 features.\n",
      "Fitting estimator with 5836 features.\n",
      "Fitting estimator with 5736 features.\n",
      "Fitting estimator with 5636 features.\n",
      "Fitting estimator with 5536 features.\n",
      "Fitting estimator with 5436 features.\n",
      "Fitting estimator with 5336 features.\n",
      "Fitting estimator with 5236 features.\n",
      "Fitting estimator with 5136 features.\n",
      "Fitting estimator with 5036 features.\n",
      "Fitting estimator with 4936 features.\n",
      "Fitting estimator with 4836 features.\n",
      "Fitting estimator with 4736 features.\n",
      "Fitting estimator with 4636 features.\n",
      "Fitting estimator with 4536 features.\n",
      "Fitting estimator with 4436 features.\n",
      "Fitting estimator with 4336 features.\n",
      "Fitting estimator with 4236 features.\n",
      "Fitting estimator with 4136 features.\n",
      "Fitting estimator with 4036 features.\n",
      "Fitting estimator with 3936 features.\n",
      "Fitting estimator with 3836 features.\n",
      "Fitting estimator with 3736 features.\n",
      "Fitting estimator with 3636 features.\n",
      "Fitting estimator with 3536 features.\n",
      "Fitting estimator with 3436 features.\n",
      "Fitting estimator with 3336 features.\n",
      "Fitting estimator with 3236 features.\n",
      "Fitting estimator with 3136 features.\n",
      "Fitting estimator with 3036 features.\n",
      "Fitting estimator with 2936 features.\n",
      "Fitting estimator with 2836 features.\n",
      "Fitting estimator with 2736 features.\n",
      "Fitting estimator with 2636 features.\n",
      "Fitting estimator with 2536 features.\n",
      "Fitting estimator with 2436 features.\n",
      "Fitting estimator with 2336 features.\n",
      "Fitting estimator with 2236 features.\n",
      "Fitting estimator with 2136 features.\n",
      "Fitting estimator with 2036 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001\n",
      "acc_train:0.4356 pre_train:0.4667 recall_train:0.6473 F1_train:0.5423 AUC_train:0.4271\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.3004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0002\n",
      "acc_train:0.4911 pre_train:0.5074 recall_train:0.5183 F1_train:0.5128 AUC_train:0.5075\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.3012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0003\n",
      "acc_train:0.5544 pre_train:0.5658 recall_train:0.5914 F1_train:0.5783 AUC_train:0.5822\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.3188\n",
      "Epoch:0004\n",
      "acc_train:0.5578 pre_train:0.5717 recall_train:0.5742 F1_train:0.5730 AUC_train:0.6083\n",
      "acc_val:0.4100 pre_val:0.3333 recall_val:0.1800 F1_val:0.233766 AUC_val:0.3212\n",
      "Epoch:0005\n",
      "acc_train:0.5833 pre_train:0.5983 recall_train:0.5892 F1_train:0.5937 AUC_train:0.6282\n",
      "acc_val:0.3500 pre_val:0.3913 recall_val:0.5400 F1_val:0.453782 AUC_val:0.3300\n",
      "Epoch:0006\n",
      "acc_train:0.5811 pre_train:0.5894 recall_train:0.6237 F1_train:0.6061 AUC_train:0.6209\n",
      "acc_val:0.3800 pre_val:0.4211 recall_val:0.6400 F1_val:0.507937 AUC_val:0.3308\n",
      "Epoch:0007\n",
      "acc_train:0.5956 pre_train:0.6095 recall_train:0.6043 F1_train:0.6069 AUC_train:0.6324\n",
      "acc_val:0.4000 pre_val:0.4375 recall_val:0.7000 F1_val:0.538462 AUC_val:0.3260\n",
      "Epoch:0008\n",
      "acc_train:0.6078 pre_train:0.6181 recall_train:0.6301 F1_train:0.6241 AUC_train:0.6237\n",
      "acc_val:0.4400 pre_val:0.4651 recall_val:0.8000 F1_val:0.588235 AUC_val:0.3146\n",
      "Epoch:0009\n",
      "acc_train:0.6056 pre_train:0.6160 recall_train:0.6280 F1_train:0.6219 AUC_train:0.6313\n",
      "acc_val:0.4600 pre_val:0.4773 recall_val:0.8400 F1_val:0.608696 AUC_val:0.3040\n",
      "Epoch:0010\n",
      "acc_train:0.6256 pre_train:0.6345 recall_train:0.6495 F1_train:0.6419 AUC_train:0.6520\n",
      "acc_val:0.4800 pre_val:0.4889 recall_val:0.8800 F1_val:0.628571 AUC_val:0.2976\n",
      "Epoch:0011\n",
      "acc_train:0.6000 pre_train:0.6015 recall_train:0.6688 F1_train:0.6334 AUC_train:0.6430\n",
      "acc_val:0.4800 pre_val:0.4891 recall_val:0.9000 F1_val:0.633803 AUC_val:0.3020\n",
      "Epoch:0012\n",
      "acc_train:0.5878 pre_train:0.5929 recall_train:0.6452 F1_train:0.6179 AUC_train:0.6313\n",
      "acc_val:0.4800 pre_val:0.4891 recall_val:0.9000 F1_val:0.633803 AUC_val:0.3068\n",
      "Epoch:0013\n",
      "acc_train:0.5922 pre_train:0.5961 recall_train:0.6538 F1_train:0.6236 AUC_train:0.6212\n",
      "acc_val:0.4700 pre_val:0.4839 recall_val:0.9000 F1_val:0.629371 AUC_val:0.3096\n",
      "Epoch:0014\n",
      "acc_train:0.6067 pre_train:0.6086 recall_train:0.6688 F1_train:0.6373 AUC_train:0.6529\n",
      "acc_val:0.4800 pre_val:0.4896 recall_val:0.9400 F1_val:0.643836 AUC_val:0.3168\n",
      "Epoch:0015\n",
      "acc_train:0.6078 pre_train:0.6077 recall_train:0.6796 F1_train:0.6416 AUC_train:0.6338\n",
      "acc_val:0.4700 pre_val:0.4845 recall_val:0.9400 F1_val:0.639456 AUC_val:0.3108\n",
      "Epoch:0016\n",
      "acc_train:0.5878 pre_train:0.5951 recall_train:0.6323 F1_train:0.6131 AUC_train:0.6277\n",
      "acc_val:0.4800 pre_val:0.4898 recall_val:0.9600 F1_val:0.648649 AUC_val:0.2840\n",
      "Epoch:0017\n",
      "acc_train:0.6044 pre_train:0.6128 recall_train:0.6366 F1_train:0.6245 AUC_train:0.6457\n",
      "acc_val:0.4900 pre_val:0.4949 recall_val:0.9800 F1_val:0.657718 AUC_val:0.3140\n",
      "Epoch:0018\n",
      "acc_train:0.6100 pre_train:0.6159 recall_train:0.6516 F1_train:0.6332 AUC_train:0.6381\n",
      "acc_val:0.4900 pre_val:0.4949 recall_val:0.9800 F1_val:0.657718 AUC_val:0.3752\n",
      "Epoch:0019\n",
      "acc_train:0.6056 pre_train:0.6109 recall_train:0.6516 F1_train:0.6306 AUC_train:0.6473\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.5244\n",
      "Epoch:0020\n",
      "acc_train:0.6111 pre_train:0.6143 recall_train:0.6645 F1_train:0.6384 AUC_train:0.6501\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.5428\n",
      "Epoch:0021\n",
      "acc_train:0.5756 pre_train:0.5803 recall_train:0.6452 F1_train:0.6110 AUC_train:0.6277\n",
      "acc_val:0.4900 pre_val:0.4949 recall_val:0.9800 F1_val:0.657718 AUC_val:0.5660\n",
      "Epoch:0022\n",
      "acc_train:0.6056 pre_train:0.6104 recall_train:0.6538 F1_train:0.6314 AUC_train:0.6460\n",
      "acc_val:0.4900 pre_val:0.4949 recall_val:0.9800 F1_val:0.657718 AUC_val:0.5856\n",
      "Epoch:0023\n",
      "acc_train:0.5933 pre_train:0.6074 recall_train:0.6022 F1_train:0.6048 AUC_train:0.6258\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6128\n",
      "Epoch:0024\n",
      "acc_train:0.6511 pre_train:0.6667 recall_train:0.6495 F1_train:0.6580 AUC_train:0.6903\n",
      "acc_val:0.4700 pre_val:0.4839 recall_val:0.9000 F1_val:0.629371 AUC_val:0.6160\n",
      "Epoch:0025\n",
      "acc_train:0.5989 pre_train:0.6083 recall_train:0.6280 F1_train:0.6180 AUC_train:0.6233\n",
      "acc_val:0.5400 pre_val:0.5263 recall_val:0.8000 F1_val:0.634921 AUC_val:0.6392\n",
      "Epoch:0026\n",
      "acc_train:0.6311 pre_train:0.6481 recall_train:0.6258 F1_train:0.6368 AUC_train:0.6670\n",
      "acc_val:0.5500 pre_val:0.5373 recall_val:0.7200 F1_val:0.615385 AUC_val:0.6380\n",
      "Epoch:0027\n",
      "acc_train:0.6411 pre_train:0.6628 recall_train:0.6215 F1_train:0.6415 AUC_train:0.6658\n",
      "acc_val:0.5900 pre_val:0.5789 recall_val:0.6600 F1_val:0.616822 AUC_val:0.6396\n",
      "Epoch:0028\n",
      "acc_train:0.6144 pre_train:0.6272 recall_train:0.6258 F1_train:0.6265 AUC_train:0.6576\n",
      "acc_val:0.6300 pre_val:0.6327 recall_val:0.6200 F1_val:0.626263 AUC_val:0.6396\n",
      "Epoch:0029\n",
      "acc_train:0.6244 pre_train:0.6360 recall_train:0.6387 F1_train:0.6373 AUC_train:0.6411\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.6268\n",
      "Epoch:0030\n",
      "acc_train:0.6278 pre_train:0.6451 recall_train:0.6215 F1_train:0.6331 AUC_train:0.6706\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6300\n",
      "Epoch:0031\n",
      "acc_train:0.6333 pre_train:0.6545 recall_train:0.6151 F1_train:0.6341 AUC_train:0.6791\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6364\n",
      "Epoch:0032\n",
      "acc_train:0.6467 pre_train:0.6815 recall_train:0.5935 F1_train:0.6345 AUC_train:0.6706\n",
      "acc_val:0.6300 pre_val:0.6585 recall_val:0.5400 F1_val:0.593407 AUC_val:0.6416\n",
      "Epoch:0033\n",
      "acc_train:0.6467 pre_train:0.6581 recall_train:0.6581 F1_train:0.6581 AUC_train:0.6945\n",
      "acc_val:0.6100 pre_val:0.6222 recall_val:0.5600 F1_val:0.589474 AUC_val:0.6472\n",
      "Epoch:0034\n",
      "acc_train:0.6167 pre_train:0.6310 recall_train:0.6215 F1_train:0.6262 AUC_train:0.6557\n",
      "acc_val:0.6000 pre_val:0.6087 recall_val:0.5600 F1_val:0.583333 AUC_val:0.6512\n",
      "Epoch:0035\n",
      "acc_train:0.6356 pre_train:0.6424 recall_train:0.6645 F1_train:0.6533 AUC_train:0.6731\n",
      "acc_val:0.6000 pre_val:0.6042 recall_val:0.5800 F1_val:0.591837 AUC_val:0.6564\n",
      "Epoch:0036\n",
      "acc_train:0.6344 pre_train:0.6447 recall_train:0.6516 F1_train:0.6481 AUC_train:0.6777\n",
      "acc_val:0.6000 pre_val:0.6042 recall_val:0.5800 F1_val:0.591837 AUC_val:0.6640\n",
      "Epoch:0037\n",
      "acc_train:0.6311 pre_train:0.6430 recall_train:0.6430 F1_train:0.6430 AUC_train:0.6798\n",
      "acc_val:0.6000 pre_val:0.6042 recall_val:0.5800 F1_val:0.591837 AUC_val:0.6796\n",
      "Epoch:0038\n",
      "acc_train:0.6378 pre_train:0.6439 recall_train:0.6688 F1_train:0.6561 AUC_train:0.6793\n",
      "acc_val:0.6000 pre_val:0.6042 recall_val:0.5800 F1_val:0.591837 AUC_val:0.6804\n",
      "Epoch:0039\n",
      "acc_train:0.6322 pre_train:0.6288 recall_train:0.7032 F1_train:0.6640 AUC_train:0.6787\n",
      "acc_val:0.6000 pre_val:0.6042 recall_val:0.5800 F1_val:0.591837 AUC_val:0.6820\n",
      "Epoch:0040\n",
      "acc_train:0.6000 pre_train:0.6110 recall_train:0.6215 F1_train:0.6162 AUC_train:0.6464\n",
      "acc_val:0.6000 pre_val:0.6042 recall_val:0.5800 F1_val:0.591837 AUC_val:0.6852\n",
      "Epoch:0041\n",
      "acc_train:0.6400 pre_train:0.6529 recall_train:0.6473 F1_train:0.6501 AUC_train:0.6822\n",
      "acc_val:0.6100 pre_val:0.6222 recall_val:0.5600 F1_val:0.589474 AUC_val:0.6804\n",
      "Epoch:0042\n",
      "acc_train:0.6089 pre_train:0.6165 recall_train:0.6430 F1_train:0.6295 AUC_train:0.6716\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.6808\n",
      "Epoch:0043\n",
      "acc_train:0.6511 pre_train:0.6697 recall_train:0.6409 F1_train:0.6549 AUC_train:0.6848\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6804\n",
      "Epoch:0044\n",
      "acc_train:0.6344 pre_train:0.6589 recall_train:0.6065 F1_train:0.6316 AUC_train:0.6860\n",
      "acc_val:0.6500 pre_val:0.6829 recall_val:0.5600 F1_val:0.615385 AUC_val:0.6836\n",
      "Epoch:0045\n",
      "acc_train:0.6633 pre_train:0.6816 recall_train:0.6538 F1_train:0.6674 AUC_train:0.7041\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.6880\n",
      "Epoch:0046\n",
      "acc_train:0.6433 pre_train:0.6629 recall_train:0.6301 F1_train:0.6461 AUC_train:0.6906\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.6956\n",
      "Epoch:0047\n",
      "acc_train:0.6467 pre_train:0.6682 recall_train:0.6280 F1_train:0.6475 AUC_train:0.6854\n",
      "acc_val:0.6500 pre_val:0.6923 recall_val:0.5400 F1_val:0.606742 AUC_val:0.6952\n",
      "Epoch:0048\n",
      "acc_train:0.6333 pre_train:0.6559 recall_train:0.6108 F1_train:0.6325 AUC_train:0.6953\n",
      "acc_val:0.6600 pre_val:0.7000 recall_val:0.5600 F1_val:0.622222 AUC_val:0.6996\n",
      "Epoch:0049\n",
      "acc_train:0.6411 pre_train:0.6511 recall_train:0.6581 F1_train:0.6545 AUC_train:0.6808\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.6980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0050\n",
      "acc_train:0.6211 pre_train:0.6403 recall_train:0.6086 F1_train:0.6240 AUC_train:0.6786\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.6980\n",
      "Epoch:0051\n",
      "acc_train:0.6567 pre_train:0.6840 recall_train:0.6237 F1_train:0.6524 AUC_train:0.7080\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7004\n",
      "Epoch:0052\n",
      "acc_train:0.6500 pre_train:0.6659 recall_train:0.6473 F1_train:0.6565 AUC_train:0.7101\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7036\n",
      "Epoch:0053\n",
      "acc_train:0.6500 pre_train:0.6894 recall_train:0.5871 F1_train:0.6341 AUC_train:0.7046\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7064\n",
      "Epoch:0054\n",
      "acc_train:0.6444 pre_train:0.6808 recall_train:0.5871 F1_train:0.6305 AUC_train:0.7150\n",
      "acc_val:0.6900 pre_val:0.7714 recall_val:0.5400 F1_val:0.635294 AUC_val:0.7108\n",
      "Epoch:0055\n",
      "acc_train:0.6567 pre_train:0.6741 recall_train:0.6495 F1_train:0.6616 AUC_train:0.7158\n",
      "acc_val:0.6800 pre_val:0.7647 recall_val:0.5200 F1_val:0.619048 AUC_val:0.7136\n",
      "Epoch:0056\n",
      "acc_train:0.6611 pre_train:0.6905 recall_train:0.6237 F1_train:0.6554 AUC_train:0.7329\n",
      "acc_val:0.6800 pre_val:0.7647 recall_val:0.5200 F1_val:0.619048 AUC_val:0.7004\n",
      "Epoch:0057\n",
      "acc_train:0.6944 pre_train:0.7121 recall_train:0.6860 F1_train:0.6988 AUC_train:0.7612\n",
      "acc_val:0.6800 pre_val:0.7647 recall_val:0.5200 F1_val:0.619048 AUC_val:0.7048\n",
      "Epoch:0058\n",
      "acc_train:0.6711 pre_train:0.6979 recall_train:0.6409 F1_train:0.6682 AUC_train:0.7449\n",
      "acc_val:0.6700 pre_val:0.7576 recall_val:0.5000 F1_val:0.602410 AUC_val:0.7252\n",
      "Epoch:0059\n",
      "acc_train:0.6689 pre_train:0.6993 recall_train:0.6301 F1_train:0.6629 AUC_train:0.7513\n",
      "acc_val:0.6800 pre_val:0.7647 recall_val:0.5200 F1_val:0.619048 AUC_val:0.7308\n",
      "Epoch:0060\n",
      "acc_train:0.6767 pre_train:0.6977 recall_train:0.6602 F1_train:0.6785 AUC_train:0.7450\n",
      "acc_val:0.6800 pre_val:0.7647 recall_val:0.5200 F1_val:0.619048 AUC_val:0.7424\n",
      "Epoch:0061\n",
      "acc_train:0.6700 pre_train:0.7019 recall_train:0.6280 F1_train:0.6629 AUC_train:0.7452\n",
      "acc_val:0.6900 pre_val:0.7879 recall_val:0.5200 F1_val:0.626506 AUC_val:0.7548\n",
      "Epoch:0062\n",
      "acc_train:0.6922 pre_train:0.7127 recall_train:0.6774 F1_train:0.6946 AUC_train:0.7479\n",
      "acc_val:0.6800 pre_val:0.7812 recall_val:0.5000 F1_val:0.609756 AUC_val:0.7604\n",
      "Epoch:0063\n",
      "acc_train:0.7122 pre_train:0.7269 recall_train:0.7097 F1_train:0.7182 AUC_train:0.7795\n",
      "acc_val:0.6800 pre_val:0.7812 recall_val:0.5000 F1_val:0.609756 AUC_val:0.7680\n",
      "Epoch:0064\n",
      "acc_train:0.7044 pre_train:0.7196 recall_train:0.7011 F1_train:0.7102 AUC_train:0.7752\n",
      "acc_val:0.6800 pre_val:0.7647 recall_val:0.5200 F1_val:0.619048 AUC_val:0.7744\n",
      "Epoch:0065\n",
      "acc_train:0.7078 pre_train:0.7405 recall_train:0.6688 F1_train:0.7028 AUC_train:0.7904\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7780\n",
      "Epoch:0066\n",
      "acc_train:0.7033 pre_train:0.7181 recall_train:0.7011 F1_train:0.7095 AUC_train:0.7877\n",
      "acc_val:0.6800 pre_val:0.7368 recall_val:0.5600 F1_val:0.636364 AUC_val:0.7844\n",
      "Epoch:0067\n",
      "acc_train:0.7056 pre_train:0.7262 recall_train:0.6903 F1_train:0.7078 AUC_train:0.7933\n",
      "acc_val:0.7100 pre_val:0.7442 recall_val:0.6400 F1_val:0.688172 AUC_val:0.7944\n",
      "Epoch:0068\n",
      "acc_train:0.7311 pre_train:0.7408 recall_train:0.7376 F1_train:0.7392 AUC_train:0.8033\n",
      "acc_val:0.7200 pre_val:0.7500 recall_val:0.6600 F1_val:0.702128 AUC_val:0.8088\n",
      "Epoch:0069\n",
      "acc_train:0.7533 pre_train:0.7682 recall_train:0.7484 F1_train:0.7582 AUC_train:0.8221\n",
      "acc_val:0.7300 pre_val:0.7347 recall_val:0.7200 F1_val:0.727273 AUC_val:0.7992\n",
      "Epoch:0070\n",
      "acc_train:0.7622 pre_train:0.7577 recall_train:0.7935 F1_train:0.7752 AUC_train:0.8325\n",
      "acc_val:0.7100 pre_val:0.6909 recall_val:0.7600 F1_val:0.723810 AUC_val:0.7968\n",
      "Epoch:0071\n",
      "acc_train:0.7800 pre_train:0.7719 recall_train:0.8151 F1_train:0.7929 AUC_train:0.8358\n",
      "acc_val:0.7000 pre_val:0.6562 recall_val:0.8400 F1_val:0.736842 AUC_val:0.8008\n",
      "Epoch:0072\n",
      "acc_train:0.8033 pre_train:0.7880 recall_train:0.8473 F1_train:0.8166 AUC_train:0.8494\n",
      "acc_val:0.6900 pre_val:0.6377 recall_val:0.8800 F1_val:0.739496 AUC_val:0.8168\n",
      "Epoch:0073\n",
      "acc_train:0.8033 pre_train:0.7707 recall_train:0.8817 F1_train:0.8225 AUC_train:0.8491\n",
      "acc_val:0.6500 pre_val:0.6027 recall_val:0.8800 F1_val:0.715447 AUC_val:0.8188\n",
      "Epoch:0074\n",
      "acc_train:0.8311 pre_train:0.7970 recall_train:0.9032 F1_train:0.8468 AUC_train:0.8676\n",
      "acc_val:0.6400 pre_val:0.5897 recall_val:0.9200 F1_val:0.718750 AUC_val:0.8184\n",
      "Epoch:0075\n",
      "acc_train:0.7944 pre_train:0.7612 recall_train:0.8774 F1_train:0.8152 AUC_train:0.8673\n",
      "acc_val:0.6100 pre_val:0.5679 recall_val:0.9200 F1_val:0.702290 AUC_val:0.8156\n",
      "Epoch:0076\n",
      "acc_train:0.8167 pre_train:0.7747 recall_train:0.9097 F1_train:0.8368 AUC_train:0.8863\n",
      "acc_val:0.6300 pre_val:0.5802 recall_val:0.9400 F1_val:0.717557 AUC_val:0.8168\n",
      "Epoch:0077\n",
      "acc_train:0.8267 pre_train:0.7774 recall_train:0.9312 F1_train:0.8474 AUC_train:0.8831\n",
      "acc_val:0.6400 pre_val:0.5875 recall_val:0.9400 F1_val:0.723077 AUC_val:0.8164\n",
      "Epoch:0078\n",
      "acc_train:0.8378 pre_train:0.7895 recall_train:0.9355 F1_train:0.8563 AUC_train:0.8948\n",
      "acc_val:0.6400 pre_val:0.5897 recall_val:0.9200 F1_val:0.718750 AUC_val:0.8228\n",
      "Epoch:0079\n",
      "acc_train:0.8589 pre_train:0.7996 recall_train:0.9699 F1_train:0.8766 AUC_train:0.9243\n",
      "acc_val:0.6500 pre_val:0.6056 recall_val:0.8600 F1_val:0.710744 AUC_val:0.8168\n",
      "Epoch:0080\n",
      "acc_train:0.8589 pre_train:0.8130 recall_train:0.9441 F1_train:0.8736 AUC_train:0.9276\n",
      "acc_val:0.6600 pre_val:0.6212 recall_val:0.8200 F1_val:0.706897 AUC_val:0.8008\n",
      "Epoch:0081\n",
      "acc_train:0.8567 pre_train:0.8066 recall_train:0.9505 F1_train:0.8727 AUC_train:0.9163\n",
      "acc_val:0.7200 pre_val:0.6833 recall_val:0.8200 F1_val:0.745455 AUC_val:0.7984\n",
      "Epoch:0082\n",
      "acc_train:0.8756 pre_train:0.8299 recall_train:0.9548 F1_train:0.8880 AUC_train:0.9216\n",
      "acc_val:0.7500 pre_val:0.7049 recall_val:0.8600 F1_val:0.774775 AUC_val:0.8088\n",
      "Epoch:0083\n",
      "acc_train:0.8789 pre_train:0.8410 recall_train:0.9441 F1_train:0.8896 AUC_train:0.9338\n",
      "acc_val:0.7400 pre_val:0.7000 recall_val:0.8400 F1_val:0.763636 AUC_val:0.8024\n",
      "Epoch:0084\n",
      "acc_train:0.8644 pre_train:0.8158 recall_train:0.9527 F1_train:0.8790 AUC_train:0.9140\n",
      "acc_val:0.7300 pre_val:0.6885 recall_val:0.8400 F1_val:0.756757 AUC_val:0.7996\n",
      "Epoch:0085\n",
      "acc_train:0.8633 pre_train:0.8143 recall_train:0.9527 F1_train:0.8781 AUC_train:0.9253\n",
      "acc_val:0.7300 pre_val:0.6825 recall_val:0.8600 F1_val:0.761062 AUC_val:0.7936\n",
      "Epoch:0086\n",
      "acc_train:0.8867 pre_train:0.8431 recall_train:0.9591 F1_train:0.8974 AUC_train:0.9330\n",
      "acc_val:0.7100 pre_val:0.6615 recall_val:0.8600 F1_val:0.747826 AUC_val:0.7864\n",
      "Epoch:0087\n",
      "acc_train:0.9044 pre_train:0.8708 recall_train:0.9570 F1_train:0.9119 AUC_train:0.9504\n",
      "acc_val:0.7000 pre_val:0.6667 recall_val:0.8000 F1_val:0.727273 AUC_val:0.7852\n",
      "Epoch:0088\n",
      "acc_train:0.9044 pre_train:0.8582 recall_train:0.9763 F1_train:0.9135 AUC_train:0.9492\n",
      "acc_val:0.7500 pre_val:0.7119 recall_val:0.8400 F1_val:0.770642 AUC_val:0.7928\n",
      "Epoch:0089\n",
      "acc_train:0.8922 pre_train:0.8566 recall_train:0.9505 F1_train:0.9011 AUC_train:0.9527\n",
      "acc_val:0.7300 pre_val:0.7018 recall_val:0.8000 F1_val:0.747664 AUC_val:0.7972\n",
      "Epoch:0090\n",
      "acc_train:0.8989 pre_train:0.8725 recall_train:0.9419 F1_train:0.9059 AUC_train:0.9652\n",
      "acc_val:0.7700 pre_val:0.7872 recall_val:0.7400 F1_val:0.762887 AUC_val:0.8348\n",
      "Epoch:0091\n",
      "acc_train:0.9044 pre_train:0.8637 recall_train:0.9677 F1_train:0.9128 AUC_train:0.9704\n",
      "acc_val:0.7400 pre_val:0.7609 recall_val:0.7000 F1_val:0.729167 AUC_val:0.8432\n",
      "Epoch:0092\n",
      "acc_train:0.9078 pre_train:0.8760 recall_train:0.9570 F1_train:0.9147 AUC_train:0.9623\n",
      "acc_val:0.7600 pre_val:0.7826 recall_val:0.7200 F1_val:0.750000 AUC_val:0.8456\n",
      "Epoch:0093\n",
      "acc_train:0.8956 pre_train:0.8659 recall_train:0.9441 F1_train:0.9033 AUC_train:0.9624\n",
      "acc_val:0.7800 pre_val:0.7800 recall_val:0.7800 F1_val:0.780000 AUC_val:0.8560\n",
      "Epoch:0094\n",
      "acc_train:0.9133 pre_train:0.8802 recall_train:0.9634 F1_train:0.9199 AUC_train:0.9713\n",
      "acc_val:0.8100 pre_val:0.7925 recall_val:0.8400 F1_val:0.815534 AUC_val:0.8616\n",
      "Epoch:0095\n",
      "acc_train:0.9122 pre_train:0.8988 recall_train:0.9355 F1_train:0.9168 AUC_train:0.9645\n",
      "acc_val:0.8000 pre_val:0.7778 recall_val:0.8400 F1_val:0.807692 AUC_val:0.8528\n",
      "Epoch:0096\n",
      "acc_train:0.9344 pre_train:0.9076 recall_train:0.9720 F1_train:0.9387 AUC_train:0.9795\n",
      "acc_val:0.8000 pre_val:0.7885 recall_val:0.8200 F1_val:0.803922 AUC_val:0.8492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0097\n",
      "acc_train:0.9378 pre_train:0.9115 recall_train:0.9742 F1_train:0.9418 AUC_train:0.9832\n",
      "acc_val:0.7900 pre_val:0.7843 recall_val:0.8000 F1_val:0.792079 AUC_val:0.8412\n",
      "Epoch:0098\n",
      "acc_train:0.9333 pre_train:0.9074 recall_train:0.9699 F1_train:0.9376 AUC_train:0.9755\n",
      "acc_val:0.7500 pre_val:0.7778 recall_val:0.7000 F1_val:0.736842 AUC_val:0.8356\n",
      "Epoch:0099\n",
      "acc_train:0.9311 pre_train:0.9038 recall_train:0.9699 F1_train:0.9357 AUC_train:0.9822\n",
      "acc_val:0.7400 pre_val:0.7857 recall_val:0.6600 F1_val:0.717391 AUC_val:0.8316\n",
      "Epoch:0100\n",
      "acc_train:0.9311 pre_train:0.9172 recall_train:0.9527 F1_train:0.9346 AUC_train:0.9834\n",
      "acc_val:0.7400 pre_val:0.8000 recall_val:0.6400 F1_val:0.711111 AUC_val:0.8328\n",
      "Epoch:0101\n",
      "acc_train:0.9322 pre_train:0.9226 recall_train:0.9484 F1_train:0.9353 AUC_train:0.9798\n",
      "acc_val:0.7500 pre_val:0.8205 recall_val:0.6400 F1_val:0.719101 AUC_val:0.8400\n",
      "Epoch:0102\n",
      "acc_train:0.9422 pre_train:0.9138 recall_train:0.9806 F1_train:0.9461 AUC_train:0.9899\n",
      "acc_val:0.7500 pre_val:0.7907 recall_val:0.6800 F1_val:0.731183 AUC_val:0.8320\n",
      "Epoch:0103\n",
      "acc_train:0.9500 pre_train:0.9321 recall_train:0.9742 F1_train:0.9527 AUC_train:0.9860\n",
      "acc_val:0.7600 pre_val:0.8250 recall_val:0.6600 F1_val:0.733333 AUC_val:0.8380\n",
      "Epoch:0104\n",
      "acc_train:0.9578 pre_train:0.9402 recall_train:0.9806 F1_train:0.9600 AUC_train:0.9865\n",
      "acc_val:0.7800 pre_val:0.8684 recall_val:0.6600 F1_val:0.750000 AUC_val:0.8448\n",
      "Epoch:0105\n",
      "acc_train:0.9389 pre_train:0.9307 recall_train:0.9527 F1_train:0.9416 AUC_train:0.9771\n",
      "acc_val:0.7600 pre_val:0.8250 recall_val:0.6600 F1_val:0.733333 AUC_val:0.8460\n",
      "Epoch:0106\n",
      "acc_train:0.9522 pre_train:0.9396 recall_train:0.9699 F1_train:0.9545 AUC_train:0.9712\n",
      "acc_val:0.8000 pre_val:0.8125 recall_val:0.7800 F1_val:0.795918 AUC_val:0.8468\n",
      "Epoch:0107\n",
      "acc_train:0.9600 pre_train:0.9405 recall_train:0.9849 F1_train:0.9622 AUC_train:0.9924\n",
      "acc_val:0.8000 pre_val:0.7885 recall_val:0.8200 F1_val:0.803922 AUC_val:0.8508\n",
      "Epoch:0108\n",
      "acc_train:0.9544 pre_train:0.9398 recall_train:0.9742 F1_train:0.9567 AUC_train:0.9815\n",
      "acc_val:0.8100 pre_val:0.8163 recall_val:0.8000 F1_val:0.808081 AUC_val:0.8556\n",
      "Epoch:0109\n",
      "acc_train:0.9622 pre_train:0.9480 recall_train:0.9806 F1_train:0.9641 AUC_train:0.9854\n",
      "acc_val:0.8000 pre_val:0.8125 recall_val:0.7800 F1_val:0.795918 AUC_val:0.8660\n",
      "Epoch:0110\n",
      "acc_train:0.9533 pre_train:0.9325 recall_train:0.9806 F1_train:0.9560 AUC_train:0.9917\n",
      "acc_val:0.8000 pre_val:0.8125 recall_val:0.7800 F1_val:0.795918 AUC_val:0.8756\n",
      "Epoch:0111\n",
      "acc_train:0.9500 pre_train:0.9234 recall_train:0.9849 F1_train:0.9532 AUC_train:0.9907\n",
      "acc_val:0.7800 pre_val:0.7917 recall_val:0.7600 F1_val:0.775510 AUC_val:0.8812\n",
      "Epoch:0112\n",
      "acc_train:0.9511 pre_train:0.9340 recall_train:0.9742 F1_train:0.9537 AUC_train:0.9922\n",
      "acc_val:0.8000 pre_val:0.8125 recall_val:0.7800 F1_val:0.795918 AUC_val:0.8880\n",
      "Epoch:0113\n",
      "acc_train:0.9589 pre_train:0.9440 recall_train:0.9785 F1_train:0.9609 AUC_train:0.9931\n",
      "acc_val:0.8100 pre_val:0.8039 recall_val:0.8200 F1_val:0.811881 AUC_val:0.8900\n",
      "Epoch:0114\n",
      "acc_train:0.9644 pre_train:0.9520 recall_train:0.9806 F1_train:0.9661 AUC_train:0.9924\n",
      "acc_val:0.8000 pre_val:0.7778 recall_val:0.8400 F1_val:0.807692 AUC_val:0.8928\n",
      "Epoch:0115\n",
      "acc_train:0.9600 pre_train:0.9387 recall_train:0.9871 F1_train:0.9623 AUC_train:0.9937\n",
      "acc_val:0.8100 pre_val:0.7818 recall_val:0.8600 F1_val:0.819048 AUC_val:0.8924\n",
      "Epoch:0116\n",
      "acc_train:0.9611 pre_train:0.9370 recall_train:0.9914 F1_train:0.9634 AUC_train:0.9942\n",
      "acc_val:0.8000 pre_val:0.7586 recall_val:0.8800 F1_val:0.814815 AUC_val:0.8880\n",
      "Epoch:0117\n",
      "acc_train:0.9700 pre_train:0.9582 recall_train:0.9849 F1_train:0.9714 AUC_train:0.9929\n",
      "acc_val:0.8000 pre_val:0.7500 recall_val:0.9000 F1_val:0.818182 AUC_val:0.8880\n",
      "Epoch:0118\n",
      "acc_train:0.9700 pre_train:0.9506 recall_train:0.9935 F1_train:0.9716 AUC_train:0.9943\n",
      "acc_val:0.7600 pre_val:0.7097 recall_val:0.8800 F1_val:0.785714 AUC_val:0.8856\n",
      "Epoch:0119\n",
      "acc_train:0.9689 pre_train:0.9543 recall_train:0.9871 F1_train:0.9704 AUC_train:0.9924\n",
      "acc_val:0.7400 pre_val:0.6935 recall_val:0.8600 F1_val:0.767857 AUC_val:0.8816\n",
      "Epoch:0120\n",
      "acc_train:0.9689 pre_train:0.9524 recall_train:0.9892 F1_train:0.9705 AUC_train:0.9932\n",
      "acc_val:0.7700 pre_val:0.7368 recall_val:0.8400 F1_val:0.785047 AUC_val:0.8812\n",
      "Epoch:0121\n",
      "acc_train:0.9644 pre_train:0.9482 recall_train:0.9849 F1_train:0.9662 AUC_train:0.9897\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8816\n",
      "Epoch:0122\n",
      "acc_train:0.9678 pre_train:0.9580 recall_train:0.9806 F1_train:0.9692 AUC_train:0.9955\n",
      "acc_val:0.8100 pre_val:0.8039 recall_val:0.8200 F1_val:0.811881 AUC_val:0.8760\n",
      "Epoch:0123\n",
      "acc_train:0.9611 pre_train:0.9442 recall_train:0.9828 F1_train:0.9631 AUC_train:0.9873\n",
      "acc_val:0.8200 pre_val:0.8200 recall_val:0.8200 F1_val:0.820000 AUC_val:0.8876\n",
      "Epoch:0124\n",
      "acc_train:0.9811 pre_train:0.9706 recall_train:0.9935 F1_train:0.9819 AUC_train:0.9966\n",
      "acc_val:0.7800 pre_val:0.7692 recall_val:0.8000 F1_val:0.784314 AUC_val:0.8900\n",
      "Epoch:0125\n",
      "acc_train:0.9778 pre_train:0.9704 recall_train:0.9871 F1_train:0.9787 AUC_train:0.9945\n",
      "acc_val:0.7800 pre_val:0.7593 recall_val:0.8200 F1_val:0.788462 AUC_val:0.8884\n",
      "Epoch:0126\n",
      "acc_train:0.9700 pre_train:0.9582 recall_train:0.9849 F1_train:0.9714 AUC_train:0.9938\n",
      "acc_val:0.7800 pre_val:0.7692 recall_val:0.8000 F1_val:0.784314 AUC_val:0.8912\n",
      "Epoch:0127\n",
      "acc_train:0.9811 pre_train:0.9746 recall_train:0.9892 F1_train:0.9819 AUC_train:0.9957\n",
      "acc_val:0.7900 pre_val:0.7736 recall_val:0.8200 F1_val:0.796117 AUC_val:0.8916\n",
      "Epoch:0128\n",
      "acc_train:0.9811 pre_train:0.9706 recall_train:0.9935 F1_train:0.9819 AUC_train:0.9982\n",
      "acc_val:0.8100 pre_val:0.8039 recall_val:0.8200 F1_val:0.811881 AUC_val:0.8960\n",
      "Epoch:0129\n",
      "acc_train:0.9811 pre_train:0.9746 recall_train:0.9892 F1_train:0.9819 AUC_train:0.9934\n",
      "acc_val:0.8200 pre_val:0.8200 recall_val:0.8200 F1_val:0.820000 AUC_val:0.8932\n",
      "Epoch:0130\n",
      "acc_train:0.9756 pre_train:0.9743 recall_train:0.9785 F1_train:0.9764 AUC_train:0.9953\n",
      "acc_val:0.8200 pre_val:0.8200 recall_val:0.8200 F1_val:0.820000 AUC_val:0.8968\n",
      "Epoch:0131\n",
      "acc_train:0.9844 pre_train:0.9829 recall_train:0.9871 F1_train:0.9850 AUC_train:0.9974\n",
      "acc_val:0.8100 pre_val:0.8039 recall_val:0.8200 F1_val:0.811881 AUC_val:0.8992\n",
      "Epoch:0132\n",
      "acc_train:0.9778 pre_train:0.9665 recall_train:0.9914 F1_train:0.9788 AUC_train:0.9939\n",
      "acc_val:0.8000 pre_val:0.7885 recall_val:0.8200 F1_val:0.803922 AUC_val:0.8980\n",
      "Epoch:0133\n",
      "acc_train:0.9756 pre_train:0.9644 recall_train:0.9892 F1_train:0.9766 AUC_train:0.9964\n",
      "acc_val:0.8000 pre_val:0.7885 recall_val:0.8200 F1_val:0.803922 AUC_val:0.8920\n",
      "Epoch:0134\n",
      "acc_train:0.9811 pre_train:0.9706 recall_train:0.9935 F1_train:0.9819 AUC_train:0.9955\n",
      "acc_val:0.8100 pre_val:0.7925 recall_val:0.8400 F1_val:0.815534 AUC_val:0.8908\n",
      "Early Stopping!!! epoch：133\n",
      " Starting the 1-3 Fold:：\n",
      "Fitting estimator with 76636 features.\n",
      "Fitting estimator with 76536 features.\n",
      "Fitting estimator with 76436 features.\n",
      "Fitting estimator with 76336 features.\n",
      "Fitting estimator with 76236 features.\n",
      "Fitting estimator with 76136 features.\n",
      "Fitting estimator with 76036 features.\n",
      "Fitting estimator with 75936 features.\n",
      "Fitting estimator with 75836 features.\n",
      "Fitting estimator with 75736 features.\n",
      "Fitting estimator with 75636 features.\n",
      "Fitting estimator with 75536 features.\n",
      "Fitting estimator with 75436 features.\n",
      "Fitting estimator with 75336 features.\n",
      "Fitting estimator with 75236 features.\n",
      "Fitting estimator with 75136 features.\n",
      "Fitting estimator with 75036 features.\n",
      "Fitting estimator with 74936 features.\n",
      "Fitting estimator with 74836 features.\n",
      "Fitting estimator with 74736 features.\n",
      "Fitting estimator with 74636 features.\n",
      "Fitting estimator with 74536 features.\n",
      "Fitting estimator with 74436 features.\n",
      "Fitting estimator with 74336 features.\n",
      "Fitting estimator with 74236 features.\n",
      "Fitting estimator with 74136 features.\n",
      "Fitting estimator with 74036 features.\n",
      "Fitting estimator with 73936 features.\n",
      "Fitting estimator with 73836 features.\n",
      "Fitting estimator with 73736 features.\n",
      "Fitting estimator with 73636 features.\n",
      "Fitting estimator with 73536 features.\n",
      "Fitting estimator with 73436 features.\n",
      "Fitting estimator with 73336 features.\n",
      "Fitting estimator with 73236 features.\n",
      "Fitting estimator with 73136 features.\n",
      "Fitting estimator with 73036 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 72936 features.\n",
      "Fitting estimator with 72836 features.\n",
      "Fitting estimator with 72736 features.\n",
      "Fitting estimator with 72636 features.\n",
      "Fitting estimator with 72536 features.\n",
      "Fitting estimator with 72436 features.\n",
      "Fitting estimator with 72336 features.\n",
      "Fitting estimator with 72236 features.\n",
      "Fitting estimator with 72136 features.\n",
      "Fitting estimator with 72036 features.\n",
      "Fitting estimator with 71936 features.\n",
      "Fitting estimator with 71836 features.\n",
      "Fitting estimator with 71736 features.\n",
      "Fitting estimator with 71636 features.\n",
      "Fitting estimator with 71536 features.\n",
      "Fitting estimator with 71436 features.\n",
      "Fitting estimator with 71336 features.\n",
      "Fitting estimator with 71236 features.\n",
      "Fitting estimator with 71136 features.\n",
      "Fitting estimator with 71036 features.\n",
      "Fitting estimator with 70936 features.\n",
      "Fitting estimator with 70836 features.\n",
      "Fitting estimator with 70736 features.\n",
      "Fitting estimator with 70636 features.\n",
      "Fitting estimator with 70536 features.\n",
      "Fitting estimator with 70436 features.\n",
      "Fitting estimator with 70336 features.\n",
      "Fitting estimator with 70236 features.\n",
      "Fitting estimator with 70136 features.\n",
      "Fitting estimator with 70036 features.\n",
      "Fitting estimator with 69936 features.\n",
      "Fitting estimator with 69836 features.\n",
      "Fitting estimator with 69736 features.\n",
      "Fitting estimator with 69636 features.\n",
      "Fitting estimator with 69536 features.\n",
      "Fitting estimator with 69436 features.\n",
      "Fitting estimator with 69336 features.\n",
      "Fitting estimator with 69236 features.\n",
      "Fitting estimator with 69136 features.\n",
      "Fitting estimator with 69036 features.\n",
      "Fitting estimator with 68936 features.\n",
      "Fitting estimator with 68836 features.\n",
      "Fitting estimator with 68736 features.\n",
      "Fitting estimator with 68636 features.\n",
      "Fitting estimator with 68536 features.\n",
      "Fitting estimator with 68436 features.\n",
      "Fitting estimator with 68336 features.\n",
      "Fitting estimator with 68236 features.\n",
      "Fitting estimator with 68136 features.\n",
      "Fitting estimator with 68036 features.\n",
      "Fitting estimator with 67936 features.\n",
      "Fitting estimator with 67836 features.\n",
      "Fitting estimator with 67736 features.\n",
      "Fitting estimator with 67636 features.\n",
      "Fitting estimator with 67536 features.\n",
      "Fitting estimator with 67436 features.\n",
      "Fitting estimator with 67336 features.\n",
      "Fitting estimator with 67236 features.\n",
      "Fitting estimator with 67136 features.\n",
      "Fitting estimator with 67036 features.\n",
      "Fitting estimator with 66936 features.\n",
      "Fitting estimator with 66836 features.\n",
      "Fitting estimator with 66736 features.\n",
      "Fitting estimator with 66636 features.\n",
      "Fitting estimator with 66536 features.\n",
      "Fitting estimator with 66436 features.\n",
      "Fitting estimator with 66336 features.\n",
      "Fitting estimator with 66236 features.\n",
      "Fitting estimator with 66136 features.\n",
      "Fitting estimator with 66036 features.\n",
      "Fitting estimator with 65936 features.\n",
      "Fitting estimator with 65836 features.\n",
      "Fitting estimator with 65736 features.\n",
      "Fitting estimator with 65636 features.\n",
      "Fitting estimator with 65536 features.\n",
      "Fitting estimator with 65436 features.\n",
      "Fitting estimator with 65336 features.\n",
      "Fitting estimator with 65236 features.\n",
      "Fitting estimator with 65136 features.\n",
      "Fitting estimator with 65036 features.\n",
      "Fitting estimator with 64936 features.\n",
      "Fitting estimator with 64836 features.\n",
      "Fitting estimator with 64736 features.\n",
      "Fitting estimator with 64636 features.\n",
      "Fitting estimator with 64536 features.\n",
      "Fitting estimator with 64436 features.\n",
      "Fitting estimator with 64336 features.\n",
      "Fitting estimator with 64236 features.\n",
      "Fitting estimator with 64136 features.\n",
      "Fitting estimator with 64036 features.\n",
      "Fitting estimator with 63936 features.\n",
      "Fitting estimator with 63836 features.\n",
      "Fitting estimator with 63736 features.\n",
      "Fitting estimator with 63636 features.\n",
      "Fitting estimator with 63536 features.\n",
      "Fitting estimator with 63436 features.\n",
      "Fitting estimator with 63336 features.\n",
      "Fitting estimator with 63236 features.\n",
      "Fitting estimator with 63136 features.\n",
      "Fitting estimator with 63036 features.\n",
      "Fitting estimator with 62936 features.\n",
      "Fitting estimator with 62836 features.\n",
      "Fitting estimator with 62736 features.\n",
      "Fitting estimator with 62636 features.\n",
      "Fitting estimator with 62536 features.\n",
      "Fitting estimator with 62436 features.\n",
      "Fitting estimator with 62336 features.\n",
      "Fitting estimator with 62236 features.\n",
      "Fitting estimator with 62136 features.\n",
      "Fitting estimator with 62036 features.\n",
      "Fitting estimator with 61936 features.\n",
      "Fitting estimator with 61836 features.\n",
      "Fitting estimator with 61736 features.\n",
      "Fitting estimator with 61636 features.\n",
      "Fitting estimator with 61536 features.\n",
      "Fitting estimator with 61436 features.\n",
      "Fitting estimator with 61336 features.\n",
      "Fitting estimator with 61236 features.\n",
      "Fitting estimator with 61136 features.\n",
      "Fitting estimator with 61036 features.\n",
      "Fitting estimator with 60936 features.\n",
      "Fitting estimator with 60836 features.\n",
      "Fitting estimator with 60736 features.\n",
      "Fitting estimator with 60636 features.\n",
      "Fitting estimator with 60536 features.\n",
      "Fitting estimator with 60436 features.\n",
      "Fitting estimator with 60336 features.\n",
      "Fitting estimator with 60236 features.\n",
      "Fitting estimator with 60136 features.\n",
      "Fitting estimator with 60036 features.\n",
      "Fitting estimator with 59936 features.\n",
      "Fitting estimator with 59836 features.\n",
      "Fitting estimator with 59736 features.\n",
      "Fitting estimator with 59636 features.\n",
      "Fitting estimator with 59536 features.\n",
      "Fitting estimator with 59436 features.\n",
      "Fitting estimator with 59336 features.\n",
      "Fitting estimator with 59236 features.\n",
      "Fitting estimator with 59136 features.\n",
      "Fitting estimator with 59036 features.\n",
      "Fitting estimator with 58936 features.\n",
      "Fitting estimator with 58836 features.\n",
      "Fitting estimator with 58736 features.\n",
      "Fitting estimator with 58636 features.\n",
      "Fitting estimator with 58536 features.\n",
      "Fitting estimator with 58436 features.\n",
      "Fitting estimator with 58336 features.\n",
      "Fitting estimator with 58236 features.\n",
      "Fitting estimator with 58136 features.\n",
      "Fitting estimator with 58036 features.\n",
      "Fitting estimator with 57936 features.\n",
      "Fitting estimator with 57836 features.\n",
      "Fitting estimator with 57736 features.\n",
      "Fitting estimator with 57636 features.\n",
      "Fitting estimator with 57536 features.\n",
      "Fitting estimator with 57436 features.\n",
      "Fitting estimator with 57336 features.\n",
      "Fitting estimator with 57236 features.\n",
      "Fitting estimator with 57136 features.\n",
      "Fitting estimator with 57036 features.\n",
      "Fitting estimator with 56936 features.\n",
      "Fitting estimator with 56836 features.\n",
      "Fitting estimator with 56736 features.\n",
      "Fitting estimator with 56636 features.\n",
      "Fitting estimator with 56536 features.\n",
      "Fitting estimator with 56436 features.\n",
      "Fitting estimator with 56336 features.\n",
      "Fitting estimator with 56236 features.\n",
      "Fitting estimator with 56136 features.\n",
      "Fitting estimator with 56036 features.\n",
      "Fitting estimator with 55936 features.\n",
      "Fitting estimator with 55836 features.\n",
      "Fitting estimator with 55736 features.\n",
      "Fitting estimator with 55636 features.\n",
      "Fitting estimator with 55536 features.\n",
      "Fitting estimator with 55436 features.\n",
      "Fitting estimator with 55336 features.\n",
      "Fitting estimator with 55236 features.\n",
      "Fitting estimator with 55136 features.\n",
      "Fitting estimator with 55036 features.\n",
      "Fitting estimator with 54936 features.\n",
      "Fitting estimator with 54836 features.\n",
      "Fitting estimator with 54736 features.\n",
      "Fitting estimator with 54636 features.\n",
      "Fitting estimator with 54536 features.\n",
      "Fitting estimator with 54436 features.\n",
      "Fitting estimator with 54336 features.\n",
      "Fitting estimator with 54236 features.\n",
      "Fitting estimator with 54136 features.\n",
      "Fitting estimator with 54036 features.\n",
      "Fitting estimator with 53936 features.\n",
      "Fitting estimator with 53836 features.\n",
      "Fitting estimator with 53736 features.\n",
      "Fitting estimator with 53636 features.\n",
      "Fitting estimator with 53536 features.\n",
      "Fitting estimator with 53436 features.\n",
      "Fitting estimator with 53336 features.\n",
      "Fitting estimator with 53236 features.\n",
      "Fitting estimator with 53136 features.\n",
      "Fitting estimator with 53036 features.\n",
      "Fitting estimator with 52936 features.\n",
      "Fitting estimator with 52836 features.\n",
      "Fitting estimator with 52736 features.\n",
      "Fitting estimator with 52636 features.\n",
      "Fitting estimator with 52536 features.\n",
      "Fitting estimator with 52436 features.\n",
      "Fitting estimator with 52336 features.\n",
      "Fitting estimator with 52236 features.\n",
      "Fitting estimator with 52136 features.\n",
      "Fitting estimator with 52036 features.\n",
      "Fitting estimator with 51936 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 51836 features.\n",
      "Fitting estimator with 51736 features.\n",
      "Fitting estimator with 51636 features.\n",
      "Fitting estimator with 51536 features.\n",
      "Fitting estimator with 51436 features.\n",
      "Fitting estimator with 51336 features.\n",
      "Fitting estimator with 51236 features.\n",
      "Fitting estimator with 51136 features.\n",
      "Fitting estimator with 51036 features.\n",
      "Fitting estimator with 50936 features.\n",
      "Fitting estimator with 50836 features.\n",
      "Fitting estimator with 50736 features.\n",
      "Fitting estimator with 50636 features.\n",
      "Fitting estimator with 50536 features.\n",
      "Fitting estimator with 50436 features.\n",
      "Fitting estimator with 50336 features.\n",
      "Fitting estimator with 50236 features.\n",
      "Fitting estimator with 50136 features.\n",
      "Fitting estimator with 50036 features.\n",
      "Fitting estimator with 49936 features.\n",
      "Fitting estimator with 49836 features.\n",
      "Fitting estimator with 49736 features.\n",
      "Fitting estimator with 49636 features.\n",
      "Fitting estimator with 49536 features.\n",
      "Fitting estimator with 49436 features.\n",
      "Fitting estimator with 49336 features.\n",
      "Fitting estimator with 49236 features.\n",
      "Fitting estimator with 49136 features.\n",
      "Fitting estimator with 49036 features.\n",
      "Fitting estimator with 48936 features.\n",
      "Fitting estimator with 48836 features.\n",
      "Fitting estimator with 48736 features.\n",
      "Fitting estimator with 48636 features.\n",
      "Fitting estimator with 48536 features.\n",
      "Fitting estimator with 48436 features.\n",
      "Fitting estimator with 48336 features.\n",
      "Fitting estimator with 48236 features.\n",
      "Fitting estimator with 48136 features.\n",
      "Fitting estimator with 48036 features.\n",
      "Fitting estimator with 47936 features.\n",
      "Fitting estimator with 47836 features.\n",
      "Fitting estimator with 47736 features.\n",
      "Fitting estimator with 47636 features.\n",
      "Fitting estimator with 47536 features.\n",
      "Fitting estimator with 47436 features.\n",
      "Fitting estimator with 47336 features.\n",
      "Fitting estimator with 47236 features.\n",
      "Fitting estimator with 47136 features.\n",
      "Fitting estimator with 47036 features.\n",
      "Fitting estimator with 46936 features.\n",
      "Fitting estimator with 46836 features.\n",
      "Fitting estimator with 46736 features.\n",
      "Fitting estimator with 46636 features.\n",
      "Fitting estimator with 46536 features.\n",
      "Fitting estimator with 46436 features.\n",
      "Fitting estimator with 46336 features.\n",
      "Fitting estimator with 46236 features.\n",
      "Fitting estimator with 46136 features.\n",
      "Fitting estimator with 46036 features.\n",
      "Fitting estimator with 45936 features.\n",
      "Fitting estimator with 45836 features.\n",
      "Fitting estimator with 45736 features.\n",
      "Fitting estimator with 45636 features.\n",
      "Fitting estimator with 45536 features.\n",
      "Fitting estimator with 45436 features.\n",
      "Fitting estimator with 45336 features.\n",
      "Fitting estimator with 45236 features.\n",
      "Fitting estimator with 45136 features.\n",
      "Fitting estimator with 45036 features.\n",
      "Fitting estimator with 44936 features.\n",
      "Fitting estimator with 44836 features.\n",
      "Fitting estimator with 44736 features.\n",
      "Fitting estimator with 44636 features.\n",
      "Fitting estimator with 44536 features.\n",
      "Fitting estimator with 44436 features.\n",
      "Fitting estimator with 44336 features.\n",
      "Fitting estimator with 44236 features.\n",
      "Fitting estimator with 44136 features.\n",
      "Fitting estimator with 44036 features.\n",
      "Fitting estimator with 43936 features.\n",
      "Fitting estimator with 43836 features.\n",
      "Fitting estimator with 43736 features.\n",
      "Fitting estimator with 43636 features.\n",
      "Fitting estimator with 43536 features.\n",
      "Fitting estimator with 43436 features.\n",
      "Fitting estimator with 43336 features.\n",
      "Fitting estimator with 43236 features.\n",
      "Fitting estimator with 43136 features.\n",
      "Fitting estimator with 43036 features.\n",
      "Fitting estimator with 42936 features.\n",
      "Fitting estimator with 42836 features.\n",
      "Fitting estimator with 42736 features.\n",
      "Fitting estimator with 42636 features.\n",
      "Fitting estimator with 42536 features.\n",
      "Fitting estimator with 42436 features.\n",
      "Fitting estimator with 42336 features.\n",
      "Fitting estimator with 42236 features.\n",
      "Fitting estimator with 42136 features.\n",
      "Fitting estimator with 42036 features.\n",
      "Fitting estimator with 41936 features.\n",
      "Fitting estimator with 41836 features.\n",
      "Fitting estimator with 41736 features.\n",
      "Fitting estimator with 41636 features.\n",
      "Fitting estimator with 41536 features.\n",
      "Fitting estimator with 41436 features.\n",
      "Fitting estimator with 41336 features.\n",
      "Fitting estimator with 41236 features.\n",
      "Fitting estimator with 41136 features.\n",
      "Fitting estimator with 41036 features.\n",
      "Fitting estimator with 40936 features.\n",
      "Fitting estimator with 40836 features.\n",
      "Fitting estimator with 40736 features.\n",
      "Fitting estimator with 40636 features.\n",
      "Fitting estimator with 40536 features.\n",
      "Fitting estimator with 40436 features.\n",
      "Fitting estimator with 40336 features.\n",
      "Fitting estimator with 40236 features.\n",
      "Fitting estimator with 40136 features.\n",
      "Fitting estimator with 40036 features.\n",
      "Fitting estimator with 39936 features.\n",
      "Fitting estimator with 39836 features.\n",
      "Fitting estimator with 39736 features.\n",
      "Fitting estimator with 39636 features.\n",
      "Fitting estimator with 39536 features.\n",
      "Fitting estimator with 39436 features.\n",
      "Fitting estimator with 39336 features.\n",
      "Fitting estimator with 39236 features.\n",
      "Fitting estimator with 39136 features.\n",
      "Fitting estimator with 39036 features.\n",
      "Fitting estimator with 38936 features.\n",
      "Fitting estimator with 38836 features.\n",
      "Fitting estimator with 38736 features.\n",
      "Fitting estimator with 38636 features.\n",
      "Fitting estimator with 38536 features.\n",
      "Fitting estimator with 38436 features.\n",
      "Fitting estimator with 38336 features.\n",
      "Fitting estimator with 38236 features.\n",
      "Fitting estimator with 38136 features.\n",
      "Fitting estimator with 38036 features.\n",
      "Fitting estimator with 37936 features.\n",
      "Fitting estimator with 37836 features.\n",
      "Fitting estimator with 37736 features.\n",
      "Fitting estimator with 37636 features.\n",
      "Fitting estimator with 37536 features.\n",
      "Fitting estimator with 37436 features.\n",
      "Fitting estimator with 37336 features.\n",
      "Fitting estimator with 37236 features.\n",
      "Fitting estimator with 37136 features.\n",
      "Fitting estimator with 37036 features.\n",
      "Fitting estimator with 36936 features.\n",
      "Fitting estimator with 36836 features.\n",
      "Fitting estimator with 36736 features.\n",
      "Fitting estimator with 36636 features.\n",
      "Fitting estimator with 36536 features.\n",
      "Fitting estimator with 36436 features.\n",
      "Fitting estimator with 36336 features.\n",
      "Fitting estimator with 36236 features.\n",
      "Fitting estimator with 36136 features.\n",
      "Fitting estimator with 36036 features.\n",
      "Fitting estimator with 35936 features.\n",
      "Fitting estimator with 35836 features.\n",
      "Fitting estimator with 35736 features.\n",
      "Fitting estimator with 35636 features.\n",
      "Fitting estimator with 35536 features.\n",
      "Fitting estimator with 35436 features.\n",
      "Fitting estimator with 35336 features.\n",
      "Fitting estimator with 35236 features.\n",
      "Fitting estimator with 35136 features.\n",
      "Fitting estimator with 35036 features.\n",
      "Fitting estimator with 34936 features.\n",
      "Fitting estimator with 34836 features.\n",
      "Fitting estimator with 34736 features.\n",
      "Fitting estimator with 34636 features.\n",
      "Fitting estimator with 34536 features.\n",
      "Fitting estimator with 34436 features.\n",
      "Fitting estimator with 34336 features.\n",
      "Fitting estimator with 34236 features.\n",
      "Fitting estimator with 34136 features.\n",
      "Fitting estimator with 34036 features.\n",
      "Fitting estimator with 33936 features.\n",
      "Fitting estimator with 33836 features.\n",
      "Fitting estimator with 33736 features.\n",
      "Fitting estimator with 33636 features.\n",
      "Fitting estimator with 33536 features.\n",
      "Fitting estimator with 33436 features.\n",
      "Fitting estimator with 33336 features.\n",
      "Fitting estimator with 33236 features.\n",
      "Fitting estimator with 33136 features.\n",
      "Fitting estimator with 33036 features.\n",
      "Fitting estimator with 32936 features.\n",
      "Fitting estimator with 32836 features.\n",
      "Fitting estimator with 32736 features.\n",
      "Fitting estimator with 32636 features.\n",
      "Fitting estimator with 32536 features.\n",
      "Fitting estimator with 32436 features.\n",
      "Fitting estimator with 32336 features.\n",
      "Fitting estimator with 32236 features.\n",
      "Fitting estimator with 32136 features.\n",
      "Fitting estimator with 32036 features.\n",
      "Fitting estimator with 31936 features.\n",
      "Fitting estimator with 31836 features.\n",
      "Fitting estimator with 31736 features.\n",
      "Fitting estimator with 31636 features.\n",
      "Fitting estimator with 31536 features.\n",
      "Fitting estimator with 31436 features.\n",
      "Fitting estimator with 31336 features.\n",
      "Fitting estimator with 31236 features.\n",
      "Fitting estimator with 31136 features.\n",
      "Fitting estimator with 31036 features.\n",
      "Fitting estimator with 30936 features.\n",
      "Fitting estimator with 30836 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 30736 features.\n",
      "Fitting estimator with 30636 features.\n",
      "Fitting estimator with 30536 features.\n",
      "Fitting estimator with 30436 features.\n",
      "Fitting estimator with 30336 features.\n",
      "Fitting estimator with 30236 features.\n",
      "Fitting estimator with 30136 features.\n",
      "Fitting estimator with 30036 features.\n",
      "Fitting estimator with 29936 features.\n",
      "Fitting estimator with 29836 features.\n",
      "Fitting estimator with 29736 features.\n",
      "Fitting estimator with 29636 features.\n",
      "Fitting estimator with 29536 features.\n",
      "Fitting estimator with 29436 features.\n",
      "Fitting estimator with 29336 features.\n",
      "Fitting estimator with 29236 features.\n",
      "Fitting estimator with 29136 features.\n",
      "Fitting estimator with 29036 features.\n",
      "Fitting estimator with 28936 features.\n",
      "Fitting estimator with 28836 features.\n",
      "Fitting estimator with 28736 features.\n",
      "Fitting estimator with 28636 features.\n",
      "Fitting estimator with 28536 features.\n",
      "Fitting estimator with 28436 features.\n",
      "Fitting estimator with 28336 features.\n",
      "Fitting estimator with 28236 features.\n",
      "Fitting estimator with 28136 features.\n",
      "Fitting estimator with 28036 features.\n",
      "Fitting estimator with 27936 features.\n",
      "Fitting estimator with 27836 features.\n",
      "Fitting estimator with 27736 features.\n",
      "Fitting estimator with 27636 features.\n",
      "Fitting estimator with 27536 features.\n",
      "Fitting estimator with 27436 features.\n",
      "Fitting estimator with 27336 features.\n",
      "Fitting estimator with 27236 features.\n",
      "Fitting estimator with 27136 features.\n",
      "Fitting estimator with 27036 features.\n",
      "Fitting estimator with 26936 features.\n",
      "Fitting estimator with 26836 features.\n",
      "Fitting estimator with 26736 features.\n",
      "Fitting estimator with 26636 features.\n",
      "Fitting estimator with 26536 features.\n",
      "Fitting estimator with 26436 features.\n",
      "Fitting estimator with 26336 features.\n",
      "Fitting estimator with 26236 features.\n",
      "Fitting estimator with 26136 features.\n",
      "Fitting estimator with 26036 features.\n",
      "Fitting estimator with 25936 features.\n",
      "Fitting estimator with 25836 features.\n",
      "Fitting estimator with 25736 features.\n",
      "Fitting estimator with 25636 features.\n",
      "Fitting estimator with 25536 features.\n",
      "Fitting estimator with 25436 features.\n",
      "Fitting estimator with 25336 features.\n",
      "Fitting estimator with 25236 features.\n",
      "Fitting estimator with 25136 features.\n",
      "Fitting estimator with 25036 features.\n",
      "Fitting estimator with 24936 features.\n",
      "Fitting estimator with 24836 features.\n",
      "Fitting estimator with 24736 features.\n",
      "Fitting estimator with 24636 features.\n",
      "Fitting estimator with 24536 features.\n",
      "Fitting estimator with 24436 features.\n",
      "Fitting estimator with 24336 features.\n",
      "Fitting estimator with 24236 features.\n",
      "Fitting estimator with 24136 features.\n",
      "Fitting estimator with 24036 features.\n",
      "Fitting estimator with 23936 features.\n",
      "Fitting estimator with 23836 features.\n",
      "Fitting estimator with 23736 features.\n",
      "Fitting estimator with 23636 features.\n",
      "Fitting estimator with 23536 features.\n",
      "Fitting estimator with 23436 features.\n",
      "Fitting estimator with 23336 features.\n",
      "Fitting estimator with 23236 features.\n",
      "Fitting estimator with 23136 features.\n",
      "Fitting estimator with 23036 features.\n",
      "Fitting estimator with 22936 features.\n",
      "Fitting estimator with 22836 features.\n",
      "Fitting estimator with 22736 features.\n",
      "Fitting estimator with 22636 features.\n",
      "Fitting estimator with 22536 features.\n",
      "Fitting estimator with 22436 features.\n",
      "Fitting estimator with 22336 features.\n",
      "Fitting estimator with 22236 features.\n",
      "Fitting estimator with 22136 features.\n",
      "Fitting estimator with 22036 features.\n",
      "Fitting estimator with 21936 features.\n",
      "Fitting estimator with 21836 features.\n",
      "Fitting estimator with 21736 features.\n",
      "Fitting estimator with 21636 features.\n",
      "Fitting estimator with 21536 features.\n",
      "Fitting estimator with 21436 features.\n",
      "Fitting estimator with 21336 features.\n",
      "Fitting estimator with 21236 features.\n",
      "Fitting estimator with 21136 features.\n",
      "Fitting estimator with 21036 features.\n",
      "Fitting estimator with 20936 features.\n",
      "Fitting estimator with 20836 features.\n",
      "Fitting estimator with 20736 features.\n",
      "Fitting estimator with 20636 features.\n",
      "Fitting estimator with 20536 features.\n",
      "Fitting estimator with 20436 features.\n",
      "Fitting estimator with 20336 features.\n",
      "Fitting estimator with 20236 features.\n",
      "Fitting estimator with 20136 features.\n",
      "Fitting estimator with 20036 features.\n",
      "Fitting estimator with 19936 features.\n",
      "Fitting estimator with 19836 features.\n",
      "Fitting estimator with 19736 features.\n",
      "Fitting estimator with 19636 features.\n",
      "Fitting estimator with 19536 features.\n",
      "Fitting estimator with 19436 features.\n",
      "Fitting estimator with 19336 features.\n",
      "Fitting estimator with 19236 features.\n",
      "Fitting estimator with 19136 features.\n",
      "Fitting estimator with 19036 features.\n",
      "Fitting estimator with 18936 features.\n",
      "Fitting estimator with 18836 features.\n",
      "Fitting estimator with 18736 features.\n",
      "Fitting estimator with 18636 features.\n",
      "Fitting estimator with 18536 features.\n",
      "Fitting estimator with 18436 features.\n",
      "Fitting estimator with 18336 features.\n",
      "Fitting estimator with 18236 features.\n",
      "Fitting estimator with 18136 features.\n",
      "Fitting estimator with 18036 features.\n",
      "Fitting estimator with 17936 features.\n",
      "Fitting estimator with 17836 features.\n",
      "Fitting estimator with 17736 features.\n",
      "Fitting estimator with 17636 features.\n",
      "Fitting estimator with 17536 features.\n",
      "Fitting estimator with 17436 features.\n",
      "Fitting estimator with 17336 features.\n",
      "Fitting estimator with 17236 features.\n",
      "Fitting estimator with 17136 features.\n",
      "Fitting estimator with 17036 features.\n",
      "Fitting estimator with 16936 features.\n",
      "Fitting estimator with 16836 features.\n",
      "Fitting estimator with 16736 features.\n",
      "Fitting estimator with 16636 features.\n",
      "Fitting estimator with 16536 features.\n",
      "Fitting estimator with 16436 features.\n",
      "Fitting estimator with 16336 features.\n",
      "Fitting estimator with 16236 features.\n",
      "Fitting estimator with 16136 features.\n",
      "Fitting estimator with 16036 features.\n",
      "Fitting estimator with 15936 features.\n",
      "Fitting estimator with 15836 features.\n",
      "Fitting estimator with 15736 features.\n",
      "Fitting estimator with 15636 features.\n",
      "Fitting estimator with 15536 features.\n",
      "Fitting estimator with 15436 features.\n",
      "Fitting estimator with 15336 features.\n",
      "Fitting estimator with 15236 features.\n",
      "Fitting estimator with 15136 features.\n",
      "Fitting estimator with 15036 features.\n",
      "Fitting estimator with 14936 features.\n",
      "Fitting estimator with 14836 features.\n",
      "Fitting estimator with 14736 features.\n",
      "Fitting estimator with 14636 features.\n",
      "Fitting estimator with 14536 features.\n",
      "Fitting estimator with 14436 features.\n",
      "Fitting estimator with 14336 features.\n",
      "Fitting estimator with 14236 features.\n",
      "Fitting estimator with 14136 features.\n",
      "Fitting estimator with 14036 features.\n",
      "Fitting estimator with 13936 features.\n",
      "Fitting estimator with 13836 features.\n",
      "Fitting estimator with 13736 features.\n",
      "Fitting estimator with 13636 features.\n",
      "Fitting estimator with 13536 features.\n",
      "Fitting estimator with 13436 features.\n",
      "Fitting estimator with 13336 features.\n",
      "Fitting estimator with 13236 features.\n",
      "Fitting estimator with 13136 features.\n",
      "Fitting estimator with 13036 features.\n",
      "Fitting estimator with 12936 features.\n",
      "Fitting estimator with 12836 features.\n",
      "Fitting estimator with 12736 features.\n",
      "Fitting estimator with 12636 features.\n",
      "Fitting estimator with 12536 features.\n",
      "Fitting estimator with 12436 features.\n",
      "Fitting estimator with 12336 features.\n",
      "Fitting estimator with 12236 features.\n",
      "Fitting estimator with 12136 features.\n",
      "Fitting estimator with 12036 features.\n",
      "Fitting estimator with 11936 features.\n",
      "Fitting estimator with 11836 features.\n",
      "Fitting estimator with 11736 features.\n",
      "Fitting estimator with 11636 features.\n",
      "Fitting estimator with 11536 features.\n",
      "Fitting estimator with 11436 features.\n",
      "Fitting estimator with 11336 features.\n",
      "Fitting estimator with 11236 features.\n",
      "Fitting estimator with 11136 features.\n",
      "Fitting estimator with 11036 features.\n",
      "Fitting estimator with 10936 features.\n",
      "Fitting estimator with 10836 features.\n",
      "Fitting estimator with 10736 features.\n",
      "Fitting estimator with 10636 features.\n",
      "Fitting estimator with 10536 features.\n",
      "Fitting estimator with 10436 features.\n",
      "Fitting estimator with 10336 features.\n",
      "Fitting estimator with 10236 features.\n",
      "Fitting estimator with 10136 features.\n",
      "Fitting estimator with 10036 features.\n",
      "Fitting estimator with 9936 features.\n",
      "Fitting estimator with 9836 features.\n",
      "Fitting estimator with 9736 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 9636 features.\n",
      "Fitting estimator with 9536 features.\n",
      "Fitting estimator with 9436 features.\n",
      "Fitting estimator with 9336 features.\n",
      "Fitting estimator with 9236 features.\n",
      "Fitting estimator with 9136 features.\n",
      "Fitting estimator with 9036 features.\n",
      "Fitting estimator with 8936 features.\n",
      "Fitting estimator with 8836 features.\n",
      "Fitting estimator with 8736 features.\n",
      "Fitting estimator with 8636 features.\n",
      "Fitting estimator with 8536 features.\n",
      "Fitting estimator with 8436 features.\n",
      "Fitting estimator with 8336 features.\n",
      "Fitting estimator with 8236 features.\n",
      "Fitting estimator with 8136 features.\n",
      "Fitting estimator with 8036 features.\n",
      "Fitting estimator with 7936 features.\n",
      "Fitting estimator with 7836 features.\n",
      "Fitting estimator with 7736 features.\n",
      "Fitting estimator with 7636 features.\n",
      "Fitting estimator with 7536 features.\n",
      "Fitting estimator with 7436 features.\n",
      "Fitting estimator with 7336 features.\n",
      "Fitting estimator with 7236 features.\n",
      "Fitting estimator with 7136 features.\n",
      "Fitting estimator with 7036 features.\n",
      "Fitting estimator with 6936 features.\n",
      "Fitting estimator with 6836 features.\n",
      "Fitting estimator with 6736 features.\n",
      "Fitting estimator with 6636 features.\n",
      "Fitting estimator with 6536 features.\n",
      "Fitting estimator with 6436 features.\n",
      "Fitting estimator with 6336 features.\n",
      "Fitting estimator with 6236 features.\n",
      "Fitting estimator with 6136 features.\n",
      "Fitting estimator with 6036 features.\n",
      "Fitting estimator with 5936 features.\n",
      "Fitting estimator with 5836 features.\n",
      "Fitting estimator with 5736 features.\n",
      "Fitting estimator with 5636 features.\n",
      "Fitting estimator with 5536 features.\n",
      "Fitting estimator with 5436 features.\n",
      "Fitting estimator with 5336 features.\n",
      "Fitting estimator with 5236 features.\n",
      "Fitting estimator with 5136 features.\n",
      "Fitting estimator with 5036 features.\n",
      "Fitting estimator with 4936 features.\n",
      "Fitting estimator with 4836 features.\n",
      "Fitting estimator with 4736 features.\n",
      "Fitting estimator with 4636 features.\n",
      "Fitting estimator with 4536 features.\n",
      "Fitting estimator with 4436 features.\n",
      "Fitting estimator with 4336 features.\n",
      "Fitting estimator with 4236 features.\n",
      "Fitting estimator with 4136 features.\n",
      "Fitting estimator with 4036 features.\n",
      "Fitting estimator with 3936 features.\n",
      "Fitting estimator with 3836 features.\n",
      "Fitting estimator with 3736 features.\n",
      "Fitting estimator with 3636 features.\n",
      "Fitting estimator with 3536 features.\n",
      "Fitting estimator with 3436 features.\n",
      "Fitting estimator with 3336 features.\n",
      "Fitting estimator with 3236 features.\n",
      "Fitting estimator with 3136 features.\n",
      "Fitting estimator with 3036 features.\n",
      "Fitting estimator with 2936 features.\n",
      "Fitting estimator with 2836 features.\n",
      "Fitting estimator with 2736 features.\n",
      "Fitting estimator with 2636 features.\n",
      "Fitting estimator with 2536 features.\n",
      "Fitting estimator with 2436 features.\n",
      "Fitting estimator with 2336 features.\n",
      "Fitting estimator with 2236 features.\n",
      "Fitting estimator with 2136 features.\n",
      "Fitting estimator with 2036 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.5667 pre_train:0.6812 recall_train:0.3032 F1_train:0.4196 AUC_train:0.5961\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6864\n",
      "Epoch:0002\n",
      "acc_train:0.5733 pre_train:0.7120 recall_train:0.2925 F1_train:0.4146 AUC_train:0.6352\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6628\n",
      "Epoch:0003\n",
      "acc_train:0.6044 pre_train:0.7319 recall_train:0.3699 F1_train:0.4914 AUC_train:0.6399\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6568\n",
      "Epoch:0004\n",
      "acc_train:0.5967 pre_train:0.7179 recall_train:0.3613 F1_train:0.4807 AUC_train:0.6413\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6724\n",
      "Epoch:0005\n",
      "acc_train:0.5778 pre_train:0.6481 recall_train:0.4000 F1_train:0.4947 AUC_train:0.6091\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6824\n",
      "Epoch:0006\n",
      "acc_train:0.5500 pre_train:0.5962 recall_train:0.4000 F1_train:0.4788 AUC_train:0.5762\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6928\n",
      "Epoch:0007\n",
      "acc_train:0.5178 pre_train:0.5512 recall_train:0.3591 F1_train:0.4349 AUC_train:0.5286\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7024\n",
      "Epoch:0008\n",
      "acc_train:0.6256 pre_train:0.7162 recall_train:0.4559 F1_train:0.5572 AUC_train:0.6432\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7120\n",
      "Epoch:0009\n",
      "acc_train:0.6200 pre_train:0.6965 recall_train:0.4688 F1_train:0.5604 AUC_train:0.6634\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7176\n",
      "Epoch:0010\n",
      "acc_train:0.6089 pre_train:0.6647 recall_train:0.4903 F1_train:0.5644 AUC_train:0.6612\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7296\n",
      "Epoch:0011\n",
      "acc_train:0.5989 pre_train:0.7080 recall_train:0.3806 F1_train:0.4951 AUC_train:0.6367\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7332\n",
      "Epoch:0012\n",
      "acc_train:0.6322 pre_train:0.7175 recall_train:0.4753 F1_train:0.5718 AUC_train:0.6753\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7132\n",
      "Epoch:0013\n",
      "acc_train:0.6256 pre_train:0.6988 recall_train:0.4839 F1_train:0.5718 AUC_train:0.6398\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7124\n",
      "Epoch:0014\n",
      "acc_train:0.6267 pre_train:0.6729 recall_train:0.5398 F1_train:0.5990 AUC_train:0.6606\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7084\n",
      "Epoch:0015\n",
      "acc_train:0.6378 pre_train:0.6925 recall_train:0.5376 F1_train:0.6053 AUC_train:0.6887\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7100\n",
      "Epoch:0016\n",
      "acc_train:0.6289 pre_train:0.6955 recall_train:0.5011 F1_train:0.5825 AUC_train:0.6661\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7160\n",
      "Epoch:0017\n",
      "acc_train:0.6078 pre_train:0.6333 recall_train:0.5720 F1_train:0.6011 AUC_train:0.6378\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7356\n",
      "Epoch:0018\n",
      "acc_train:0.6311 pre_train:0.6522 recall_train:0.6129 F1_train:0.6319 AUC_train:0.6768\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7292\n",
      "Epoch:0019\n",
      "acc_train:0.6389 pre_train:0.6691 recall_train:0.5957 F1_train:0.6303 AUC_train:0.6865\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7368\n",
      "Epoch:0020\n",
      "acc_train:0.6444 pre_train:0.6507 recall_train:0.6731 F1_train:0.6617 AUC_train:0.6827\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7428\n",
      "Epoch:0021\n",
      "acc_train:0.6211 pre_train:0.6325 recall_train:0.6366 F1_train:0.6345 AUC_train:0.6584\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7316\n",
      "Epoch:0022\n",
      "acc_train:0.6533 pre_train:0.6564 recall_train:0.6903 F1_train:0.6730 AUC_train:0.7024\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7268\n",
      "Epoch:0023\n",
      "acc_train:0.6433 pre_train:0.6481 recall_train:0.6774 F1_train:0.6625 AUC_train:0.6933\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7304\n",
      "Epoch:0024\n",
      "acc_train:0.6744 pre_train:0.6937 recall_train:0.6624 F1_train:0.6777 AUC_train:0.7190\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7316\n",
      "Epoch:0025\n",
      "acc_train:0.6656 pre_train:0.6806 recall_train:0.6645 F1_train:0.6725 AUC_train:0.7093\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7484\n",
      "Epoch:0026\n",
      "acc_train:0.6789 pre_train:0.6930 recall_train:0.6796 F1_train:0.6862 AUC_train:0.7309\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7528\n",
      "Epoch:0027\n",
      "acc_train:0.6556 pre_train:0.6511 recall_train:0.7183 F1_train:0.6830 AUC_train:0.7150\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7548\n",
      "Epoch:0028\n",
      "acc_train:0.6989 pre_train:0.7156 recall_train:0.6925 F1_train:0.7038 AUC_train:0.7499\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7556\n",
      "Epoch:0029\n",
      "acc_train:0.6867 pre_train:0.7113 recall_train:0.6624 F1_train:0.6860 AUC_train:0.7567\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7588\n",
      "Epoch:0030\n",
      "acc_train:0.6933 pre_train:0.7266 recall_train:0.6516 F1_train:0.6871 AUC_train:0.7561\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0031\n",
      "acc_train:0.7022 pre_train:0.7184 recall_train:0.6968 F1_train:0.7074 AUC_train:0.7689\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.7716\n",
      "Epoch:0032\n",
      "acc_train:0.6778 pre_train:0.6797 recall_train:0.7118 F1_train:0.6954 AUC_train:0.7534\n",
      "acc_val:0.5200 pre_val:0.5102 recall_val:1.0000 F1_val:0.675676 AUC_val:0.7720\n",
      "Epoch:0033\n",
      "acc_train:0.6744 pre_train:0.6629 recall_train:0.7527 F1_train:0.7049 AUC_train:0.7426\n",
      "acc_val:0.5300 pre_val:0.5158 recall_val:0.9800 F1_val:0.675862 AUC_val:0.7736\n",
      "Epoch:0034\n",
      "acc_train:0.7122 pre_train:0.7068 recall_train:0.7570 F1_train:0.7310 AUC_train:0.7937\n",
      "acc_val:0.5600 pre_val:0.5326 recall_val:0.9800 F1_val:0.690141 AUC_val:0.7768\n",
      "Epoch:0035\n",
      "acc_train:0.7389 pre_train:0.7426 recall_train:0.7570 F1_train:0.7497 AUC_train:0.8341\n",
      "acc_val:0.6400 pre_val:0.5897 recall_val:0.9200 F1_val:0.718750 AUC_val:0.7812\n",
      "Epoch:0036\n",
      "acc_train:0.7256 pre_train:0.7271 recall_train:0.7505 F1_train:0.7386 AUC_train:0.8148\n",
      "acc_val:0.6800 pre_val:0.6216 recall_val:0.9200 F1_val:0.741935 AUC_val:0.7912\n",
      "Epoch:0037\n",
      "acc_train:0.7256 pre_train:0.7300 recall_train:0.7441 F1_train:0.7370 AUC_train:0.8116\n",
      "acc_val:0.7100 pre_val:0.6522 recall_val:0.9000 F1_val:0.756303 AUC_val:0.8060\n",
      "Epoch:0038\n",
      "acc_train:0.7400 pre_train:0.7463 recall_train:0.7527 F1_train:0.7495 AUC_train:0.8297\n",
      "acc_val:0.7300 pre_val:0.6769 recall_val:0.8800 F1_val:0.765217 AUC_val:0.8148\n",
      "Epoch:0039\n",
      "acc_train:0.7700 pre_train:0.7854 recall_train:0.7634 F1_train:0.7743 AUC_train:0.8657\n",
      "acc_val:0.7900 pre_val:0.7458 recall_val:0.8800 F1_val:0.807339 AUC_val:0.8192\n",
      "Epoch:0040\n",
      "acc_train:0.7733 pre_train:0.7973 recall_train:0.7527 F1_train:0.7743 AUC_train:0.8557\n",
      "acc_val:0.7500 pre_val:0.7451 recall_val:0.7600 F1_val:0.752475 AUC_val:0.8256\n",
      "Epoch:0041\n",
      "acc_train:0.7989 pre_train:0.8381 recall_train:0.7570 F1_train:0.7955 AUC_train:0.8887\n",
      "acc_val:0.7200 pre_val:0.7750 recall_val:0.6200 F1_val:0.688889 AUC_val:0.8312\n",
      "Epoch:0042\n",
      "acc_train:0.8156 pre_train:0.8568 recall_train:0.7720 F1_train:0.8122 AUC_train:0.8906\n",
      "acc_val:0.7300 pre_val:0.8485 recall_val:0.5600 F1_val:0.674699 AUC_val:0.8316\n",
      "Epoch:0043\n",
      "acc_train:0.8278 pre_train:0.8934 recall_train:0.7570 F1_train:0.8196 AUC_train:0.9211\n",
      "acc_val:0.7400 pre_val:0.9000 recall_val:0.5400 F1_val:0.675000 AUC_val:0.8440\n",
      "Epoch:0044\n",
      "acc_train:0.8189 pre_train:0.8912 recall_train:0.7398 F1_train:0.8085 AUC_train:0.9177\n",
      "acc_val:0.7400 pre_val:0.9000 recall_val:0.5400 F1_val:0.675000 AUC_val:0.8376\n",
      "Epoch:0045\n",
      "acc_train:0.8400 pre_train:0.8983 recall_train:0.7785 F1_train:0.8341 AUC_train:0.9313\n",
      "acc_val:0.7800 pre_val:0.8889 recall_val:0.6400 F1_val:0.744186 AUC_val:0.8324\n",
      "Epoch:0046\n",
      "acc_train:0.8356 pre_train:0.8856 recall_train:0.7828 F1_train:0.8311 AUC_train:0.9263\n",
      "acc_val:0.7700 pre_val:0.8462 recall_val:0.6600 F1_val:0.741573 AUC_val:0.8364\n",
      "Epoch:0047\n",
      "acc_train:0.8622 pre_train:0.8956 recall_train:0.8301 F1_train:0.8616 AUC_train:0.9413\n",
      "acc_val:0.7900 pre_val:0.8537 recall_val:0.7000 F1_val:0.769231 AUC_val:0.8364\n",
      "Epoch:0048\n",
      "acc_train:0.8689 pre_train:0.9082 recall_train:0.8301 F1_train:0.8674 AUC_train:0.9492\n",
      "acc_val:0.7700 pre_val:0.8293 recall_val:0.6800 F1_val:0.747253 AUC_val:0.8452\n",
      "Epoch:0049\n",
      "acc_train:0.8644 pre_train:0.9113 recall_train:0.8172 F1_train:0.8617 AUC_train:0.9459\n",
      "acc_val:0.7700 pre_val:0.8000 recall_val:0.7200 F1_val:0.757895 AUC_val:0.8428\n",
      "Epoch:0050\n",
      "acc_train:0.8744 pre_train:0.9055 recall_train:0.8452 F1_train:0.8743 AUC_train:0.9491\n",
      "acc_val:0.7600 pre_val:0.7500 recall_val:0.7800 F1_val:0.764706 AUC_val:0.8300\n",
      "Epoch:0051\n",
      "acc_train:0.8989 pre_train:0.9349 recall_train:0.8645 F1_train:0.8983 AUC_train:0.9623\n",
      "acc_val:0.7800 pre_val:0.7692 recall_val:0.8000 F1_val:0.784314 AUC_val:0.8552\n",
      "Epoch:0052\n",
      "acc_train:0.8889 pre_train:0.9356 recall_train:0.8430 F1_train:0.8869 AUC_train:0.9592\n",
      "acc_val:0.7900 pre_val:0.7843 recall_val:0.8000 F1_val:0.792079 AUC_val:0.8692\n",
      "Epoch:0053\n",
      "acc_train:0.8844 pre_train:0.9227 recall_train:0.8473 F1_train:0.8834 AUC_train:0.9595\n",
      "acc_val:0.8000 pre_val:0.8000 recall_val:0.8000 F1_val:0.800000 AUC_val:0.8748\n",
      "Epoch:0054\n",
      "acc_train:0.8889 pre_train:0.9398 recall_train:0.8387 F1_train:0.8864 AUC_train:0.9629\n",
      "acc_val:0.7900 pre_val:0.8085 recall_val:0.7600 F1_val:0.783505 AUC_val:0.8712\n",
      "Epoch:0055\n",
      "acc_train:0.9133 pre_train:0.9596 recall_train:0.8688 F1_train:0.9120 AUC_train:0.9694\n",
      "acc_val:0.7800 pre_val:0.7692 recall_val:0.8000 F1_val:0.784314 AUC_val:0.8648\n",
      "Epoch:0056\n",
      "acc_train:0.9022 pre_train:0.9435 recall_train:0.8624 F1_train:0.9011 AUC_train:0.9760\n",
      "acc_val:0.7700 pre_val:0.7455 recall_val:0.8200 F1_val:0.780952 AUC_val:0.8520\n",
      "Epoch:0057\n",
      "acc_train:0.9033 pre_train:0.9479 recall_train:0.8602 F1_train:0.9019 AUC_train:0.9720\n",
      "acc_val:0.7600 pre_val:0.7826 recall_val:0.7200 F1_val:0.750000 AUC_val:0.8556\n",
      "Epoch:0058\n",
      "acc_train:0.9244 pre_train:0.9649 recall_train:0.8860 F1_train:0.9238 AUC_train:0.9796\n",
      "acc_val:0.7900 pre_val:0.8718 recall_val:0.6800 F1_val:0.764045 AUC_val:0.8520\n",
      "Epoch:0059\n",
      "acc_train:0.9144 pre_train:0.9641 recall_train:0.8667 F1_train:0.9128 AUC_train:0.9770\n",
      "acc_val:0.7700 pre_val:0.8649 recall_val:0.6400 F1_val:0.735632 AUC_val:0.8452\n",
      "Epoch:0060\n",
      "acc_train:0.9144 pre_train:0.9686 recall_train:0.8624 F1_train:0.9124 AUC_train:0.9797\n",
      "acc_val:0.7700 pre_val:0.8462 recall_val:0.6600 F1_val:0.741573 AUC_val:0.8432\n",
      "Epoch:0061\n",
      "acc_train:0.9267 pre_train:0.9761 recall_train:0.8796 F1_train:0.9253 AUC_train:0.9842\n",
      "acc_val:0.7700 pre_val:0.8140 recall_val:0.7000 F1_val:0.752688 AUC_val:0.8472\n",
      "Epoch:0062\n",
      "acc_train:0.9367 pre_train:0.9700 recall_train:0.9054 F1_train:0.9366 AUC_train:0.9778\n",
      "acc_val:0.7700 pre_val:0.8000 recall_val:0.7200 F1_val:0.757895 AUC_val:0.8488\n",
      "Epoch:0063\n",
      "acc_train:0.9311 pre_train:0.9764 recall_train:0.8882 F1_train:0.9302 AUC_train:0.9884\n",
      "acc_val:0.7900 pre_val:0.8085 recall_val:0.7600 F1_val:0.783505 AUC_val:0.8548\n",
      "Epoch:0064\n",
      "acc_train:0.9456 pre_train:0.9771 recall_train:0.9161 F1_train:0.9456 AUC_train:0.9843\n",
      "acc_val:0.7800 pre_val:0.7917 recall_val:0.7600 F1_val:0.775510 AUC_val:0.8604\n",
      "Epoch:0065\n",
      "acc_train:0.9400 pre_train:0.9681 recall_train:0.9140 F1_train:0.9403 AUC_train:0.9810\n",
      "acc_val:0.7900 pre_val:0.7843 recall_val:0.8000 F1_val:0.792079 AUC_val:0.8668\n",
      "Epoch:0066\n",
      "acc_train:0.9478 pre_train:0.9686 recall_train:0.9290 F1_train:0.9484 AUC_train:0.9880\n",
      "acc_val:0.7900 pre_val:0.7736 recall_val:0.8200 F1_val:0.796117 AUC_val:0.8664\n",
      "Epoch:0067\n",
      "acc_train:0.9422 pre_train:0.9640 recall_train:0.9226 F1_train:0.9429 AUC_train:0.9887\n",
      "acc_val:0.7900 pre_val:0.7736 recall_val:0.8200 F1_val:0.796117 AUC_val:0.8688\n",
      "Epoch:0068\n",
      "acc_train:0.9522 pre_train:0.9752 recall_train:0.9312 F1_train:0.9527 AUC_train:0.9913\n",
      "acc_val:0.8000 pre_val:0.7885 recall_val:0.8200 F1_val:0.803922 AUC_val:0.8696\n",
      "Epoch:0069\n",
      "acc_train:0.9411 pre_train:0.9598 recall_train:0.9247 F1_train:0.9419 AUC_train:0.9899\n",
      "acc_val:0.8000 pre_val:0.7885 recall_val:0.8200 F1_val:0.803922 AUC_val:0.8664\n",
      "Epoch:0070\n",
      "acc_train:0.9489 pre_train:0.9708 recall_train:0.9290 F1_train:0.9495 AUC_train:0.9920\n",
      "acc_val:0.8000 pre_val:0.7885 recall_val:0.8200 F1_val:0.803922 AUC_val:0.8692\n",
      "Epoch:0071\n",
      "acc_train:0.9633 pre_train:0.9821 recall_train:0.9462 F1_train:0.9639 AUC_train:0.9909\n",
      "acc_val:0.8100 pre_val:0.8039 recall_val:0.8200 F1_val:0.811881 AUC_val:0.8716\n",
      "Epoch:0072\n",
      "acc_train:0.9622 pre_train:0.9675 recall_train:0.9591 F1_train:0.9633 AUC_train:0.9930\n",
      "acc_val:0.7800 pre_val:0.8182 recall_val:0.7200 F1_val:0.765957 AUC_val:0.8724\n",
      "Epoch:0073\n",
      "acc_train:0.9567 pre_train:0.9776 recall_train:0.9376 F1_train:0.9572 AUC_train:0.9911\n",
      "acc_val:0.7700 pre_val:0.7872 recall_val:0.7400 F1_val:0.762887 AUC_val:0.8684\n",
      "Epoch:0074\n",
      "acc_train:0.9622 pre_train:0.9778 recall_train:0.9484 F1_train:0.9629 AUC_train:0.9949\n",
      "acc_val:0.7800 pre_val:0.7917 recall_val:0.7600 F1_val:0.775510 AUC_val:0.8664\n",
      "Epoch:0075\n",
      "acc_train:0.9744 pre_train:0.9868 recall_train:0.9634 F1_train:0.9750 AUC_train:0.9950\n",
      "acc_val:0.8000 pre_val:0.8000 recall_val:0.8000 F1_val:0.800000 AUC_val:0.8620\n",
      "Epoch:0076\n",
      "acc_train:0.9600 pre_train:0.9735 recall_train:0.9484 F1_train:0.9608 AUC_train:0.9945\n",
      "acc_val:0.7900 pre_val:0.7959 recall_val:0.7800 F1_val:0.787879 AUC_val:0.8628\n",
      "Epoch:0077\n",
      "acc_train:0.9767 pre_train:0.9912 recall_train:0.9634 F1_train:0.9771 AUC_train:0.9959\n",
      "acc_val:0.7700 pre_val:0.7872 recall_val:0.7400 F1_val:0.762887 AUC_val:0.8732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0078\n",
      "acc_train:0.9622 pre_train:0.9757 recall_train:0.9505 F1_train:0.9630 AUC_train:0.9913\n",
      "acc_val:0.7800 pre_val:0.8043 recall_val:0.7400 F1_val:0.770833 AUC_val:0.8732\n",
      "Epoch:0079\n",
      "acc_train:0.9600 pre_train:0.9799 recall_train:0.9419 F1_train:0.9605 AUC_train:0.9956\n",
      "acc_val:0.7900 pre_val:0.8222 recall_val:0.7400 F1_val:0.778947 AUC_val:0.8732\n",
      "Epoch:0080\n",
      "acc_train:0.9722 pre_train:0.9825 recall_train:0.9634 F1_train:0.9729 AUC_train:0.9933\n",
      "acc_val:0.7900 pre_val:0.8222 recall_val:0.7400 F1_val:0.778947 AUC_val:0.8760\n",
      "Epoch:0081\n",
      "acc_train:0.9656 pre_train:0.9865 recall_train:0.9462 F1_train:0.9660 AUC_train:0.9969\n",
      "acc_val:0.8000 pre_val:0.8000 recall_val:0.8000 F1_val:0.800000 AUC_val:0.8744\n",
      "Epoch:0082\n",
      "acc_train:0.9633 pre_train:0.9758 recall_train:0.9527 F1_train:0.9641 AUC_train:0.9963\n",
      "acc_val:0.7700 pre_val:0.7368 recall_val:0.8400 F1_val:0.785047 AUC_val:0.8644\n",
      "Epoch:0083\n",
      "acc_train:0.9789 pre_train:0.9934 recall_train:0.9656 F1_train:0.9793 AUC_train:0.9957\n",
      "acc_val:0.7300 pre_val:0.6949 recall_val:0.8200 F1_val:0.752294 AUC_val:0.8536\n",
      "Epoch:0084\n",
      "acc_train:0.9667 pre_train:0.9739 recall_train:0.9613 F1_train:0.9675 AUC_train:0.9961\n",
      "acc_val:0.7700 pre_val:0.7368 recall_val:0.8400 F1_val:0.785047 AUC_val:0.8672\n",
      "Epoch:0085\n",
      "acc_train:0.9622 pre_train:0.9675 recall_train:0.9591 F1_train:0.9633 AUC_train:0.9939\n",
      "acc_val:0.7900 pre_val:0.7959 recall_val:0.7800 F1_val:0.787879 AUC_val:0.8776\n",
      "Epoch:0086\n",
      "acc_train:0.9733 pre_train:0.9825 recall_train:0.9656 F1_train:0.9740 AUC_train:0.9934\n",
      "acc_val:0.8100 pre_val:0.8298 recall_val:0.7800 F1_val:0.804124 AUC_val:0.8836\n",
      "Epoch:0087\n",
      "acc_train:0.9689 pre_train:0.9802 recall_train:0.9591 F1_train:0.9696 AUC_train:0.9959\n",
      "acc_val:0.8000 pre_val:0.8125 recall_val:0.7800 F1_val:0.795918 AUC_val:0.8776\n",
      "Epoch:0088\n",
      "acc_train:0.9789 pre_train:0.9806 recall_train:0.9785 F1_train:0.9795 AUC_train:0.9979\n",
      "acc_val:0.7700 pre_val:0.7547 recall_val:0.8000 F1_val:0.776699 AUC_val:0.8648\n",
      "Epoch:0089\n",
      "acc_train:0.9722 pre_train:0.9783 recall_train:0.9677 F1_train:0.9730 AUC_train:0.9975\n",
      "acc_val:0.7400 pre_val:0.7069 recall_val:0.8200 F1_val:0.759259 AUC_val:0.8428\n",
      "Epoch:0090\n",
      "acc_train:0.9767 pre_train:0.9890 recall_train:0.9656 F1_train:0.9771 AUC_train:0.9956\n",
      "acc_val:0.7600 pre_val:0.7407 recall_val:0.8000 F1_val:0.769231 AUC_val:0.8476\n",
      "Epoch:0091\n",
      "acc_train:0.9689 pre_train:0.9866 recall_train:0.9527 F1_train:0.9694 AUC_train:0.9934\n",
      "acc_val:0.7800 pre_val:0.7917 recall_val:0.7600 F1_val:0.775510 AUC_val:0.8776\n",
      "Epoch:0092\n",
      "acc_train:0.9767 pre_train:0.9890 recall_train:0.9656 F1_train:0.9771 AUC_train:0.9954\n",
      "acc_val:0.8000 pre_val:0.8261 recall_val:0.7600 F1_val:0.791667 AUC_val:0.8832\n",
      "Epoch:0093\n",
      "acc_train:0.9800 pre_train:0.9869 recall_train:0.9742 F1_train:0.9805 AUC_train:0.9962\n",
      "acc_val:0.8000 pre_val:0.8261 recall_val:0.7600 F1_val:0.791667 AUC_val:0.8852\n",
      "Epoch:0094\n",
      "acc_train:0.9722 pre_train:0.9825 recall_train:0.9634 F1_train:0.9729 AUC_train:0.9961\n",
      "acc_val:0.8000 pre_val:0.7778 recall_val:0.8400 F1_val:0.807692 AUC_val:0.8788\n",
      "Epoch:0095\n",
      "acc_train:0.9822 pre_train:0.9912 recall_train:0.9742 F1_train:0.9826 AUC_train:0.9989\n",
      "acc_val:0.7800 pre_val:0.7500 recall_val:0.8400 F1_val:0.792453 AUC_val:0.8680\n",
      "Epoch:0096\n",
      "acc_train:0.9844 pre_train:0.9913 recall_train:0.9785 F1_train:0.9848 AUC_train:0.9990\n",
      "acc_val:0.7600 pre_val:0.7241 recall_val:0.8400 F1_val:0.777778 AUC_val:0.8636\n",
      "Epoch:0097\n",
      "acc_train:0.9778 pre_train:0.9764 recall_train:0.9806 F1_train:0.9785 AUC_train:0.9987\n",
      "acc_val:0.7800 pre_val:0.7500 recall_val:0.8400 F1_val:0.792453 AUC_val:0.8668\n",
      "Epoch:0098\n",
      "acc_train:0.9800 pre_train:0.9806 recall_train:0.9806 F1_train:0.9806 AUC_train:0.9982\n",
      "acc_val:0.7900 pre_val:0.7736 recall_val:0.8200 F1_val:0.796117 AUC_val:0.8780\n",
      "Epoch:0099\n",
      "acc_train:0.9800 pre_train:0.9786 recall_train:0.9828 F1_train:0.9807 AUC_train:0.9979\n",
      "acc_val:0.8000 pre_val:0.8125 recall_val:0.7800 F1_val:0.795918 AUC_val:0.8796\n",
      "Epoch:0100\n",
      "acc_train:0.9756 pre_train:0.9784 recall_train:0.9742 F1_train:0.9763 AUC_train:0.9973\n",
      "acc_val:0.7800 pre_val:0.7917 recall_val:0.7600 F1_val:0.775510 AUC_val:0.8764\n",
      "Epoch:0101\n",
      "acc_train:0.9811 pre_train:0.9828 recall_train:0.9806 F1_train:0.9817 AUC_train:0.9974\n",
      "acc_val:0.7800 pre_val:0.7692 recall_val:0.8000 F1_val:0.784314 AUC_val:0.8636\n",
      "Epoch:0102\n",
      "acc_train:0.9844 pre_train:0.9913 recall_train:0.9785 F1_train:0.9848 AUC_train:0.9963\n",
      "acc_val:0.7600 pre_val:0.7500 recall_val:0.7800 F1_val:0.764706 AUC_val:0.8464\n",
      "Epoch:0103\n",
      "acc_train:0.9800 pre_train:0.9912 recall_train:0.9699 F1_train:0.9804 AUC_train:0.9968\n",
      "acc_val:0.7600 pre_val:0.7500 recall_val:0.7800 F1_val:0.764706 AUC_val:0.8512\n",
      "Epoch:0104\n",
      "acc_train:0.9889 pre_train:0.9935 recall_train:0.9849 F1_train:0.9892 AUC_train:0.9982\n",
      "acc_val:0.7600 pre_val:0.7600 recall_val:0.7600 F1_val:0.760000 AUC_val:0.8772\n",
      "Epoch:0105\n",
      "acc_train:0.9889 pre_train:0.9935 recall_train:0.9849 F1_train:0.9892 AUC_train:0.9988\n",
      "acc_val:0.7800 pre_val:0.7800 recall_val:0.7800 F1_val:0.780000 AUC_val:0.8824\n",
      "Epoch:0106\n",
      "acc_train:0.9756 pre_train:0.9826 recall_train:0.9699 F1_train:0.9762 AUC_train:0.9951\n",
      "acc_val:0.7800 pre_val:0.7692 recall_val:0.8000 F1_val:0.784314 AUC_val:0.8828\n",
      "Early Stopping!!! epoch：105\n",
      " Starting the 1-4 Fold:：\n",
      "Fitting estimator with 76636 features.\n",
      "Fitting estimator with 76536 features.\n",
      "Fitting estimator with 76436 features.\n",
      "Fitting estimator with 76336 features.\n",
      "Fitting estimator with 76236 features.\n",
      "Fitting estimator with 76136 features.\n",
      "Fitting estimator with 76036 features.\n",
      "Fitting estimator with 75936 features.\n",
      "Fitting estimator with 75836 features.\n",
      "Fitting estimator with 75736 features.\n",
      "Fitting estimator with 75636 features.\n",
      "Fitting estimator with 75536 features.\n",
      "Fitting estimator with 75436 features.\n",
      "Fitting estimator with 75336 features.\n",
      "Fitting estimator with 75236 features.\n",
      "Fitting estimator with 75136 features.\n",
      "Fitting estimator with 75036 features.\n",
      "Fitting estimator with 74936 features.\n",
      "Fitting estimator with 74836 features.\n",
      "Fitting estimator with 74736 features.\n",
      "Fitting estimator with 74636 features.\n",
      "Fitting estimator with 74536 features.\n",
      "Fitting estimator with 74436 features.\n",
      "Fitting estimator with 74336 features.\n",
      "Fitting estimator with 74236 features.\n",
      "Fitting estimator with 74136 features.\n",
      "Fitting estimator with 74036 features.\n",
      "Fitting estimator with 73936 features.\n",
      "Fitting estimator with 73836 features.\n",
      "Fitting estimator with 73736 features.\n",
      "Fitting estimator with 73636 features.\n",
      "Fitting estimator with 73536 features.\n",
      "Fitting estimator with 73436 features.\n",
      "Fitting estimator with 73336 features.\n",
      "Fitting estimator with 73236 features.\n",
      "Fitting estimator with 73136 features.\n",
      "Fitting estimator with 73036 features.\n",
      "Fitting estimator with 72936 features.\n",
      "Fitting estimator with 72836 features.\n",
      "Fitting estimator with 72736 features.\n",
      "Fitting estimator with 72636 features.\n",
      "Fitting estimator with 72536 features.\n",
      "Fitting estimator with 72436 features.\n",
      "Fitting estimator with 72336 features.\n",
      "Fitting estimator with 72236 features.\n",
      "Fitting estimator with 72136 features.\n",
      "Fitting estimator with 72036 features.\n",
      "Fitting estimator with 71936 features.\n",
      "Fitting estimator with 71836 features.\n",
      "Fitting estimator with 71736 features.\n",
      "Fitting estimator with 71636 features.\n",
      "Fitting estimator with 71536 features.\n",
      "Fitting estimator with 71436 features.\n",
      "Fitting estimator with 71336 features.\n",
      "Fitting estimator with 71236 features.\n",
      "Fitting estimator with 71136 features.\n",
      "Fitting estimator with 71036 features.\n",
      "Fitting estimator with 70936 features.\n",
      "Fitting estimator with 70836 features.\n",
      "Fitting estimator with 70736 features.\n",
      "Fitting estimator with 70636 features.\n",
      "Fitting estimator with 70536 features.\n",
      "Fitting estimator with 70436 features.\n",
      "Fitting estimator with 70336 features.\n",
      "Fitting estimator with 70236 features.\n",
      "Fitting estimator with 70136 features.\n",
      "Fitting estimator with 70036 features.\n",
      "Fitting estimator with 69936 features.\n",
      "Fitting estimator with 69836 features.\n",
      "Fitting estimator with 69736 features.\n",
      "Fitting estimator with 69636 features.\n",
      "Fitting estimator with 69536 features.\n",
      "Fitting estimator with 69436 features.\n",
      "Fitting estimator with 69336 features.\n",
      "Fitting estimator with 69236 features.\n",
      "Fitting estimator with 69136 features.\n",
      "Fitting estimator with 69036 features.\n",
      "Fitting estimator with 68936 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 68836 features.\n",
      "Fitting estimator with 68736 features.\n",
      "Fitting estimator with 68636 features.\n",
      "Fitting estimator with 68536 features.\n",
      "Fitting estimator with 68436 features.\n",
      "Fitting estimator with 68336 features.\n",
      "Fitting estimator with 68236 features.\n",
      "Fitting estimator with 68136 features.\n",
      "Fitting estimator with 68036 features.\n",
      "Fitting estimator with 67936 features.\n",
      "Fitting estimator with 67836 features.\n",
      "Fitting estimator with 67736 features.\n",
      "Fitting estimator with 67636 features.\n",
      "Fitting estimator with 67536 features.\n",
      "Fitting estimator with 67436 features.\n",
      "Fitting estimator with 67336 features.\n",
      "Fitting estimator with 67236 features.\n",
      "Fitting estimator with 67136 features.\n",
      "Fitting estimator with 67036 features.\n",
      "Fitting estimator with 66936 features.\n",
      "Fitting estimator with 66836 features.\n",
      "Fitting estimator with 66736 features.\n",
      "Fitting estimator with 66636 features.\n",
      "Fitting estimator with 66536 features.\n",
      "Fitting estimator with 66436 features.\n",
      "Fitting estimator with 66336 features.\n",
      "Fitting estimator with 66236 features.\n",
      "Fitting estimator with 66136 features.\n",
      "Fitting estimator with 66036 features.\n",
      "Fitting estimator with 65936 features.\n",
      "Fitting estimator with 65836 features.\n",
      "Fitting estimator with 65736 features.\n",
      "Fitting estimator with 65636 features.\n",
      "Fitting estimator with 65536 features.\n",
      "Fitting estimator with 65436 features.\n",
      "Fitting estimator with 65336 features.\n",
      "Fitting estimator with 65236 features.\n",
      "Fitting estimator with 65136 features.\n",
      "Fitting estimator with 65036 features.\n",
      "Fitting estimator with 64936 features.\n",
      "Fitting estimator with 64836 features.\n",
      "Fitting estimator with 64736 features.\n",
      "Fitting estimator with 64636 features.\n",
      "Fitting estimator with 64536 features.\n",
      "Fitting estimator with 64436 features.\n",
      "Fitting estimator with 64336 features.\n",
      "Fitting estimator with 64236 features.\n",
      "Fitting estimator with 64136 features.\n",
      "Fitting estimator with 64036 features.\n",
      "Fitting estimator with 63936 features.\n",
      "Fitting estimator with 63836 features.\n",
      "Fitting estimator with 63736 features.\n",
      "Fitting estimator with 63636 features.\n",
      "Fitting estimator with 63536 features.\n",
      "Fitting estimator with 63436 features.\n",
      "Fitting estimator with 63336 features.\n",
      "Fitting estimator with 63236 features.\n",
      "Fitting estimator with 63136 features.\n",
      "Fitting estimator with 63036 features.\n",
      "Fitting estimator with 62936 features.\n",
      "Fitting estimator with 62836 features.\n",
      "Fitting estimator with 62736 features.\n",
      "Fitting estimator with 62636 features.\n",
      "Fitting estimator with 62536 features.\n",
      "Fitting estimator with 62436 features.\n",
      "Fitting estimator with 62336 features.\n",
      "Fitting estimator with 62236 features.\n",
      "Fitting estimator with 62136 features.\n",
      "Fitting estimator with 62036 features.\n",
      "Fitting estimator with 61936 features.\n",
      "Fitting estimator with 61836 features.\n",
      "Fitting estimator with 61736 features.\n",
      "Fitting estimator with 61636 features.\n",
      "Fitting estimator with 61536 features.\n",
      "Fitting estimator with 61436 features.\n",
      "Fitting estimator with 61336 features.\n",
      "Fitting estimator with 61236 features.\n",
      "Fitting estimator with 61136 features.\n",
      "Fitting estimator with 61036 features.\n",
      "Fitting estimator with 60936 features.\n",
      "Fitting estimator with 60836 features.\n",
      "Fitting estimator with 60736 features.\n",
      "Fitting estimator with 60636 features.\n",
      "Fitting estimator with 60536 features.\n",
      "Fitting estimator with 60436 features.\n",
      "Fitting estimator with 60336 features.\n",
      "Fitting estimator with 60236 features.\n",
      "Fitting estimator with 60136 features.\n",
      "Fitting estimator with 60036 features.\n",
      "Fitting estimator with 59936 features.\n",
      "Fitting estimator with 59836 features.\n",
      "Fitting estimator with 59736 features.\n",
      "Fitting estimator with 59636 features.\n",
      "Fitting estimator with 59536 features.\n",
      "Fitting estimator with 59436 features.\n",
      "Fitting estimator with 59336 features.\n",
      "Fitting estimator with 59236 features.\n",
      "Fitting estimator with 59136 features.\n",
      "Fitting estimator with 59036 features.\n",
      "Fitting estimator with 58936 features.\n",
      "Fitting estimator with 58836 features.\n",
      "Fitting estimator with 58736 features.\n",
      "Fitting estimator with 58636 features.\n",
      "Fitting estimator with 58536 features.\n",
      "Fitting estimator with 58436 features.\n",
      "Fitting estimator with 58336 features.\n",
      "Fitting estimator with 58236 features.\n",
      "Fitting estimator with 58136 features.\n",
      "Fitting estimator with 58036 features.\n",
      "Fitting estimator with 57936 features.\n",
      "Fitting estimator with 57836 features.\n",
      "Fitting estimator with 57736 features.\n",
      "Fitting estimator with 57636 features.\n",
      "Fitting estimator with 57536 features.\n",
      "Fitting estimator with 57436 features.\n",
      "Fitting estimator with 57336 features.\n",
      "Fitting estimator with 57236 features.\n",
      "Fitting estimator with 57136 features.\n",
      "Fitting estimator with 57036 features.\n",
      "Fitting estimator with 56936 features.\n",
      "Fitting estimator with 56836 features.\n",
      "Fitting estimator with 56736 features.\n",
      "Fitting estimator with 56636 features.\n",
      "Fitting estimator with 56536 features.\n",
      "Fitting estimator with 56436 features.\n",
      "Fitting estimator with 56336 features.\n",
      "Fitting estimator with 56236 features.\n",
      "Fitting estimator with 56136 features.\n",
      "Fitting estimator with 56036 features.\n",
      "Fitting estimator with 55936 features.\n",
      "Fitting estimator with 55836 features.\n",
      "Fitting estimator with 55736 features.\n",
      "Fitting estimator with 55636 features.\n",
      "Fitting estimator with 55536 features.\n",
      "Fitting estimator with 55436 features.\n",
      "Fitting estimator with 55336 features.\n",
      "Fitting estimator with 55236 features.\n",
      "Fitting estimator with 55136 features.\n",
      "Fitting estimator with 55036 features.\n",
      "Fitting estimator with 54936 features.\n",
      "Fitting estimator with 54836 features.\n",
      "Fitting estimator with 54736 features.\n",
      "Fitting estimator with 54636 features.\n",
      "Fitting estimator with 54536 features.\n",
      "Fitting estimator with 54436 features.\n",
      "Fitting estimator with 54336 features.\n",
      "Fitting estimator with 54236 features.\n",
      "Fitting estimator with 54136 features.\n",
      "Fitting estimator with 54036 features.\n",
      "Fitting estimator with 53936 features.\n",
      "Fitting estimator with 53836 features.\n",
      "Fitting estimator with 53736 features.\n",
      "Fitting estimator with 53636 features.\n",
      "Fitting estimator with 53536 features.\n",
      "Fitting estimator with 53436 features.\n",
      "Fitting estimator with 53336 features.\n",
      "Fitting estimator with 53236 features.\n",
      "Fitting estimator with 53136 features.\n",
      "Fitting estimator with 53036 features.\n",
      "Fitting estimator with 52936 features.\n",
      "Fitting estimator with 52836 features.\n",
      "Fitting estimator with 52736 features.\n",
      "Fitting estimator with 52636 features.\n",
      "Fitting estimator with 52536 features.\n",
      "Fitting estimator with 52436 features.\n",
      "Fitting estimator with 52336 features.\n",
      "Fitting estimator with 52236 features.\n",
      "Fitting estimator with 52136 features.\n",
      "Fitting estimator with 52036 features.\n",
      "Fitting estimator with 51936 features.\n",
      "Fitting estimator with 51836 features.\n",
      "Fitting estimator with 51736 features.\n",
      "Fitting estimator with 51636 features.\n",
      "Fitting estimator with 51536 features.\n",
      "Fitting estimator with 51436 features.\n",
      "Fitting estimator with 51336 features.\n",
      "Fitting estimator with 51236 features.\n",
      "Fitting estimator with 51136 features.\n",
      "Fitting estimator with 51036 features.\n",
      "Fitting estimator with 50936 features.\n",
      "Fitting estimator with 50836 features.\n",
      "Fitting estimator with 50736 features.\n",
      "Fitting estimator with 50636 features.\n",
      "Fitting estimator with 50536 features.\n",
      "Fitting estimator with 50436 features.\n",
      "Fitting estimator with 50336 features.\n",
      "Fitting estimator with 50236 features.\n",
      "Fitting estimator with 50136 features.\n",
      "Fitting estimator with 50036 features.\n",
      "Fitting estimator with 49936 features.\n",
      "Fitting estimator with 49836 features.\n",
      "Fitting estimator with 49736 features.\n",
      "Fitting estimator with 49636 features.\n",
      "Fitting estimator with 49536 features.\n",
      "Fitting estimator with 49436 features.\n",
      "Fitting estimator with 49336 features.\n",
      "Fitting estimator with 49236 features.\n",
      "Fitting estimator with 49136 features.\n",
      "Fitting estimator with 49036 features.\n",
      "Fitting estimator with 48936 features.\n",
      "Fitting estimator with 48836 features.\n",
      "Fitting estimator with 48736 features.\n",
      "Fitting estimator with 48636 features.\n",
      "Fitting estimator with 48536 features.\n",
      "Fitting estimator with 48436 features.\n",
      "Fitting estimator with 48336 features.\n",
      "Fitting estimator with 48236 features.\n",
      "Fitting estimator with 48136 features.\n",
      "Fitting estimator with 48036 features.\n",
      "Fitting estimator with 47936 features.\n",
      "Fitting estimator with 47836 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 47736 features.\n",
      "Fitting estimator with 47636 features.\n",
      "Fitting estimator with 47536 features.\n",
      "Fitting estimator with 47436 features.\n",
      "Fitting estimator with 47336 features.\n",
      "Fitting estimator with 47236 features.\n",
      "Fitting estimator with 47136 features.\n",
      "Fitting estimator with 47036 features.\n",
      "Fitting estimator with 46936 features.\n",
      "Fitting estimator with 46836 features.\n",
      "Fitting estimator with 46736 features.\n",
      "Fitting estimator with 46636 features.\n",
      "Fitting estimator with 46536 features.\n",
      "Fitting estimator with 46436 features.\n",
      "Fitting estimator with 46336 features.\n",
      "Fitting estimator with 46236 features.\n",
      "Fitting estimator with 46136 features.\n",
      "Fitting estimator with 46036 features.\n",
      "Fitting estimator with 45936 features.\n",
      "Fitting estimator with 45836 features.\n",
      "Fitting estimator with 45736 features.\n",
      "Fitting estimator with 45636 features.\n",
      "Fitting estimator with 45536 features.\n",
      "Fitting estimator with 45436 features.\n",
      "Fitting estimator with 45336 features.\n",
      "Fitting estimator with 45236 features.\n",
      "Fitting estimator with 45136 features.\n",
      "Fitting estimator with 45036 features.\n",
      "Fitting estimator with 44936 features.\n",
      "Fitting estimator with 44836 features.\n",
      "Fitting estimator with 44736 features.\n",
      "Fitting estimator with 44636 features.\n",
      "Fitting estimator with 44536 features.\n",
      "Fitting estimator with 44436 features.\n",
      "Fitting estimator with 44336 features.\n",
      "Fitting estimator with 44236 features.\n",
      "Fitting estimator with 44136 features.\n",
      "Fitting estimator with 44036 features.\n",
      "Fitting estimator with 43936 features.\n",
      "Fitting estimator with 43836 features.\n",
      "Fitting estimator with 43736 features.\n",
      "Fitting estimator with 43636 features.\n",
      "Fitting estimator with 43536 features.\n",
      "Fitting estimator with 43436 features.\n",
      "Fitting estimator with 43336 features.\n",
      "Fitting estimator with 43236 features.\n",
      "Fitting estimator with 43136 features.\n",
      "Fitting estimator with 43036 features.\n",
      "Fitting estimator with 42936 features.\n",
      "Fitting estimator with 42836 features.\n",
      "Fitting estimator with 42736 features.\n",
      "Fitting estimator with 42636 features.\n",
      "Fitting estimator with 42536 features.\n",
      "Fitting estimator with 42436 features.\n",
      "Fitting estimator with 42336 features.\n",
      "Fitting estimator with 42236 features.\n",
      "Fitting estimator with 42136 features.\n",
      "Fitting estimator with 42036 features.\n",
      "Fitting estimator with 41936 features.\n",
      "Fitting estimator with 41836 features.\n",
      "Fitting estimator with 41736 features.\n",
      "Fitting estimator with 41636 features.\n",
      "Fitting estimator with 41536 features.\n",
      "Fitting estimator with 41436 features.\n",
      "Fitting estimator with 41336 features.\n",
      "Fitting estimator with 41236 features.\n",
      "Fitting estimator with 41136 features.\n",
      "Fitting estimator with 41036 features.\n",
      "Fitting estimator with 40936 features.\n",
      "Fitting estimator with 40836 features.\n",
      "Fitting estimator with 40736 features.\n",
      "Fitting estimator with 40636 features.\n",
      "Fitting estimator with 40536 features.\n",
      "Fitting estimator with 40436 features.\n",
      "Fitting estimator with 40336 features.\n",
      "Fitting estimator with 40236 features.\n",
      "Fitting estimator with 40136 features.\n",
      "Fitting estimator with 40036 features.\n",
      "Fitting estimator with 39936 features.\n",
      "Fitting estimator with 39836 features.\n",
      "Fitting estimator with 39736 features.\n",
      "Fitting estimator with 39636 features.\n",
      "Fitting estimator with 39536 features.\n",
      "Fitting estimator with 39436 features.\n",
      "Fitting estimator with 39336 features.\n",
      "Fitting estimator with 39236 features.\n",
      "Fitting estimator with 39136 features.\n",
      "Fitting estimator with 39036 features.\n",
      "Fitting estimator with 38936 features.\n",
      "Fitting estimator with 38836 features.\n",
      "Fitting estimator with 38736 features.\n",
      "Fitting estimator with 38636 features.\n",
      "Fitting estimator with 38536 features.\n",
      "Fitting estimator with 38436 features.\n",
      "Fitting estimator with 38336 features.\n",
      "Fitting estimator with 38236 features.\n",
      "Fitting estimator with 38136 features.\n",
      "Fitting estimator with 38036 features.\n",
      "Fitting estimator with 37936 features.\n",
      "Fitting estimator with 37836 features.\n",
      "Fitting estimator with 37736 features.\n",
      "Fitting estimator with 37636 features.\n",
      "Fitting estimator with 37536 features.\n",
      "Fitting estimator with 37436 features.\n",
      "Fitting estimator with 37336 features.\n",
      "Fitting estimator with 37236 features.\n",
      "Fitting estimator with 37136 features.\n",
      "Fitting estimator with 37036 features.\n",
      "Fitting estimator with 36936 features.\n",
      "Fitting estimator with 36836 features.\n",
      "Fitting estimator with 36736 features.\n",
      "Fitting estimator with 36636 features.\n",
      "Fitting estimator with 36536 features.\n",
      "Fitting estimator with 36436 features.\n",
      "Fitting estimator with 36336 features.\n",
      "Fitting estimator with 36236 features.\n",
      "Fitting estimator with 36136 features.\n",
      "Fitting estimator with 36036 features.\n",
      "Fitting estimator with 35936 features.\n",
      "Fitting estimator with 35836 features.\n",
      "Fitting estimator with 35736 features.\n",
      "Fitting estimator with 35636 features.\n",
      "Fitting estimator with 35536 features.\n",
      "Fitting estimator with 35436 features.\n",
      "Fitting estimator with 35336 features.\n",
      "Fitting estimator with 35236 features.\n",
      "Fitting estimator with 35136 features.\n",
      "Fitting estimator with 35036 features.\n",
      "Fitting estimator with 34936 features.\n",
      "Fitting estimator with 34836 features.\n",
      "Fitting estimator with 34736 features.\n",
      "Fitting estimator with 34636 features.\n",
      "Fitting estimator with 34536 features.\n",
      "Fitting estimator with 34436 features.\n",
      "Fitting estimator with 34336 features.\n",
      "Fitting estimator with 34236 features.\n",
      "Fitting estimator with 34136 features.\n",
      "Fitting estimator with 34036 features.\n",
      "Fitting estimator with 33936 features.\n",
      "Fitting estimator with 33836 features.\n",
      "Fitting estimator with 33736 features.\n",
      "Fitting estimator with 33636 features.\n",
      "Fitting estimator with 33536 features.\n",
      "Fitting estimator with 33436 features.\n",
      "Fitting estimator with 33336 features.\n",
      "Fitting estimator with 33236 features.\n",
      "Fitting estimator with 33136 features.\n",
      "Fitting estimator with 33036 features.\n",
      "Fitting estimator with 32936 features.\n",
      "Fitting estimator with 32836 features.\n",
      "Fitting estimator with 32736 features.\n",
      "Fitting estimator with 32636 features.\n",
      "Fitting estimator with 32536 features.\n",
      "Fitting estimator with 32436 features.\n",
      "Fitting estimator with 32336 features.\n",
      "Fitting estimator with 32236 features.\n",
      "Fitting estimator with 32136 features.\n",
      "Fitting estimator with 32036 features.\n",
      "Fitting estimator with 31936 features.\n",
      "Fitting estimator with 31836 features.\n",
      "Fitting estimator with 31736 features.\n",
      "Fitting estimator with 31636 features.\n",
      "Fitting estimator with 31536 features.\n",
      "Fitting estimator with 31436 features.\n",
      "Fitting estimator with 31336 features.\n",
      "Fitting estimator with 31236 features.\n",
      "Fitting estimator with 31136 features.\n",
      "Fitting estimator with 31036 features.\n",
      "Fitting estimator with 30936 features.\n",
      "Fitting estimator with 30836 features.\n",
      "Fitting estimator with 30736 features.\n",
      "Fitting estimator with 30636 features.\n",
      "Fitting estimator with 30536 features.\n",
      "Fitting estimator with 30436 features.\n",
      "Fitting estimator with 30336 features.\n",
      "Fitting estimator with 30236 features.\n",
      "Fitting estimator with 30136 features.\n",
      "Fitting estimator with 30036 features.\n",
      "Fitting estimator with 29936 features.\n",
      "Fitting estimator with 29836 features.\n",
      "Fitting estimator with 29736 features.\n",
      "Fitting estimator with 29636 features.\n",
      "Fitting estimator with 29536 features.\n",
      "Fitting estimator with 29436 features.\n",
      "Fitting estimator with 29336 features.\n",
      "Fitting estimator with 29236 features.\n",
      "Fitting estimator with 29136 features.\n",
      "Fitting estimator with 29036 features.\n",
      "Fitting estimator with 28936 features.\n",
      "Fitting estimator with 28836 features.\n",
      "Fitting estimator with 28736 features.\n",
      "Fitting estimator with 28636 features.\n",
      "Fitting estimator with 28536 features.\n",
      "Fitting estimator with 28436 features.\n",
      "Fitting estimator with 28336 features.\n",
      "Fitting estimator with 28236 features.\n",
      "Fitting estimator with 28136 features.\n",
      "Fitting estimator with 28036 features.\n",
      "Fitting estimator with 27936 features.\n",
      "Fitting estimator with 27836 features.\n",
      "Fitting estimator with 27736 features.\n",
      "Fitting estimator with 27636 features.\n",
      "Fitting estimator with 27536 features.\n",
      "Fitting estimator with 27436 features.\n",
      "Fitting estimator with 27336 features.\n",
      "Fitting estimator with 27236 features.\n",
      "Fitting estimator with 27136 features.\n",
      "Fitting estimator with 27036 features.\n",
      "Fitting estimator with 26936 features.\n",
      "Fitting estimator with 26836 features.\n",
      "Fitting estimator with 26736 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 26636 features.\n",
      "Fitting estimator with 26536 features.\n",
      "Fitting estimator with 26436 features.\n",
      "Fitting estimator with 26336 features.\n",
      "Fitting estimator with 26236 features.\n",
      "Fitting estimator with 26136 features.\n",
      "Fitting estimator with 26036 features.\n",
      "Fitting estimator with 25936 features.\n",
      "Fitting estimator with 25836 features.\n",
      "Fitting estimator with 25736 features.\n",
      "Fitting estimator with 25636 features.\n",
      "Fitting estimator with 25536 features.\n",
      "Fitting estimator with 25436 features.\n",
      "Fitting estimator with 25336 features.\n",
      "Fitting estimator with 25236 features.\n",
      "Fitting estimator with 25136 features.\n",
      "Fitting estimator with 25036 features.\n",
      "Fitting estimator with 24936 features.\n",
      "Fitting estimator with 24836 features.\n",
      "Fitting estimator with 24736 features.\n",
      "Fitting estimator with 24636 features.\n",
      "Fitting estimator with 24536 features.\n",
      "Fitting estimator with 24436 features.\n",
      "Fitting estimator with 24336 features.\n",
      "Fitting estimator with 24236 features.\n",
      "Fitting estimator with 24136 features.\n",
      "Fitting estimator with 24036 features.\n",
      "Fitting estimator with 23936 features.\n",
      "Fitting estimator with 23836 features.\n",
      "Fitting estimator with 23736 features.\n",
      "Fitting estimator with 23636 features.\n",
      "Fitting estimator with 23536 features.\n",
      "Fitting estimator with 23436 features.\n",
      "Fitting estimator with 23336 features.\n",
      "Fitting estimator with 23236 features.\n",
      "Fitting estimator with 23136 features.\n",
      "Fitting estimator with 23036 features.\n",
      "Fitting estimator with 22936 features.\n",
      "Fitting estimator with 22836 features.\n",
      "Fitting estimator with 22736 features.\n",
      "Fitting estimator with 22636 features.\n",
      "Fitting estimator with 22536 features.\n",
      "Fitting estimator with 22436 features.\n",
      "Fitting estimator with 22336 features.\n",
      "Fitting estimator with 22236 features.\n",
      "Fitting estimator with 22136 features.\n",
      "Fitting estimator with 22036 features.\n",
      "Fitting estimator with 21936 features.\n",
      "Fitting estimator with 21836 features.\n",
      "Fitting estimator with 21736 features.\n",
      "Fitting estimator with 21636 features.\n",
      "Fitting estimator with 21536 features.\n",
      "Fitting estimator with 21436 features.\n",
      "Fitting estimator with 21336 features.\n",
      "Fitting estimator with 21236 features.\n",
      "Fitting estimator with 21136 features.\n",
      "Fitting estimator with 21036 features.\n",
      "Fitting estimator with 20936 features.\n",
      "Fitting estimator with 20836 features.\n",
      "Fitting estimator with 20736 features.\n",
      "Fitting estimator with 20636 features.\n",
      "Fitting estimator with 20536 features.\n",
      "Fitting estimator with 20436 features.\n",
      "Fitting estimator with 20336 features.\n",
      "Fitting estimator with 20236 features.\n",
      "Fitting estimator with 20136 features.\n",
      "Fitting estimator with 20036 features.\n",
      "Fitting estimator with 19936 features.\n",
      "Fitting estimator with 19836 features.\n",
      "Fitting estimator with 19736 features.\n",
      "Fitting estimator with 19636 features.\n",
      "Fitting estimator with 19536 features.\n",
      "Fitting estimator with 19436 features.\n",
      "Fitting estimator with 19336 features.\n",
      "Fitting estimator with 19236 features.\n",
      "Fitting estimator with 19136 features.\n",
      "Fitting estimator with 19036 features.\n",
      "Fitting estimator with 18936 features.\n",
      "Fitting estimator with 18836 features.\n",
      "Fitting estimator with 18736 features.\n",
      "Fitting estimator with 18636 features.\n",
      "Fitting estimator with 18536 features.\n",
      "Fitting estimator with 18436 features.\n",
      "Fitting estimator with 18336 features.\n",
      "Fitting estimator with 18236 features.\n",
      "Fitting estimator with 18136 features.\n",
      "Fitting estimator with 18036 features.\n",
      "Fitting estimator with 17936 features.\n",
      "Fitting estimator with 17836 features.\n",
      "Fitting estimator with 17736 features.\n",
      "Fitting estimator with 17636 features.\n",
      "Fitting estimator with 17536 features.\n",
      "Fitting estimator with 17436 features.\n",
      "Fitting estimator with 17336 features.\n",
      "Fitting estimator with 17236 features.\n",
      "Fitting estimator with 17136 features.\n",
      "Fitting estimator with 17036 features.\n",
      "Fitting estimator with 16936 features.\n",
      "Fitting estimator with 16836 features.\n",
      "Fitting estimator with 16736 features.\n",
      "Fitting estimator with 16636 features.\n",
      "Fitting estimator with 16536 features.\n",
      "Fitting estimator with 16436 features.\n",
      "Fitting estimator with 16336 features.\n",
      "Fitting estimator with 16236 features.\n",
      "Fitting estimator with 16136 features.\n",
      "Fitting estimator with 16036 features.\n",
      "Fitting estimator with 15936 features.\n",
      "Fitting estimator with 15836 features.\n",
      "Fitting estimator with 15736 features.\n",
      "Fitting estimator with 15636 features.\n",
      "Fitting estimator with 15536 features.\n",
      "Fitting estimator with 15436 features.\n",
      "Fitting estimator with 15336 features.\n",
      "Fitting estimator with 15236 features.\n",
      "Fitting estimator with 15136 features.\n",
      "Fitting estimator with 15036 features.\n",
      "Fitting estimator with 14936 features.\n",
      "Fitting estimator with 14836 features.\n",
      "Fitting estimator with 14736 features.\n",
      "Fitting estimator with 14636 features.\n",
      "Fitting estimator with 14536 features.\n",
      "Fitting estimator with 14436 features.\n",
      "Fitting estimator with 14336 features.\n",
      "Fitting estimator with 14236 features.\n",
      "Fitting estimator with 14136 features.\n",
      "Fitting estimator with 14036 features.\n",
      "Fitting estimator with 13936 features.\n",
      "Fitting estimator with 13836 features.\n",
      "Fitting estimator with 13736 features.\n",
      "Fitting estimator with 13636 features.\n",
      "Fitting estimator with 13536 features.\n",
      "Fitting estimator with 13436 features.\n",
      "Fitting estimator with 13336 features.\n",
      "Fitting estimator with 13236 features.\n",
      "Fitting estimator with 13136 features.\n",
      "Fitting estimator with 13036 features.\n",
      "Fitting estimator with 12936 features.\n",
      "Fitting estimator with 12836 features.\n",
      "Fitting estimator with 12736 features.\n",
      "Fitting estimator with 12636 features.\n",
      "Fitting estimator with 12536 features.\n",
      "Fitting estimator with 12436 features.\n",
      "Fitting estimator with 12336 features.\n",
      "Fitting estimator with 12236 features.\n",
      "Fitting estimator with 12136 features.\n",
      "Fitting estimator with 12036 features.\n",
      "Fitting estimator with 11936 features.\n",
      "Fitting estimator with 11836 features.\n",
      "Fitting estimator with 11736 features.\n",
      "Fitting estimator with 11636 features.\n",
      "Fitting estimator with 11536 features.\n",
      "Fitting estimator with 11436 features.\n",
      "Fitting estimator with 11336 features.\n",
      "Fitting estimator with 11236 features.\n",
      "Fitting estimator with 11136 features.\n",
      "Fitting estimator with 11036 features.\n",
      "Fitting estimator with 10936 features.\n",
      "Fitting estimator with 10836 features.\n",
      "Fitting estimator with 10736 features.\n",
      "Fitting estimator with 10636 features.\n",
      "Fitting estimator with 10536 features.\n",
      "Fitting estimator with 10436 features.\n",
      "Fitting estimator with 10336 features.\n",
      "Fitting estimator with 10236 features.\n",
      "Fitting estimator with 10136 features.\n",
      "Fitting estimator with 10036 features.\n",
      "Fitting estimator with 9936 features.\n",
      "Fitting estimator with 9836 features.\n",
      "Fitting estimator with 9736 features.\n",
      "Fitting estimator with 9636 features.\n",
      "Fitting estimator with 9536 features.\n",
      "Fitting estimator with 9436 features.\n",
      "Fitting estimator with 9336 features.\n",
      "Fitting estimator with 9236 features.\n",
      "Fitting estimator with 9136 features.\n",
      "Fitting estimator with 9036 features.\n",
      "Fitting estimator with 8936 features.\n",
      "Fitting estimator with 8836 features.\n",
      "Fitting estimator with 8736 features.\n",
      "Fitting estimator with 8636 features.\n",
      "Fitting estimator with 8536 features.\n",
      "Fitting estimator with 8436 features.\n",
      "Fitting estimator with 8336 features.\n",
      "Fitting estimator with 8236 features.\n",
      "Fitting estimator with 8136 features.\n",
      "Fitting estimator with 8036 features.\n",
      "Fitting estimator with 7936 features.\n",
      "Fitting estimator with 7836 features.\n",
      "Fitting estimator with 7736 features.\n",
      "Fitting estimator with 7636 features.\n",
      "Fitting estimator with 7536 features.\n",
      "Fitting estimator with 7436 features.\n",
      "Fitting estimator with 7336 features.\n",
      "Fitting estimator with 7236 features.\n",
      "Fitting estimator with 7136 features.\n",
      "Fitting estimator with 7036 features.\n",
      "Fitting estimator with 6936 features.\n",
      "Fitting estimator with 6836 features.\n",
      "Fitting estimator with 6736 features.\n",
      "Fitting estimator with 6636 features.\n",
      "Fitting estimator with 6536 features.\n",
      "Fitting estimator with 6436 features.\n",
      "Fitting estimator with 6336 features.\n",
      "Fitting estimator with 6236 features.\n",
      "Fitting estimator with 6136 features.\n",
      "Fitting estimator with 6036 features.\n",
      "Fitting estimator with 5936 features.\n",
      "Fitting estimator with 5836 features.\n",
      "Fitting estimator with 5736 features.\n",
      "Fitting estimator with 5636 features.\n",
      "Fitting estimator with 5536 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 5436 features.\n",
      "Fitting estimator with 5336 features.\n",
      "Fitting estimator with 5236 features.\n",
      "Fitting estimator with 5136 features.\n",
      "Fitting estimator with 5036 features.\n",
      "Fitting estimator with 4936 features.\n",
      "Fitting estimator with 4836 features.\n",
      "Fitting estimator with 4736 features.\n",
      "Fitting estimator with 4636 features.\n",
      "Fitting estimator with 4536 features.\n",
      "Fitting estimator with 4436 features.\n",
      "Fitting estimator with 4336 features.\n",
      "Fitting estimator with 4236 features.\n",
      "Fitting estimator with 4136 features.\n",
      "Fitting estimator with 4036 features.\n",
      "Fitting estimator with 3936 features.\n",
      "Fitting estimator with 3836 features.\n",
      "Fitting estimator with 3736 features.\n",
      "Fitting estimator with 3636 features.\n",
      "Fitting estimator with 3536 features.\n",
      "Fitting estimator with 3436 features.\n",
      "Fitting estimator with 3336 features.\n",
      "Fitting estimator with 3236 features.\n",
      "Fitting estimator with 3136 features.\n",
      "Fitting estimator with 3036 features.\n",
      "Fitting estimator with 2936 features.\n",
      "Fitting estimator with 2836 features.\n",
      "Fitting estimator with 2736 features.\n",
      "Fitting estimator with 2636 features.\n",
      "Fitting estimator with 2536 features.\n",
      "Fitting estimator with 2436 features.\n",
      "Fitting estimator with 2336 features.\n",
      "Fitting estimator with 2236 features.\n",
      "Fitting estimator with 2136 features.\n",
      "Fitting estimator with 2036 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001\n",
      "acc_train:0.4378 pre_train:0.4685 recall_train:0.6559 F1_train:0.5466 AUC_train:0.3913\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.3176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0002\n",
      "acc_train:0.4978 pre_train:0.5103 recall_train:0.6925 F1_train:0.5876 AUC_train:0.5002\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.4948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0003\n",
      "acc_train:0.4989 pre_train:0.5126 recall_train:0.6108 F1_train:0.5574 AUC_train:0.5206\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.5452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0004\n",
      "acc_train:0.5767 pre_train:0.5875 recall_train:0.6065 F1_train:0.5968 AUC_train:0.5976\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.5824\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0005\n",
      "acc_train:0.5789 pre_train:0.5991 recall_train:0.5591 F1_train:0.5784 AUC_train:0.6212\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.5924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0006\n",
      "acc_train:0.6244 pre_train:0.6591 recall_train:0.5656 F1_train:0.6088 AUC_train:0.6395\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.5876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0007\n",
      "acc_train:0.6233 pre_train:0.6522 recall_train:0.5806 F1_train:0.6143 AUC_train:0.6439\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.5900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0008\n",
      "acc_train:0.6167 pre_train:0.6442 recall_train:0.5763 F1_train:0.6084 AUC_train:0.6374\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.5960\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0009\n",
      "acc_train:0.6322 pre_train:0.6650 recall_train:0.5806 F1_train:0.6200 AUC_train:0.6690\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0010\n",
      "acc_train:0.6233 pre_train:0.6552 recall_train:0.5720 F1_train:0.6108 AUC_train:0.6612\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.5996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0011\n",
      "acc_train:0.6311 pre_train:0.6802 recall_train:0.5398 F1_train:0.6019 AUC_train:0.6634\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.6104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0012\n",
      "acc_train:0.6067 pre_train:0.6364 recall_train:0.5570 F1_train:0.5940 AUC_train:0.6410\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.6144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0013\n",
      "acc_train:0.6344 pre_train:0.6762 recall_train:0.5613 F1_train:0.6134 AUC_train:0.6732\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.6236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0014\n",
      "acc_train:0.6100 pre_train:0.6432 recall_train:0.5505 F1_train:0.5933 AUC_train:0.6391\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.6272\n",
      "Epoch:0015\n",
      "acc_train:0.6133 pre_train:0.6481 recall_train:0.5505 F1_train:0.5953 AUC_train:0.6448\n",
      "acc_val:0.5400 pre_val:1.0000 recall_val:0.0800 F1_val:0.148148 AUC_val:0.6472\n",
      "Epoch:0016\n",
      "acc_train:0.6200 pre_train:0.6461 recall_train:0.5849 F1_train:0.6140 AUC_train:0.6678\n",
      "acc_val:0.5600 pre_val:0.7500 recall_val:0.1800 F1_val:0.290323 AUC_val:0.6608\n",
      "Epoch:0017\n",
      "acc_train:0.6256 pre_train:0.6488 recall_train:0.6000 F1_train:0.6235 AUC_train:0.6523\n",
      "acc_val:0.6100 pre_val:0.8235 recall_val:0.2800 F1_val:0.417910 AUC_val:0.6660\n",
      "Epoch:0018\n",
      "acc_train:0.5878 pre_train:0.6193 recall_train:0.5247 F1_train:0.5681 AUC_train:0.6274\n",
      "acc_val:0.6600 pre_val:0.8077 recall_val:0.4200 F1_val:0.552632 AUC_val:0.6724\n",
      "Epoch:0019\n",
      "acc_train:0.5878 pre_train:0.6152 recall_train:0.5398 F1_train:0.5750 AUC_train:0.6467\n",
      "acc_val:0.6400 pre_val:0.7333 recall_val:0.4400 F1_val:0.550000 AUC_val:0.6736\n",
      "Epoch:0020\n",
      "acc_train:0.6400 pre_train:0.6523 recall_train:0.6495 F1_train:0.6509 AUC_train:0.6647\n",
      "acc_val:0.6400 pre_val:0.6944 recall_val:0.5000 F1_val:0.581395 AUC_val:0.6752\n",
      "Epoch:0021\n",
      "acc_train:0.6478 pre_train:0.6745 recall_train:0.6151 F1_train:0.6434 AUC_train:0.6853\n",
      "acc_val:0.6500 pre_val:0.6923 recall_val:0.5400 F1_val:0.606742 AUC_val:0.6760\n",
      "Epoch:0022\n",
      "acc_train:0.6122 pre_train:0.6283 recall_train:0.6108 F1_train:0.6194 AUC_train:0.6407\n",
      "acc_val:0.6400 pre_val:0.6522 recall_val:0.6000 F1_val:0.625000 AUC_val:0.6776\n",
      "Epoch:0023\n",
      "acc_train:0.6522 pre_train:0.6959 recall_train:0.5806 F1_train:0.6331 AUC_train:0.6967\n",
      "acc_val:0.6300 pre_val:0.6226 recall_val:0.6600 F1_val:0.640777 AUC_val:0.6796\n",
      "Epoch:0024\n",
      "acc_train:0.6589 pre_train:0.6846 recall_train:0.6301 F1_train:0.6562 AUC_train:0.6999\n",
      "acc_val:0.6200 pre_val:0.6034 recall_val:0.7000 F1_val:0.648148 AUC_val:0.6832\n",
      "Epoch:0025\n",
      "acc_train:0.6511 pre_train:0.6846 recall_train:0.6022 F1_train:0.6407 AUC_train:0.7098\n",
      "acc_val:0.6300 pre_val:0.6000 recall_val:0.7800 F1_val:0.678261 AUC_val:0.6844\n",
      "Epoch:0026\n",
      "acc_train:0.6211 pre_train:0.6462 recall_train:0.5892 F1_train:0.6164 AUC_train:0.6647\n",
      "acc_val:0.5900 pre_val:0.5616 recall_val:0.8200 F1_val:0.666667 AUC_val:0.6876\n",
      "Epoch:0027\n",
      "acc_train:0.6356 pre_train:0.6560 recall_train:0.6194 F1_train:0.6372 AUC_train:0.6740\n",
      "acc_val:0.5500 pre_val:0.5301 recall_val:0.8800 F1_val:0.661654 AUC_val:0.6932\n",
      "Epoch:0028\n",
      "acc_train:0.6200 pre_train:0.6434 recall_train:0.5935 F1_train:0.6174 AUC_train:0.6578\n",
      "acc_val:0.5100 pre_val:0.5056 recall_val:0.9000 F1_val:0.647482 AUC_val:0.6980\n",
      "Epoch:0029\n",
      "acc_train:0.6344 pre_train:0.6518 recall_train:0.6280 F1_train:0.6396 AUC_train:0.6844\n",
      "acc_val:0.5300 pre_val:0.5165 recall_val:0.9400 F1_val:0.666667 AUC_val:0.6984\n",
      "Epoch:0030\n",
      "acc_train:0.6267 pre_train:0.6518 recall_train:0.5957 F1_train:0.6225 AUC_train:0.6629\n",
      "acc_val:0.5200 pre_val:0.5109 recall_val:0.9400 F1_val:0.661972 AUC_val:0.6984\n",
      "Epoch:0031\n",
      "acc_train:0.6300 pre_train:0.6535 recall_train:0.6043 F1_train:0.6279 AUC_train:0.6752\n",
      "acc_val:0.5100 pre_val:0.5052 recall_val:0.9800 F1_val:0.666667 AUC_val:0.7008\n",
      "Epoch:0032\n",
      "acc_train:0.6511 pre_train:0.6689 recall_train:0.6430 F1_train:0.6557 AUC_train:0.6840\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.9800 F1_val:0.662162 AUC_val:0.7016\n",
      "Epoch:0033\n",
      "acc_train:0.6356 pre_train:0.6442 recall_train:0.6581 F1_train:0.6511 AUC_train:0.6896\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.9800 F1_val:0.662162 AUC_val:0.7028\n",
      "Epoch:0034\n",
      "acc_train:0.6322 pre_train:0.6595 recall_train:0.5957 F1_train:0.6260 AUC_train:0.6817\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.9800 F1_val:0.662162 AUC_val:0.7072\n",
      "Epoch:0035\n",
      "acc_train:0.6700 pre_train:0.6909 recall_train:0.6538 F1_train:0.6718 AUC_train:0.7236\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.9800 F1_val:0.662162 AUC_val:0.7104\n",
      "Epoch:0036\n",
      "acc_train:0.6500 pre_train:0.6752 recall_train:0.6215 F1_train:0.6473 AUC_train:0.7076\n",
      "acc_val:0.5100 pre_val:0.5052 recall_val:0.9800 F1_val:0.666667 AUC_val:0.7176\n",
      "Epoch:0037\n",
      "acc_train:0.6411 pre_train:0.6621 recall_train:0.6237 F1_train:0.6423 AUC_train:0.6997\n",
      "acc_val:0.5100 pre_val:0.5052 recall_val:0.9800 F1_val:0.666667 AUC_val:0.7208\n",
      "Epoch:0038\n",
      "acc_train:0.6289 pre_train:0.6367 recall_train:0.6559 F1_train:0.6462 AUC_train:0.7022\n",
      "acc_val:0.5100 pre_val:0.5052 recall_val:0.9800 F1_val:0.666667 AUC_val:0.7204\n",
      "Epoch:0039\n",
      "acc_train:0.6456 pre_train:0.6615 recall_train:0.6430 F1_train:0.6521 AUC_train:0.7011\n",
      "acc_val:0.5100 pre_val:0.5052 recall_val:0.9800 F1_val:0.666667 AUC_val:0.7200\n",
      "Epoch:0040\n",
      "acc_train:0.6656 pre_train:0.6872 recall_train:0.6473 F1_train:0.6667 AUC_train:0.7312\n",
      "acc_val:0.5200 pre_val:0.5106 recall_val:0.9600 F1_val:0.666667 AUC_val:0.7196\n",
      "Epoch:0041\n",
      "acc_train:0.6500 pre_train:0.6752 recall_train:0.6215 F1_train:0.6473 AUC_train:0.7233\n",
      "acc_val:0.5400 pre_val:0.5227 recall_val:0.9200 F1_val:0.666667 AUC_val:0.7212\n",
      "Epoch:0042\n",
      "acc_train:0.6433 pre_train:0.6622 recall_train:0.6323 F1_train:0.6469 AUC_train:0.7181\n",
      "acc_val:0.5400 pre_val:0.5227 recall_val:0.9200 F1_val:0.666667 AUC_val:0.7216\n",
      "Epoch:0043\n",
      "acc_train:0.6533 pre_train:0.6759 recall_train:0.6323 F1_train:0.6533 AUC_train:0.7209\n",
      "acc_val:0.5400 pre_val:0.5227 recall_val:0.9200 F1_val:0.666667 AUC_val:0.7220\n",
      "Epoch:0044\n",
      "acc_train:0.6644 pre_train:0.6848 recall_train:0.6495 F1_train:0.6667 AUC_train:0.7327\n",
      "acc_val:0.5400 pre_val:0.5227 recall_val:0.9200 F1_val:0.666667 AUC_val:0.7284\n",
      "Epoch:0045\n",
      "acc_train:0.6711 pre_train:0.6865 recall_train:0.6688 F1_train:0.6776 AUC_train:0.7428\n",
      "acc_val:0.5300 pre_val:0.5176 recall_val:0.8800 F1_val:0.651852 AUC_val:0.7344\n",
      "Epoch:0046\n",
      "acc_train:0.6678 pre_train:0.6921 recall_train:0.6430 F1_train:0.6667 AUC_train:0.7514\n",
      "acc_val:0.5500 pre_val:0.5301 recall_val:0.8800 F1_val:0.661654 AUC_val:0.7360\n",
      "Epoch:0047\n",
      "acc_train:0.6867 pre_train:0.7113 recall_train:0.6624 F1_train:0.6860 AUC_train:0.7654\n",
      "acc_val:0.5600 pre_val:0.5366 recall_val:0.8800 F1_val:0.666667 AUC_val:0.7392\n",
      "Epoch:0048\n",
      "acc_train:0.7022 pre_train:0.7184 recall_train:0.6968 F1_train:0.7074 AUC_train:0.7802\n",
      "acc_val:0.5400 pre_val:0.5238 recall_val:0.8800 F1_val:0.656716 AUC_val:0.7500\n",
      "Epoch:0049\n",
      "acc_train:0.7056 pre_train:0.7315 recall_train:0.6796 F1_train:0.7046 AUC_train:0.7817\n",
      "acc_val:0.5500 pre_val:0.5287 recall_val:0.9200 F1_val:0.671533 AUC_val:0.7560\n",
      "Epoch:0050\n",
      "acc_train:0.7078 pre_train:0.7265 recall_train:0.6968 F1_train:0.7113 AUC_train:0.7913\n",
      "acc_val:0.5400 pre_val:0.5227 recall_val:0.9200 F1_val:0.666667 AUC_val:0.7732\n",
      "Epoch:0051\n",
      "acc_train:0.7311 pre_train:0.7347 recall_train:0.7505 F1_train:0.7426 AUC_train:0.8017\n",
      "acc_val:0.5200 pre_val:0.5114 recall_val:0.9000 F1_val:0.652174 AUC_val:0.7960\n",
      "Epoch:0052\n",
      "acc_train:0.7278 pre_train:0.7340 recall_train:0.7419 F1_train:0.7380 AUC_train:0.8083\n",
      "acc_val:0.5200 pre_val:0.5111 recall_val:0.9200 F1_val:0.657143 AUC_val:0.8088\n",
      "Epoch:0053\n",
      "acc_train:0.7367 pre_train:0.7545 recall_train:0.7269 F1_train:0.7404 AUC_train:0.8111\n",
      "acc_val:0.5300 pre_val:0.5165 recall_val:0.9400 F1_val:0.666667 AUC_val:0.8136\n",
      "Epoch:0054\n",
      "acc_train:0.7222 pre_train:0.7302 recall_train:0.7333 F1_train:0.7318 AUC_train:0.8208\n",
      "acc_val:0.5400 pre_val:0.5227 recall_val:0.9200 F1_val:0.666667 AUC_val:0.8168\n",
      "Epoch:0055\n",
      "acc_train:0.7822 pre_train:0.7762 recall_train:0.8129 F1_train:0.7941 AUC_train:0.8614\n",
      "acc_val:0.5600 pre_val:0.5341 recall_val:0.9400 F1_val:0.681159 AUC_val:0.8112\n",
      "Epoch:0056\n",
      "acc_train:0.8033 pre_train:0.8064 recall_train:0.8151 F1_train:0.8107 AUC_train:0.8805\n",
      "acc_val:0.5700 pre_val:0.5393 recall_val:0.9600 F1_val:0.690647 AUC_val:0.8092\n",
      "Epoch:0057\n",
      "acc_train:0.8033 pre_train:0.8130 recall_train:0.8043 F1_train:0.8086 AUC_train:0.8664\n",
      "acc_val:0.5600 pre_val:0.5341 recall_val:0.9400 F1_val:0.681159 AUC_val:0.8016\n",
      "Epoch:0058\n",
      "acc_train:0.8033 pre_train:0.7975 recall_train:0.8301 F1_train:0.8135 AUC_train:0.8792\n",
      "acc_val:0.5900 pre_val:0.5517 recall_val:0.9600 F1_val:0.700730 AUC_val:0.7956\n",
      "Epoch:0059\n",
      "acc_train:0.8167 pre_train:0.8024 recall_train:0.8559 F1_train:0.8283 AUC_train:0.8994\n",
      "acc_val:0.6000 pre_val:0.5581 recall_val:0.9600 F1_val:0.705882 AUC_val:0.7912\n",
      "Epoch:0060\n",
      "acc_train:0.8422 pre_train:0.8185 recall_train:0.8925 F1_train:0.8539 AUC_train:0.9104\n",
      "acc_val:0.6000 pre_val:0.5610 recall_val:0.9200 F1_val:0.696970 AUC_val:0.8020\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import io\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# from dataloader import dataloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "if hasattr(sys.stdout, 'buffer'):\n",
    "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.no_cuda = False\n",
    "        self.seed = 46\n",
    "        self.epochs = 200\n",
    "        self.lr = 0.001\n",
    "        self.weight_decay = 5e-5\n",
    "        self.hidden = 16\n",
    "        self.dropout = 0.2\n",
    "        self.atlas = 'cc400'\n",
    "        self.num_features = 2000\n",
    "        self.folds = 10\n",
    "        self.connectivity = 'correlation'\n",
    "        self.max_degree = 3\n",
    "        self.ngl = 4\n",
    "        self.edropout = 0.3\n",
    "        self.train = 1\n",
    "        self.ckpt_path = '../folds/rois_cc400_pth_4_layer'\n",
    "        self.early_stopping = True\n",
    "        self.early_stopping_patience = 20\n",
    "\n",
    "# Instantiate Args class\n",
    "args = Args()\n",
    "\n",
    "# Check if CUDA is available\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Create params dictionary\n",
    "params = vars(args)\n",
    "\n",
    "# Print Hyperparameters\n",
    "print('Hyperparameters:')\n",
    "for key, value in params.items():\n",
    "    print(key + \":\", value)\n",
    "\n",
    "corrects = np.zeros(args.folds, dtype=np.int32) \n",
    "accs = np.zeros(args.folds, dtype=np.float32) \n",
    "aucs = np.zeros(args.folds, dtype=np.float32)\n",
    "prfs = np.zeros([args.folds,3], dtype=np.float32) # Save Precision, Recall, F1\n",
    "test_num = np.zeros(args.folds, dtype=np.float32)\n",
    "\n",
    "\n",
    "print('  Loading dataset ...')\n",
    "dataloader = dataloader()\n",
    "raw_features, y, nonimg = dataloader.load_data(params) \n",
    "cv_splits = dataloader.data_split(args.folds)\n",
    "features=raw_features\n",
    "\n",
    "t1 = time.time()\n",
    "count=1;\n",
    "for i in range(args.folds):\n",
    "    \n",
    "    \n",
    "    \n",
    "    t_start = time.time()\n",
    "    train_ind, test_ind = cv_splits[i]\n",
    "\n",
    "    train_ind, valid_ind = train_test_split(train_ind, test_size=0.1, random_state = 24)\n",
    "    \n",
    "    cv_splits[i] = (train_ind, valid_ind)\n",
    "    cv_splits[i] = cv_splits[i] + (test_ind,)\n",
    "    print('Size of the {}-fold Training, Validation, and Test Sets:{},{},{}' .format(i+1, len(cv_splits[i][0]), len(cv_splits[i][1]), len(cv_splits[i][2])))\n",
    "\n",
    "    if args.train == 1:\n",
    "        for j in range(args.folds):\n",
    "            print(' Starting the {}-{} Fold:：'.format(i+1,j+1))\n",
    "            node_ftr = dataloader.get_node_features(train_ind)\n",
    "            edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "            edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "            \n",
    "            model = GCN(input_dim = args.num_features,\n",
    "                        nhid = args.hidden, \n",
    "                        num_classes = 2, \n",
    "                        ngl = args.ngl, \n",
    "                        dropout = args.dropout, \n",
    "                        edge_dropout = args.edropout, \n",
    "                        edgenet_input_dim = 2*nonimg.shape[1])\n",
    "            optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "            \n",
    "#             if args.cuda:\n",
    "            model\n",
    "            features = torch.tensor(node_ftr, dtype=torch.float32)\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "            edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "            labels = torch.tensor(y, dtype=torch.long)\n",
    "            fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "                \n",
    "            acc = 0\n",
    "            best_val_loss = float('inf') # early stoppping: Initialized to positive infinity\n",
    "            current_patience = 0 # early stopping: Used to record the epochs of the current early stopping\n",
    "            \n",
    "            epoch_store = []\n",
    "            acc_train_store =[]        \n",
    "            pre_train_store =[]\n",
    "            recall_train_store =[]\n",
    "            F1_train_store =[]\n",
    "            AUC_train_store =[]\n",
    "            acc_val_store=[]\n",
    "            pre_val_store=[]\n",
    "            recall_val_store=[]\n",
    "            F1_val_store=[]\n",
    "            AUC_val_store=[]\n",
    "            \n",
    "            for epoch in range(args.epochs):\n",
    "                # train\n",
    "                model.train()\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    optimizer.zero_grad()\n",
    "                    output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "                    loss_train = torch.nn.CrossEntropyLoss()(output[train_ind], labels[train_ind])\n",
    "                    loss_train.backward()\n",
    "                    optimizer.step()\n",
    "                acc_train = torchmetrics_accuracy(output[train_ind], labels[train_ind])\n",
    "                auc_train = torchmetrics_auc(output[train_ind], labels[train_ind])\n",
    "                logits_train = output[train_ind].detach().cpu().numpy()\n",
    "                prf_train = prf(logits_train, y[train_ind])\n",
    "\n",
    "                \n",
    "                # valid\n",
    "                model.eval()\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "                loss_val = torch.nn.CrossEntropyLoss()(output[valid_ind], labels[valid_ind])\n",
    "                acc_val = torchmetrics_accuracy(output[valid_ind], labels[valid_ind])\n",
    "                auc_val = torchmetrics_auc(output[valid_ind], labels[valid_ind])\n",
    "                logits_val = output[valid_ind].detach().cpu().numpy()\n",
    "                prf_val = prf(logits_val, y[valid_ind])\n",
    "\n",
    "                \n",
    "                print('Epoch:{:04d}'.format(epoch+1))\n",
    "                print('acc_train:{:.4f}'.format(acc_train),\n",
    "                      'pre_train:{:.4f}'.format(prf_train[0]),\n",
    "                      'recall_train:{:.4f}'.format(prf_train[1]),\n",
    "                      'F1_train:{:.4f}'.format(prf_train[2]),\n",
    "                      'AUC_train:{:.4f}'.format(auc_train))\n",
    "                print('acc_val:{:.4f}'.format(acc_val),\n",
    "                      'pre_val:{:.4f}'.format(prf_val[0]),\n",
    "                      'recall_val:{:.4f}'.format(prf_val[1]),\n",
    "                      'F1_val:{:4f}'.format(prf_val[2]),\n",
    "                      'AUC_val:{:.4f}'.format(auc_val))\n",
    "                \n",
    "                epoch_store.append(epoch+1)\n",
    "                acc_train_store.append(acc_train)       \n",
    "                pre_train_store.append(prf_train[0])\n",
    "                recall_train_store.append(prf_train[1])\n",
    "                F1_train_store.append(prf_train[2])\n",
    "                AUC_train_store.append(auc_train)\n",
    "                acc_val_store.append(acc_val)\n",
    "                pre_val_store.append(prf_val[0])\n",
    "                recall_val_store.append(prf_val[1])\n",
    "                F1_val_store.append(prf_val[2])\n",
    "                AUC_val_store.append(auc_val)\n",
    "                \n",
    "                # save pth\n",
    "                if acc_val > acc and epoch > 50:\n",
    "                    acc = acc_val\n",
    "                    if args.ckpt_path != '':\n",
    "                        if not os.path.exists(args.ckpt_path):\n",
    "                            os.makedirs(args.ckpt_path)\n",
    "                        torch.save(model.state_dict(), fold_model_path)\n",
    "                \n",
    "                # Early Stopping\n",
    "                if epoch > 50 and args.early_stopping == True:\n",
    "                    if loss_val < best_val_loss:\n",
    "                        best_val_loss = loss_val\n",
    "                        current_patience = 0\n",
    "                    else:\n",
    "                        current_patience += 1\n",
    "                    if current_patience >= args.early_stopping_patience:\n",
    "                        print('Early Stopping!!! epoch：{}'.format(epoch))\n",
    "                        break\n",
    "        print(\"===================================================================\",i,\"_\",j)\n",
    "        data  = { \n",
    "              \"epoch\" : epoch_store ,\n",
    "              \"acc_train\" : acc_train_store ,        \n",
    "              \"pre_train\" : pre_train_store ,\n",
    "              \"recall_train\" : recall_train_store ,\n",
    "              \"F1_train\" : F1_train_store ,\n",
    "              \"AUC_train\" : AUC_train_store ,\n",
    "              \"acc_val\" : acc_val_store,\n",
    "               \"pre_val\" : pre_val_store ,\n",
    "              \"recall_val\" : recall_val_store ,\n",
    "              \"F1_val\" : F1_val_store ,\n",
    "              \"AUC_val\" : AUC_val_store  \n",
    "        }\n",
    "        \n",
    "        \n",
    "        epoch_file_path =  f'../files/rois_cc400_4_layer/file_{i}_{j}_{count}.csv'\n",
    "        data_file = pd.DataFrame(data);\n",
    "        data_file.to_csv(epoch_file_path , index=False);\n",
    "        count=count+1;\n",
    "        # test\n",
    "        print(\"Loading the Model for the {}-th Fold:... ...\".format(i+1),\n",
    "              \"Size of samples in the test set:{}\".format(len(test_ind)))\n",
    "#         model.load_state_dict(torch.load(fold_model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "        acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "        auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "        logits_test = output[test_ind].detach().cpu().numpy()\n",
    "        correct_test = correct_num(logits_test, y[test_ind])\n",
    "        prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "        t_end = time.time()\n",
    "        t = t_end - t_start\n",
    "        print('Fold {} Results:'.format(i+1),\n",
    "              'test acc:{:.4f}'.format(acc_test),\n",
    "              'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "              'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "              'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "              'test_AUC:{:.4f}'.format(auc_test),\n",
    "              'time:{:.3f}s'.format(t))\n",
    "        \n",
    "        correct = correct_test\n",
    "        aucs[i] = auc_test\n",
    "        prfs[i] = prf_test\n",
    "        corrects[i] = correct\n",
    "        test_num[i] = len(test_ind)\n",
    "    \n",
    "    \n",
    "    if args.train == 0:\n",
    "        node_ftr = dataloader.get_node_features(train_ind)\n",
    "        edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "        edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "        \n",
    "        model = GCN(input_dim = args.num_features,\n",
    "                    nhid = args.hidden, \n",
    "                    num_classes = 2, \n",
    "                    ngl = args.ngl, \n",
    "                    dropout = args.dropout, \n",
    "                    edge_dropout = args.edropout, \n",
    "                    edgenet_input_dim = 2*nonimg.shape[1])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "#         if args.cuda\n",
    "        model\n",
    "        features = torch.tensor(node_ftr, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "        labels = torch.tensor(y, dtype=torch.long)\n",
    "        fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "        \n",
    "        model.load_state_dict(torch.load(fold_model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "        acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "        auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "        logits_test = output[test_ind].detach().cpu().numpy()\n",
    "        correct_test = correct_num(logits_test, y[test_ind])\n",
    "        prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "        t_end = time.time()\n",
    "        t = t_end - t_start\n",
    "        print('Fold {} Results:'.format(i+1),\n",
    "              'test acc:{:.4f}'.format(acc_test),\n",
    "              'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "              'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "              'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "              'test_AUC:{:.4f}'.format(auc_test),\n",
    "              'time:{:.3f}s'.format(t))\n",
    "        \n",
    "        correct = correct_test\n",
    "        aucs[i] = auc_test\n",
    "        prfs[i] = prf_test\n",
    "        corrects[i] = correct\n",
    "        test_num[i] = len(test_ind)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print('\\r\\n======Finish Results for Nested 10-fold cross-validation======')\n",
    "Nested10kCV_acc = np.sum(corrects) / np.sum(test_num)\n",
    "Nested10kCV_auc = np.mean(aucs)\n",
    "Nested10kCV_precision, Nested10kCV_recall, Nested10kCV_F1 = np.mean(prfs, axis=0)\n",
    "print('Test:',\n",
    "      'acc:{}'.format(Nested10kCV_acc),\n",
    "      'precision:{}'.format(Nested10kCV_precision),\n",
    "      'recall:{}'.format(Nested10kCV_recall),\n",
    "      'F1:{}'.format(Nested10kCV_F1),\n",
    "      'AUC:{}'.format(Nested10kCV_auc))\n",
    "print('Total duration:{}'.format(t2 - t1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efe6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import io\n",
    "# import sys\n",
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# from models import GCN\n",
    "\n",
    "# from metrics import torchmetrics_accuracy, torchmetrics_auc, correct_num, prf\n",
    "\n",
    "# # from dataloader import dataloader\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import auc\n",
    "\n",
    "# if hasattr(sys.stdout, 'buffer'):\n",
    "#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "# # Training settings\n",
    "# # parser = argparse.ArgumentParser()\n",
    "# # parser.add_argument('--no_cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "# # parser.add_argument('--seed', type=int, default=46, help='Random seed.')\n",
    "# # parser.add_argument('--epochs', type=int, default=200, help='Number of epochs to train.')\n",
    "# # parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')\n",
    "# # parser.add_argument('--weight_decay', type=float, default=5e-5, help='Weight decay (L2 loss on parameters).')\n",
    "# # parser.add_argument('--hidden', type=int, default=16, help='Number of hidden units.')\n",
    "# # parser.add_argument('--dropout', type=float, default=0.2, help='Dropout rate (1 - keep probability).')\n",
    "# # parser.add_argument('--atlas', default='cc200', help='atlas for network construction (node definition) default: ho, see preprocessed-connectomes-project.org/abide/Pipelines.html for more options ')\n",
    "# # parser.add_argument('--num_features', default=2000, type=int, help='Number of features to keep for the feature selection step (default: 2000)')\n",
    "# # parser.add_argument('--folds', default=10, type=int, help='For cross validation, specifies which fold will be used. All folds are used if set to 11 (default: 11)')\n",
    "# # parser.add_argument('--connectivity', default='correlation', help='Type of connectivity used for network construction (default: correlation, options: correlation, partial correlation, tangent)')\n",
    "# # parser.add_argument('--max_degree', type=int, default=3, help='Maximum Chebyshev polynomial degree.')\n",
    "# # parser.add_argument('--ngl', default=8, type=int, help='number of gcn hidden layders (default: 8)')\n",
    "# # parser.add_argument('--edropout', type=float, default=0.3, help='edge dropout rate')\n",
    "# # parser.add_argument('--train', default=1, type=int, help='train(default: 1) or evaluate(0)')\n",
    "# # parser.add_argument('--ckpt_path', type=str, default='./pth', help='checkpoint path to save trained models')\n",
    "# # parser.add_argument('--early_stopping', action='store_true', default=True, help='early stopping switch')\n",
    "# # parser.add_argument('--early_stopping_patience', type=int, default=20, help='early stoppng epochs')\n",
    "\n",
    "# # args = parser.parse_args()\n",
    "# # args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# # np.random.seed(args.seed)\n",
    "# # torch.manual_seed(args.seed)\n",
    "# # if args.cuda:\n",
    "# #     torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "# # params = dict()\n",
    "# # params['no_cuda'] = args.no_cuda\n",
    "# # params['seed'] = args.seed\n",
    "# # params['epochs'] = args.epochs\n",
    "# # params['lr'] = args.lr\n",
    "# # params['weight_decay'] = args.weight_decay\n",
    "# # params['hidden'] = args.hidden\n",
    "# # params['dropout'] = args.dropout\n",
    "# # params['atlas'] = args.atlas\n",
    "# # params['num_features'] = args.num_features\n",
    "# # params['folds'] = args.folds\n",
    "# # params['connectivity'] = args.connectivity\n",
    "# # params['max_degree'] = args.max_degree\n",
    "# # params['ngl'] = args.ngl\n",
    "# # params['edropout'] = args.edropout\n",
    "# # params['train'] = args.train\n",
    "# # params['ckpt_path'] = args.ckpt_path\n",
    "# # params['early_stopping'] = args.early_stopping\n",
    "# # params['early_stopping_patience'] = args.early_stopping_patience\n",
    "\n",
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.no_cuda = False\n",
    "#         self.seed = 46\n",
    "#         self.epochs = 200\n",
    "#         self.lr = 0.001\n",
    "#         self.weight_decay = 5e-5\n",
    "#         self.hidden = 16\n",
    "#         self.dropout = 0.2\n",
    "#         self.atlas = 'cc400'\n",
    "#         self.num_features = 2000\n",
    "#         self.folds = 10\n",
    "#         self.connectivity = 'correlation'\n",
    "#         self.max_degree = 3\n",
    "#         self.ngl = 8\n",
    "#         self.edropout = 0.3\n",
    "#         self.train = 0\n",
    "#         self.ckpt_path = './pth'\n",
    "#         self.early_stopping = True\n",
    "#         self.early_stopping_patience = 20\n",
    "\n",
    "# # Instantiate Args class\n",
    "# args = Args()\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# # Set random seeds\n",
    "# np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# if args.cuda:\n",
    "#     torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# # Create params dictionary\n",
    "# params = vars(args)\n",
    "\n",
    "# # Print Hyperparameters\n",
    "# print('Hyperparameters:')\n",
    "# for key, value in params.items():\n",
    "#     print(key + \":\", value)\n",
    "\n",
    "# corrects = np.zeros(args.folds, dtype=np.int32) \n",
    "# accs = np.zeros(args.folds, dtype=np.float32) \n",
    "# aucs = np.zeros(args.folds, dtype=np.float32)\n",
    "# prfs = np.zeros([args.folds,3], dtype=np.float32) # Save Precision, Recall, F1\n",
    "# test_num = np.zeros(args.folds, dtype=np.float32)\n",
    "\n",
    "# print('  Loading dataset ...')\n",
    "# dataloader = dataloader()\n",
    "# raw_features, y, nonimg = dataloader.load_data(params) \n",
    "# cv_splits = dataloader.data_split(args.folds)\n",
    "# features=raw_features\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# for i in range(args.folds):\n",
    "#     t_start = time.time()\n",
    "#     train_ind, test_ind = cv_splits[i]\n",
    "\n",
    "#     train_ind, valid_ind = train_test_split(train_ind, test_size=0.1, random_state = 24)\n",
    "    \n",
    "#     cv_splits[i] = (train_ind, valid_ind)\n",
    "#     cv_splits[i] = cv_splits[i] + (test_ind,)\n",
    "#     print('Size of the {}-fold Training, Validation, and Test Sets:{},{},{}' .format(i+1, len(cv_splits[i][0]), len(cv_splits[i][1]), len(cv_splits[i][2])))\n",
    "\n",
    "#     if args.train == 1:\n",
    "#         for j in range(args.folds):\n",
    "#             print(' Starting the {}-{} Fold:：'.format(i+1,j+1))\n",
    "#             node_ftr = dataloader.get_node_features(train_ind)\n",
    "#             edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "#             edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "            \n",
    "#             model = GCN(input_dim = args.num_features,\n",
    "#                         nhid = args.hidden, \n",
    "#                         num_classes = 2, \n",
    "#                         ngl = args.ngl, \n",
    "#                         dropout = args.dropout, \n",
    "#                         edge_dropout = args.edropout, \n",
    "#                         edgenet_input_dim = 2*nonimg.shape[1])\n",
    "#             optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "            \n",
    "# #             if args.cuda:\n",
    "#             model\n",
    "#             features = torch.tensor(node_ftr, dtype=torch.float32)\n",
    "#             edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "#             edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "#             labels = torch.tensor(y, dtype=torch.long)\n",
    "#             fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "                \n",
    "#             acc = 0\n",
    "#             best_val_loss = float('inf') # early stoppping: Initialized to positive infinity\n",
    "#             current_patience = 0 # early stopping: Used to record the epochs of the current early stopping\n",
    "\n",
    "#             for epoch in range(args.epochs):\n",
    "#                 # train\n",
    "#                 model.train()\n",
    "#                 with torch.set_grad_enabled(True):\n",
    "#                     optimizer.zero_grad()\n",
    "#                     output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#                     loss_train = torch.nn.CrossEntropyLoss()(output[train_ind], labels[train_ind])\n",
    "#                     loss_train.backward()\n",
    "#                     optimizer.step()\n",
    "#                 acc_train = torchmetrics_accuracy(output[train_ind], labels[train_ind])\n",
    "#                 auc_train = torchmetrics_auc(output[train_ind], labels[train_ind])\n",
    "#                 logits_train = output[train_ind].detach().cpu().numpy()\n",
    "#                 prf_train = prf(logits_train, y[train_ind])\n",
    "\n",
    "                \n",
    "#                 # valid\n",
    "#                 model.eval()\n",
    "#                 with torch.set_grad_enabled(False):\n",
    "#                     output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#                 loss_val = torch.nn.CrossEntropyLoss()(output[valid_ind], labels[valid_ind])\n",
    "#                 acc_val = torchmetrics_accuracy(output[valid_ind], labels[valid_ind])\n",
    "#                 auc_val = torchmetrics_auc(output[valid_ind], labels[valid_ind])\n",
    "#                 logits_val = output[valid_ind].detach().cpu().numpy()\n",
    "#                 prf_val = prf(logits_val, y[valid_ind])\n",
    "\n",
    "                \n",
    "#                 print('Epoch:{:04d}'.format(epoch+1))\n",
    "#                 print('acc_train:{:.4f}'.format(acc_train),\n",
    "#                       'pre_train:{:.4f}'.format(prf_train[0]),\n",
    "#                       'recall_train:{:.4f}'.format(prf_train[1]),\n",
    "#                       'F1_train:{:.4f}'.format(prf_train[2]),\n",
    "#                       'AUC_train:{:.4f}'.format(auc_train))\n",
    "#                 print('acc_val:{:.4f}'.format(acc_val),\n",
    "#                       'pre_val:{:.4f}'.format(prf_val[0]),\n",
    "#                       'recall_val:{:.4f}'.format(prf_val[1]),\n",
    "#                       'F1_val:{:4f}'.format(prf_val[2]),\n",
    "#                       'AUC_val:{:.4f}'.format(auc_val))\n",
    "                \n",
    "#                 # save pth\n",
    "#                 if acc_val > acc and epoch > 50:\n",
    "#                     acc = acc_val\n",
    "#                     if args.ckpt_path != '':\n",
    "#                         if not os.path.exists(args.ckpt_path):\n",
    "#                             os.makedirs(args.ckpt_path)\n",
    "#                         torch.save(model.state_dict(), fold_model_path)\n",
    "                \n",
    "#                 # Early Stopping\n",
    "#                 if epoch > 50 and args.early_stopping == True:\n",
    "#                     if loss_val < best_val_loss:\n",
    "#                         best_val_loss = loss_val\n",
    "#                         current_patience = 0\n",
    "#                     else:\n",
    "#                         current_patience += 1\n",
    "#                     if current_patience >= args.early_stopping_patience:\n",
    "#                         print('Early Stopping!!! epoch：{}'.format(epoch))\n",
    "#                         break\n",
    "                        \n",
    "#         # test\n",
    "#         print(\"Loading the Model for the {}-th Fold:... ...\".format(i+1),\n",
    "#               \"Size of samples in the test set:{}\".format(len(test_ind)))\n",
    "#         model.load_state_dict(torch.load(fold_model_path))\n",
    "#         model.eval()\n",
    "        \n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#         acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "#         auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "#         logits_test = output[test_ind].detach().cpu().numpy()\n",
    "#         correct_test = correct_num(logits_test, y[test_ind])\n",
    "#         prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "#         t_end = time.time()\n",
    "#         t = t_end - t_start\n",
    "#         print('Fold {} Results:'.format(i+1),\n",
    "#               'test acc:{:.4f}'.format(acc_test),\n",
    "#               'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "#               'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "#               'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "#               'test_AUC:{:.4f}'.format(auc_test),\n",
    "#               'time:{:.3f}s'.format(t))\n",
    "        \n",
    "#         correct = correct_test\n",
    "#         aucs[i] = auc_test\n",
    "#         prfs[i] = prf_test\n",
    "#         corrects[i] = correct\n",
    "#         test_num[i] = len(test_ind)\n",
    "    \n",
    "    \n",
    "#     if args.train == 0:\n",
    "#         node_ftr = dataloader.get_node_features(train_ind)\n",
    "#         edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "#         edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "        \n",
    "#         model = GCN(input_dim = args.num_features,\n",
    "#                     nhid = args.hidden, \n",
    "#                     num_classes = 2, \n",
    "#                     ngl = args.ngl, \n",
    "#                     dropout = args.dropout, \n",
    "#                     edge_dropout = args.edropout, \n",
    "#                     edgenet_input_dim = 2*nonimg.shape[1])\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "# #         if args.cuda\n",
    "#         model\n",
    "#         features = torch.tensor(node_ftr, dtype=torch.float)\n",
    "#         edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "#         edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "#         labels = torch.tensor(y, dtype=torch.long)\n",
    "#         fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "        \n",
    "#         model.load_state_dict(torch.load(fold_model_path))\n",
    "#         model.eval()\n",
    "        \n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#         acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "#         auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "#         logits_test = output[test_ind].detach().cpu().numpy()\n",
    "#         correct_test = correct_num(logits_test, y[test_ind])\n",
    "#         prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "#         t_end = time.time()\n",
    "#         t = t_end - t_start\n",
    "#         print('Fold {} Results:'.format(i+1),\n",
    "#               'test acc:{:.4f}'.format(acc_test),\n",
    "#               'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "#               'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "#               'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "#               'test_AUC:{:.4f}'.format(auc_test),\n",
    "#               'time:{:.3f}s'.format(t))\n",
    "        \n",
    "#         correct = correct_test\n",
    "#         aucs[i] = auc_test\n",
    "#         prfs[i] = prf_test\n",
    "#         corrects[i] = correct\n",
    "#         test_num[i] = len(test_ind)\n",
    "\n",
    "# t2 = time.time()\n",
    "\n",
    "# print('\\r\\n======Finish Results for Nested 10-fold cross-validation======')\n",
    "# Nested10kCV_acc = np.sum(corrects) / np.sum(test_num)\n",
    "# Nested10kCV_auc = np.mean(aucs)\n",
    "# Nested10kCV_precision, Nested10kCV_recall, Nested10kCV_F1 = np.mean(prfs, axis=0)\n",
    "# print('Test:',\n",
    "#       'acc:{}'.format(Nested10kCV_acc),\n",
    "#       'precision:{}'.format(Nested10kCV_precision),\n",
    "#       'recall:{}'.format(Nested10kCV_recall),\n",
    "#       'F1:{}'.format(Nested10kCV_F1),\n",
    "#       'AUC:{}'.format(Nested10kCV_auc))\n",
    "# print('Total duration:{}'.format(t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import io\n",
    "# import sys\n",
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# from models import GCN\n",
    "\n",
    "# from metrics import torchmetrics_accuracy, torchmetrics_auc, correct_num, prf\n",
    "\n",
    "# # from dataloader import dataloader\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import auc\n",
    "\n",
    "# if hasattr(sys.stdout, 'buffer'):\n",
    "#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "# # Training settings\n",
    "# # parser = argparse.ArgumentParser()\n",
    "# # parser.add_argument('--no_cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "# # parser.add_argument('--seed', type=int, default=46, help='Random seed.')\n",
    "# # parser.add_argument('--epochs', type=int, default=200, help='Number of epochs to train.')\n",
    "# # parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')\n",
    "# # parser.add_argument('--weight_decay', type=float, default=5e-5, help='Weight decay (L2 loss on parameters).')\n",
    "# # parser.add_argument('--hidden', type=int, default=16, help='Number of hidden units.')\n",
    "# # parser.add_argument('--dropout', type=float, default=0.2, help='Dropout rate (1 - keep probability).')\n",
    "# # parser.add_argument('--atlas', default='cc200', help='atlas for network construction (node definition) default: ho, see preprocessed-connectomes-project.org/abide/Pipelines.html for more options ')\n",
    "# # parser.add_argument('--num_features', default=2000, type=int, help='Number of features to keep for the feature selection step (default: 2000)')\n",
    "# # parser.add_argument('--folds', default=10, type=int, help='For cross validation, specifies which fold will be used. All folds are used if set to 11 (default: 11)')\n",
    "# # parser.add_argument('--connectivity', default='correlation', help='Type of connectivity used for network construction (default: correlation, options: correlation, partial correlation, tangent)')\n",
    "# # parser.add_argument('--max_degree', type=int, default=3, help='Maximum Chebyshev polynomial degree.')\n",
    "# # parser.add_argument('--ngl', default=8, type=int, help='number of gcn hidden layders (default: 8)')\n",
    "# # parser.add_argument('--edropout', type=float, default=0.3, help='edge dropout rate')\n",
    "# # parser.add_argument('--train', default=1, type=int, help='train(default: 1) or evaluate(0)')\n",
    "# # parser.add_argument('--ckpt_path', type=str, default='./pth', help='checkpoint path to save trained models')\n",
    "# # parser.add_argument('--early_stopping', action='store_true', default=True, help='early stopping switch')\n",
    "# # parser.add_argument('--early_stopping_patience', type=int, default=20, help='early stoppng epochs')\n",
    "\n",
    "# # args = parser.parse_args()\n",
    "# # args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# # np.random.seed(args.seed)\n",
    "# # torch.manual_seed(args.seed)\n",
    "# # if args.cuda:\n",
    "# #     torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "# # params = dict()\n",
    "# # params['no_cuda'] = args.no_cuda\n",
    "# # params['seed'] = args.seed\n",
    "# # params['epochs'] = args.epochs\n",
    "# # params['lr'] = args.lr\n",
    "# # params['weight_decay'] = args.weight_decay\n",
    "# # params['hidden'] = args.hidden\n",
    "# # params['dropout'] = args.dropout\n",
    "# # params['atlas'] = args.atlas\n",
    "# # params['num_features'] = args.num_features\n",
    "# # params['folds'] = args.folds\n",
    "# # params['connectivity'] = args.connectivity\n",
    "# # params['max_degree'] = args.max_degree\n",
    "# # params['ngl'] = args.ngl\n",
    "# # params['edropout'] = args.edropout\n",
    "# # params['train'] = args.train\n",
    "# # params['ckpt_path'] = args.ckpt_path\n",
    "# # params['early_stopping'] = args.early_stopping\n",
    "# # params['early_stopping_patience'] = args.early_stopping_patience\n",
    "\n",
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.no_cuda = False\n",
    "#         self.seed = 46\n",
    "#         self.epochs = 200\n",
    "#         self.lr = 0.001\n",
    "#         self.weight_decay = 5e-5\n",
    "#         self.hidden = 16\n",
    "#         self.dropout = 0.2\n",
    "#         self.atlas = 'cc400'\n",
    "#         self.num_features = 2000\n",
    "#         self.folds = 10\n",
    "#         self.connectivity = 'correlation'\n",
    "#         self.max_degree = 3\n",
    "#         self.ngl = 8\n",
    "#         self.edropout = 0.3\n",
    "#         self.train = 0\n",
    "#         self.ckpt_path ='./pth'\n",
    "#         self.early_stopping = True\n",
    "#         self.early_stopping_patience = 20\n",
    "\n",
    "# # Instantiate Args class\n",
    "# args = Args()\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# # Set random seeds\n",
    "# np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# if args.cuda:\n",
    "#     torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# # Create params dictionary\n",
    "# params = vars(args)\n",
    "\n",
    "# # Print Hyperparameters\n",
    "# print('Hyperparameters:')\n",
    "# for key, value in params.items():\n",
    "#     print(key + \":\", value)\n",
    "\n",
    "# corrects = np.zeros(args.folds, dtype=np.int32) \n",
    "# accs = np.zeros(args.folds, dtype=np.float32) \n",
    "# aucs = np.zeros(args.folds, dtype=np.float32)\n",
    "# prfs = np.zeros([args.folds,3], dtype=np.float32) # Save Precision, Recall, F1\n",
    "# test_num = np.zeros(args.folds, dtype=np.float32)\n",
    "\n",
    "# print('  Loading dataset ...')\n",
    "# dataloader = dataloader()\n",
    "# raw_features, y, nonimg = dataloader.load_data(params) \n",
    "# cv_splits = dataloader.data_split(args.folds)\n",
    "# features=raw_features\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# for i in range(args.folds):\n",
    "#     t_start = time.time()\n",
    "#     train_ind, test_ind = cv_splits[i]\n",
    "\n",
    "#     train_ind, valid_ind = train_test_split(train_ind, test_size=0.1, random_state = 24)\n",
    "    \n",
    "#     cv_splits[i] = (train_ind, valid_ind)\n",
    "#     cv_splits[i] = cv_splits[i] + (test_ind,)\n",
    "#     print('Size of the {}-fold Training, Validation, and Test Sets:{},{},{}' .format(i+1, len(cv_splits[i][0]), len(cv_splits[i][1]), len(cv_splits[i][2])))\n",
    "\n",
    "#     if args.train == 1:\n",
    "#         for j in range(args.folds):\n",
    "#             print(' Starting the {}-{} Fold:：'.format(i+1,j+1))\n",
    "#             node_ftr = dataloader.get_node_features(train_ind)\n",
    "#             edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "#             edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "            \n",
    "#             model = GCN(input_dim = args.num_features,\n",
    "#                         nhid = args.hidden, \n",
    "#                         num_classes = 2, \n",
    "#                         ngl = args.ngl, \n",
    "#                         dropout = args.dropout, \n",
    "#                         edge_dropout = args.edropout, \n",
    "#                         edgenet_input_dim = 2*nonimg.shape[1])\n",
    "#             optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "            \n",
    "# #             if args.cuda:\n",
    "#             model\n",
    "#             features = torch.tensor(node_ftr, dtype=torch.float32)\n",
    "#             edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "#             edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "#             labels = torch.tensor(y, dtype=torch.long)\n",
    "#             fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "                \n",
    "#             acc = 0\n",
    "#             best_val_loss = float('inf') # early stoppping: Initialized to positive infinity\n",
    "#             current_patience = 0 # early stopping: Used to record the epochs of the current early stopping\n",
    "\n",
    "#             for epoch in range(args.epochs):\n",
    "#                 # train\n",
    "#                 model.train()\n",
    "#                 with torch.set_grad_enabled(True):\n",
    "#                     optimizer.zero_grad()\n",
    "#                     output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#                     loss_train = torch.nn.CrossEntropyLoss()(output[train_ind], labels[train_ind])\n",
    "#                     loss_train.backward()\n",
    "#                     optimizer.step()\n",
    "#                 acc_train = torchmetrics_accuracy(output[train_ind], labels[train_ind])\n",
    "#                 auc_train = torchmetrics_auc(output[train_ind], labels[train_ind])\n",
    "#                 logits_train = output[train_ind].detach().cpu().numpy()\n",
    "#                 prf_train = prf(logits_train, y[train_ind])\n",
    "\n",
    "                \n",
    "#                 # valid\n",
    "#                 model.eval()\n",
    "#                 with torch.set_grad_enabled(False):\n",
    "#                     output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#                 loss_val = torch.nn.CrossEntropyLoss()(output[valid_ind], labels[valid_ind])\n",
    "#                 acc_val = torchmetrics_accuracy(output[valid_ind], labels[valid_ind])\n",
    "#                 auc_val = torchmetrics_auc(output[valid_ind], labels[valid_ind])\n",
    "#                 logits_val = output[valid_ind].detach().cpu().numpy()\n",
    "#                 prf_val = prf(logits_val, y[valid_ind])\n",
    "\n",
    "                \n",
    "#                 print('Epoch:{:04d}'.format(epoch+1))\n",
    "#                 print('acc_train:{:.4f}'.format(acc_train),\n",
    "#                       'pre_train:{:.4f}'.format(prf_train[0]),\n",
    "#                       'recall_train:{:.4f}'.format(prf_train[1]),\n",
    "#                       'F1_train:{:.4f}'.format(prf_train[2]),\n",
    "#                       'AUC_train:{:.4f}'.format(auc_train))\n",
    "#                 print('acc_val:{:.4f}'.format(acc_val),\n",
    "#                       'pre_val:{:.4f}'.format(prf_val[0]),\n",
    "#                       'recall_val:{:.4f}'.format(prf_val[1]),\n",
    "#                       'F1_val:{:4f}'.format(prf_val[2]),\n",
    "#                       'AUC_val:{:.4f}'.format(auc_val))\n",
    "                \n",
    "#                 # save pth\n",
    "#                 if acc_val > acc and epoch > 50:\n",
    "#                     acc = acc_val\n",
    "#                     if args.ckpt_path != '':\n",
    "#                         if not os.path.exists(args.ckpt_path):\n",
    "#                             os.makedirs(args.ckpt_path)\n",
    "#                         torch.save(model.state_dict(), fold_model_path)\n",
    "                \n",
    "#                 # Early Stopping\n",
    "#                 if epoch > 50 and args.early_stopping == True:\n",
    "#                     if loss_val < best_val_loss:\n",
    "#                         best_val_loss = loss_val\n",
    "#                         current_patience = 0\n",
    "#                     else:\n",
    "#                         current_patience += 1\n",
    "#                     if current_patience >= args.early_stopping_patience:\n",
    "#                         print('Early Stopping!!! epoch：{}'.format(epoch))\n",
    "#                         break\n",
    "                        \n",
    "#         # test\n",
    "#         print(\"Loading the Model for the {}-th Fold:... ...\".format(i+1),\n",
    "#               \"Size of samples in the test set:{}\".format(len(test_ind)))\n",
    "#         model.load_state_dict(torch.load(fold_model_path))\n",
    "#         model.eval()\n",
    "        \n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#         acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "#         auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "#         logits_test = output[test_ind].detach().cpu().numpy()\n",
    "#         correct_test = correct_num(logits_test, y[test_ind])\n",
    "#         prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "#         t_end = time.time()\n",
    "#         t = t_end - t_start\n",
    "#         print('Fold {} Results:'.format(i+1),\n",
    "#               'test acc:{:.4f}'.format(acc_test),\n",
    "#               'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "#               'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "#               'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "#               'test_AUC:{:.4f}'.format(auc_test),\n",
    "#               'time:{:.3f}s'.format(t))\n",
    "        \n",
    "#         correct = correct_test\n",
    "#         aucs[i] = auc_test\n",
    "#         prfs[i] = prf_test\n",
    "#         corrects[i] = correct\n",
    "#         test_num[i] = len(test_ind)\n",
    "    \n",
    "    \n",
    "#     if args.train == 0:\n",
    "#         node_ftr = dataloader.get_node_features(train_ind)\n",
    "#         edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "#         edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "        \n",
    "#         model = GCN(input_dim = args.num_features,\n",
    "#                     nhid = args.hidden, \n",
    "#                     num_classes = 2, \n",
    "#                     ngl = args.ngl, \n",
    "#                     dropout = args.dropout, \n",
    "#                     edge_dropout = args.edropout, \n",
    "#                     edgenet_input_dim = 2*nonimg.shape[1])\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "# #         if args.cuda\n",
    "#         model\n",
    "#         features = torch.tensor(node_ftr, dtype=torch.float)\n",
    "#         edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "#         edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "#         labels = torch.tensor(y, dtype=torch.long)\n",
    "#         fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "        \n",
    "#         model.load_state_dict(torch.load(fold_model_path))\n",
    "#         model.eval()\n",
    "        \n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#         acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "#         auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "#         logits_test = output[test_ind].detach().cpu().numpy()\n",
    "#         correct_test = correct_num(logits_test, y[test_ind])\n",
    "#         prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "#         t_end = time.time()\n",
    "#         t = t_end - t_start\n",
    "#         print('Fold {} Results:'.format(i+1),\n",
    "#               'test acc:{:.4f}'.format(acc_test),\n",
    "#               'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "#               'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "#               'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "#               'test_AUC:{:.4f}'.format(auc_test),\n",
    "#               'time:{:.3f}s'.format(t))\n",
    "        \n",
    "#         correct = correct_test\n",
    "#         aucs[i] = auc_test\n",
    "#         prfs[i] = prf_test\n",
    "#         corrects[i] = correct\n",
    "#         test_num[i] = len(test_ind)\n",
    "\n",
    "# t2 = time.time()\n",
    "\n",
    "# print('\\r\\n======Finish Results for Nested 10-fold cross-validation======')\n",
    "# Nested10kCV_acc = np.sum(corrects) / np.sum(test_num)\n",
    "# Nested10kCV_auc = np.mean(aucs)\n",
    "# Nested10kCV_precision, Nested10kCV_recall, Nested10kCV_F1 = np.mean(prfs, axis=0)\n",
    "# print('Test:',\n",
    "#       'acc:{}'.format(Nested10kCV_acc),\n",
    "#       'precision:{}'.format(Nested10kCV_precision),\n",
    "#       'recall:{}'.format(Nested10kCV_recall),\n",
    "#       'F1:{}'.format(Nested10kCV_F1),\n",
    "#       'AUC:{}'.format(Nested10kCV_auc))\n",
    "# print('Total duration:{}'.format(t2 - t1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03838a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
