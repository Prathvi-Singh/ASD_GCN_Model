{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5d23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from nilearn import connectome\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "# Reading and computing the input data\n",
    "\n",
    "# Selected pipeline\n",
    "pipeline = 'cpac'\n",
    "\n",
    "# Input data variables\n",
    "root_folder = '../ABIDE/'\n",
    "data_folder = os.path.join(root_folder, 'ABIDE_pcp/cpac/filt_noglobal')\n",
    "phenotype = os.path.join(root_folder, 'ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv')\n",
    "\n",
    "\n",
    "def fetch_filenames(subject_IDs, file_type):\n",
    "\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        file_type    : must be one of the available file types\n",
    "\n",
    "    returns:\n",
    "\n",
    "        filenames    : list of filetypes (same length as subject_list)\n",
    "    \"\"\"\n",
    "\n",
    "    import glob\n",
    "\n",
    "    # Specify file mappings for the possible file types\n",
    "    filemapping = {'func_preproc': '_func_preproc.nii.gz',\n",
    "                   'rois_ho': '_rois_ho.1D'}\n",
    "\n",
    "    # The list to be filled\n",
    "    filenames = []\n",
    "\n",
    "    # Fill list with requested file paths\n",
    "    for i in range(len(subject_IDs)):\n",
    "        os.chdir(data_folder)  # os.path.join(data_folder, subject_IDs[i]))\n",
    "        try:\n",
    "            filenames.append(glob.glob('*' + subject_IDs[i] + filemapping[file_type])[0])\n",
    "        except IndexError:\n",
    "            # Return N/A if subject ID is not found\n",
    "            filenames.append('N/A')\n",
    "\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# Get timeseries arrays for list of subjects\n",
    "def get_timeseries(subject_list, atlas_name):\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        atlas_name   : the atlas based on which the timeseries are generated e.g. aal, cc200\n",
    "\n",
    "    returns:\n",
    "        time_series  : list of timeseries arrays, each of shape (timepoints x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    timeseries = []\n",
    "    for i in range(len(subject_list)):\n",
    "        subject_folder = os.path.join(data_folder, subject_list[i])\n",
    "        ro_file = [f for f in os.listdir(subject_folder) if f.endswith('_rois_' + atlas_name + '.1D')]\n",
    "        fl = os.path.join(subject_folder, ro_file[0])\n",
    "        print(\"Reading timeseries file %s\" %fl)\n",
    "        timeseries.append(np.loadtxt(fl, skiprows=0))\n",
    "\n",
    "    return timeseries\n",
    "\n",
    "\n",
    "# Compute connectivity matrices\n",
    "def subject_connectivity(timeseries, subject, atlas_name, kind, save=True, save_path=data_folder):\n",
    "    \"\"\"\n",
    "        timeseries   : timeseries table for subject (timepoints x regions)\n",
    "        subject      : the subject ID\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        save         : save the connectivity matrix to a file\n",
    "        save_path    : specify path to save the matrix if different from subject folder\n",
    "\n",
    "    returns:\n",
    "        connectivity : connectivity matrix (regions x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Estimating %s matrix for subject %s\" % (kind, subject))\n",
    "\n",
    "    if kind in ['tangent', 'partial correlation', 'correlation']:\n",
    "        conn_measure = connectome.ConnectivityMeasure(kind=kind)\n",
    "        connectivity = conn_measure.fit_transform([timeseries])[0]\n",
    "\n",
    "    if save:\n",
    "        subject_file = os.path.join(save_path, subject,\n",
    "                                    subject + '_' + atlas_name + '_' + kind.replace(' ', '_') + '.mat')\n",
    "        sio.savemat(subject_file, {'connectivity': connectivity})\n",
    "\n",
    "    return connectivity\n",
    "\n",
    "\n",
    "# Get the list of subject IDs\n",
    "def get_ids(num_subjects=None):\n",
    "    \"\"\"\n",
    "\n",
    "    return:\n",
    "        subject_IDs    : list of all subject IDs\n",
    "    \"\"\"\n",
    "\n",
    "    subject_IDs = np.genfromtxt(os.path.join(data_folder, 'subject_IDs.txt'), dtype=str)\n",
    "\n",
    "    if num_subjects is not None:\n",
    "        subject_IDs = subject_IDs[:num_subjects]\n",
    "\n",
    "    return subject_IDs\n",
    "\n",
    "\n",
    "# Get phenotype values for a list of subjects\n",
    "def get_subject_score(subject_list, score):\n",
    "    scores_dict = {}\n",
    "\n",
    "    with open(phenotype) as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            if row['SUB_ID'] in subject_list:\n",
    "                scores_dict[row['SUB_ID']] = row[score]\n",
    "\n",
    "    return scores_dict\n",
    "\n",
    "\n",
    "# Dimensionality reduction step for the feature vector using a ridge classifier\n",
    "def feature_selection(matrix, labels, train_ind, fnum):\n",
    "    \"\"\"\n",
    "        matrix       : feature matrix (num_subjects x num_features)\n",
    "        labels       : ground truth labels (num_subjects x 1)\n",
    "        train_ind    : indices of the training samples\n",
    "        fnum         : size of the feature vector after feature selection\n",
    "\n",
    "    return:\n",
    "        x_data      : feature matrix of lower dimension (num_subjects x fnum)\n",
    "    \"\"\"\n",
    "\n",
    "    estimator = RidgeClassifier()\n",
    "    selector = RFE(estimator, n_features_to_select=fnum, step=100, verbose=1)\n",
    "\n",
    "    featureX = matrix[train_ind, :]\n",
    "    featureY = labels[train_ind]\n",
    "    selector = selector.fit(featureX, featureY.ravel())\n",
    "    x_data = selector.transform(matrix)\n",
    "\n",
    "    print(\"Number of labeled samples %d\" % len(train_ind))\n",
    "    print(\"Number of features selected %d\" % x_data.shape[1])\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "# Make sure each site is represented in the training set when selecting a subset of the training set\n",
    "def site_percentage(train_ind, perc, subject_list):\n",
    "    \"\"\"\n",
    "        train_ind    : indices of the training samples\n",
    "        perc         : percentage of training set used\n",
    "        subject_list : list of subject IDs\n",
    "\n",
    "    return:\n",
    "        labeled_indices      : indices of the subset of training samples\n",
    "    \"\"\"\n",
    "\n",
    "    train_list = subject_list[train_ind]\n",
    "    sites = get_subject_score(train_list, score='SITE_ID')\n",
    "    unique = np.unique(list(sites.values())).tolist()\n",
    "    site = np.array([unique.index(sites[train_list[x]]) for x in range(len(train_list))])\n",
    "\n",
    "    labeled_indices = []\n",
    "\n",
    "    for i in np.unique(site):\n",
    "        id_in_site = np.argwhere(site == i).flatten()\n",
    "\n",
    "        num_nodes = len(id_in_site)\n",
    "        labeled_num = int(round(perc * num_nodes))\n",
    "        labeled_indices.extend(train_ind[id_in_site[:labeled_num]])\n",
    "\n",
    "    return labeled_indices\n",
    "\n",
    "\n",
    "# Load precomputed fMRI connectivity networks\n",
    "def get_networks(subject_list, kind, atlas_name=\"aal\", variable='connectivity'):\n",
    "    \"\"\"\n",
    "        subject_list : list of subject IDs\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        variable     : variable name in the .mat file that has been used to save the precomputed networks\n",
    "\n",
    "\n",
    "    return:\n",
    "        matrix      : feature matrix of connectivity networks (num_subjects x network_size)\n",
    "    \"\"\"\n",
    "\n",
    "    all_networks = []\n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_cc400_2_a/matrix_rois_cc400_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_cc400_2_a/matrix_rois_cc400_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks.append(matrix)\n",
    "            \n",
    "            \n",
    "    # all_networks=np.array(all_networks)\n",
    "\n",
    "    idx = np.triu_indices_from(all_networks[0], 1)\n",
    "    norm_networks = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks]\n",
    "    vec_networks = [mat[idx] for mat in norm_networks]\n",
    "    matrix = np.vstack(vec_networks)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# Construct the adjacency matrix of the population from phenotypic scores\n",
    "def create_affinity_graph_from_scores(scores, pd_dict):\n",
    "    num_nodes = len(pd_dict[scores[0]]) \n",
    "    graph = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for l in scores:\n",
    "        label_dict = pd_dict[l]\n",
    "\n",
    "        if l in ['AGE_AT_SCAN', 'FIQ']:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    try:\n",
    "                        val = abs(float(label_dict[k]) - float(label_dict[j]))\n",
    "                        if val < 2:\n",
    "                            graph[k, j] += 1\n",
    "                            graph[j, k] += 1\n",
    "                    except ValueError:  # missing label\n",
    "                        pass\n",
    "\n",
    "        else:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    if label_dict[k] == label_dict[j]:\n",
    "                        graph[k, j] += 1\n",
    "                        graph[j, k] += 1\n",
    "\n",
    "    return graph\n",
    "\n",
    "def get_static_affinity_adj(features, pd_dict):\n",
    "    pd_affinity = create_affinity_graph_from_scores(['SEX', 'SITE_ID'], pd_dict) \n",
    "    distv = distance.pdist(features, metric='correlation') \n",
    "    dist = distance.squareform(distv)  \n",
    "    sigma = np.mean(dist)\n",
    "    feature_sim = np.exp(- dist ** 2 / (2 * sigma ** 2))\n",
    "    adj = pd_affinity * feature_sim  \n",
    "\n",
    "    return adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e2f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_21384\\3838381434.py:8: DeprecationWarning: Please use `eigsh` from the `scipy.sparse.linalg` namespace, the `scipy.sparse.linalg.eigen` namespace is deprecated.\n",
      "  from scipy.sparse.linalg.eigen import eigsh\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.spatial import distance\n",
    "from scipy.sparse.linalg.eigen import eigsh\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def get_train_test_masks(labels, idx_train, idx_val, idx_test):\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def load_data(subject_IDs, params): \n",
    "    \n",
    "    # labels\n",
    "    num_classes = 2\n",
    "    num_nodes = len(subject_IDs)\n",
    "    \n",
    "    # , y\n",
    "    y_data = np.zeros([num_nodes, num_classes])\n",
    "    y = np.zeros([num_nodes, 1])\n",
    "    \n",
    "    labels = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "    features = get_networks(subject_IDs, kind=params['connectivity'], atlas_name=params['atlas'])\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        y_data[i, int(labels[subject_IDs[i]]) - 1] = 1 # (871,2)\n",
    "        y[i] = int(labels[subject_IDs[i]]) # (871,)\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    cv_splits = list(skf.split(features, np.squeeze(y)))\n",
    "    train = cv_splits[params['folds']][0]\n",
    "    test = cv_splits[params['folds']][1]\n",
    "    val = test\n",
    "    \n",
    "    print('Number of train sample:{}' .format(len(train)))\n",
    "        \n",
    "    y_train, y_val, y_test, train_mask, val_mask, test_mask = get_train_test_masks(y_data, train, val, test)\n",
    "    \n",
    "    y_data = torch.LongTensor(np.where(y_data)[1])\n",
    "    y = torch.LongTensor(y)\n",
    "    y_train = torch.LongTensor(y_train[1])\n",
    "    y_val = torch.LongTensor(y_val[1])\n",
    "    y_test = torch.LongTensor(y_test[1])\n",
    "    \n",
    "    train = torch.LongTensor(train)\n",
    "    val = torch.LongTensor(val)\n",
    "    test = torch.LongTensor(test)\n",
    "    train_mask = torch.LongTensor(train_mask)\n",
    "    val_mask = torch.LongTensor(val_mask)\n",
    "    test_mask = torch.LongTensor(test_mask)\n",
    "    \n",
    "    # Eigenvector\n",
    "    labeled_ind = site_percentage(train, params['num_training'], subject_IDs)\n",
    "    x_data = feature_selection(features, y, labeled_ind, params['num_features'])\n",
    "    features = preprocess_features(sp.coo_matrix(x_data).tolil())\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    \n",
    "    # Adjacency matrix\n",
    "    graph = create_affinity_graph_from_scores(['SEX', 'SITE_ID'], subject_IDs)\n",
    "    distv = distance.pdist(x_data, metric='correlation')\n",
    "    dist = distance.squareform(distv)\n",
    "    sigma = np.mean(dist)\n",
    "    sparse_graph = np.exp(- dist ** 2 / (2 * sigma ** 2))\n",
    "    final_graph = graph * sparse_graph\n",
    "\n",
    "    return final_graph, features, y, y_data, y_train, y_val, y_test, train, val, test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        coords = torch.from_numpy(coords)\n",
    "        values = torch.from_numpy(values)\n",
    "        shape = torch.tensor(shape)\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return adj_normalized\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(sp.eye(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return t_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7980a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# from utils import preprocess_features\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "class dataloader():\n",
    "    def __init__(self): \n",
    "        self.pd_dict = {}\n",
    "        self.node_ftr_dim = 20000\n",
    "        self.num_classes = 2 \n",
    "\n",
    "    def load_data(self, params, connectivity='correlation', atlas='cc400'):\n",
    "        ''' load multimodal data from ABIDE\n",
    "        return: imaging features (raw), labels, non-image data\n",
    "        '''\n",
    "        subject_IDs = get_ids()\n",
    "        labels = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "        num_nodes = len(subject_IDs)\n",
    "\n",
    "        sites = get_subject_score(subject_IDs, score='SITE_ID')\n",
    "        unique = np.unique(list(sites.values())).tolist()\n",
    "        ages = get_subject_score(subject_IDs, score='AGE_AT_SCAN')\n",
    "        genders = get_subject_score(subject_IDs, score='SEX') \n",
    "\n",
    "        y_onehot = np.zeros([num_nodes, self.num_classes])\n",
    "        y = np.zeros([num_nodes])\n",
    "        site = np.zeros([num_nodes], dtype=int)\n",
    "        age = np.zeros([num_nodes], dtype=np.float32)\n",
    "        gender = np.zeros([num_nodes], dtype=int)\n",
    "        for i in range(num_nodes):\n",
    "            y_onehot[i, int(labels[subject_IDs[i]])-1] = 1\n",
    "            y[i] = int(labels[subject_IDs[i]])\n",
    "            site[i] = unique.index(sites[subject_IDs[i]])\n",
    "            age[i] = float(ages[subject_IDs[i]])\n",
    "            gender[i] = genders[subject_IDs[i]]\n",
    "        \n",
    "        self.y = y -1  \n",
    "\n",
    "        self.raw_features = get_networks(subject_IDs, kind=connectivity, atlas_name=atlas)\n",
    "\n",
    "        phonetic_data = np.zeros([num_nodes, 3], dtype=np.float32)\n",
    "        phonetic_data[:,0] = site \n",
    "        phonetic_data[:,1] = gender \n",
    "        phonetic_data[:,2] = age \n",
    "\n",
    "        self.pd_dict['SITE_ID'] = np.copy(phonetic_data[:,0])\n",
    "        self.pd_dict['SEX'] = np.copy(phonetic_data[:,1])\n",
    "        self.pd_dict['AGE_AT_SCAN'] = np.copy(phonetic_data[:,2]) \n",
    "        \n",
    "        return self.raw_features, self.y, phonetic_data\n",
    "\n",
    "    def data_split(self, n_folds):\n",
    "        # split data by k-fold CV\n",
    "        skf = StratifiedKFold(n_splits=n_folds)\n",
    "        cv_splits = list(skf.split(self.raw_features, self.y))\n",
    "        return cv_splits \n",
    "\n",
    "    def get_node_features(self, train_ind):\n",
    "        '''preprocess node features for wl-deepgcn\n",
    "        '''\n",
    "        node_ftr = feature_selection(self.raw_features, self.y, train_ind, self.node_ftr_dim)\n",
    "        self.node_ftr = preprocess_features(node_ftr) \n",
    "        return self.node_ftr\n",
    "\n",
    "    def get_WL_inputs(self, nonimg):\n",
    "        '''get WL inputs for wl-deepgcn \n",
    "        '''\n",
    "        # construct edge network inputs \n",
    "        n = self.node_ftr.shape[0] \n",
    "        num_edge = n*(1+n)//2 - n  # n*(n-1)//2,HO=6105\n",
    "        pd_ftr_dim = nonimg.shape[1]\n",
    "        edge_index = np.zeros([2, num_edge], dtype=np.int64) \n",
    "        edgenet_input = np.zeros([num_edge, 2*pd_ftr_dim], dtype=np.float32)  \n",
    "        aff_score = np.zeros(num_edge, dtype=np.float32)\n",
    "        # static affinity score used to pre-prune edges \n",
    "        aff_adj = get_static_affinity_adj(self.node_ftr, self.pd_dict)  \n",
    "        flatten_ind = 0 \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                edge_index[:,flatten_ind] = [i,j]\n",
    "                edgenet_input[flatten_ind]  = np.concatenate((nonimg[i], nonimg[j]))\n",
    "                aff_score[flatten_ind] = aff_adj[i][j]  \n",
    "                flatten_ind +=1\n",
    "\n",
    "        assert flatten_ind == num_edge, \"Error in computing edge input\"\n",
    "        \n",
    "        keep_ind = np.where(aff_score > 1.1)[0]  \n",
    "        edge_index = edge_index[:, keep_ind]\n",
    "        edgenet_input = edgenet_input[keep_ind]\n",
    "\n",
    "        return edge_index, edgenet_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc69f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn import Linear as Lin, Sequential as Seq\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class WL(torch.nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.3):\n",
    "        super(WL, self).__init__()\n",
    "        h1=256\n",
    "        h2=128\n",
    "        self.parser =nn.Sequential(\n",
    "                nn.Linear(input_dim, h1, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.BatchNorm1d(h1),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(h1, h2, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.BatchNorm1d(h2),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(h2, h2, bias=True),\n",
    "                )\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-8)\n",
    "        self.input_dim = input_dim\n",
    "        self.model_init()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.elu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:,0:self.input_dim]\n",
    "        x2 = x[:,self.input_dim:]\n",
    "        h1 = self.parser(x1) \n",
    "        h2 = self.parser(x2) \n",
    "        p = (self.cos(h1,h2) + 1)*0.5\n",
    "        return p\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Lin):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db8243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear as Lin, Sequential as Seq\n",
    "import torch_geometric as tg\n",
    "# from wl import WL\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, nhid):\n",
    "        super(MLP,self).__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "            torch.nn.Linear(input_dim,nhid))\n",
    "        \n",
    "    def forward(self, features):\n",
    "        output = self.cls(features)\n",
    "        return output\n",
    "            \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, nhid, num_classes, ngl, dropout, edge_dropout, edgenet_input_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        K=3   \n",
    "        hidden = [nhid for i in range(ngl)] \n",
    "        self.dropout = dropout\n",
    "        self.edge_dropout = edge_dropout \n",
    "        bias = False \n",
    "        self.relu = torch.nn.ReLU(inplace=True) \n",
    "        self.ngl = ngl \n",
    "        self.gconv = nn.ModuleList()\n",
    "        for i in range(ngl):\n",
    "            in_channels = input_dim if i==0  else hidden[i-1]\n",
    "            self.gconv.append(tg.nn.ChebConv(in_channels, hidden[i], K, normalization='sym', bias=bias)) \n",
    "          \n",
    "        self.cls = nn.Sequential(\n",
    "                torch.nn.Linear(16, 128),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(128), \n",
    "                torch.nn.Linear(128, num_classes))\n",
    "\n",
    "        self.edge_net = WL(input_dim=edgenet_input_dim//2, dropout=dropout)\n",
    "        self.model_init()\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Lin):\n",
    "                torch.nn.init.kaiming_normal_(m.weight) # He init\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n",
    "    def forward(self, features, edge_index, edgenet_input, enforce_edropout=False): \n",
    "        if self.edge_dropout>0:\n",
    "            if enforce_edropout or self.training:\n",
    "                one_mask = torch.ones([edgenet_input.shape[0],1])\n",
    "                self.drop_mask = F.dropout(one_mask, self.edge_dropout, True)\n",
    "                self.bool_mask = torch.squeeze(self.drop_mask.type(torch.bool))\n",
    "                edge_index = edge_index[:, self.bool_mask] \n",
    "                edgenet_input = edgenet_input[self.bool_mask] # Weights\n",
    "            \n",
    "        edge_weight = torch.squeeze(self.edge_net(edgenet_input))\n",
    "        \n",
    "\n",
    "        # GCN residual connection\n",
    "        # input layer\n",
    "        features = F.dropout(features, self.dropout, self.training)\n",
    "        x = self.relu(self.gconv[0](features, edge_index, edge_weight)) \n",
    "        x_temp = x\n",
    "        \n",
    "        # hidden layers\n",
    "        for i in range(1, self.ngl - 1): # self.ngl→7\n",
    "            x = F.dropout(x_temp, self.dropout, self.training)\n",
    "            x = self.relu(self.gconv[i](x, edge_index, edge_weight)) \n",
    "            x_temp = x_temp + x # ([871,64])\n",
    "\n",
    "        # output layer\n",
    "        x = F.dropout(x_temp, self.dropout, self.training)\n",
    "        x = self.relu(self.gconv[self.ngl - 1](x, edge_index, edge_weight))\n",
    "        x_temp = x_temp + x\n",
    "\n",
    "        output = x # Final output is not cumulative\n",
    "        output = self.cls(output) \n",
    "        \n",
    "        return output, edge_weight\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5856d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassSpecificity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def torchmetrics_accuracy(preds, labels):\n",
    "    acc = torchmetrics.functional.accuracy(preds, labels,task=\"multiclass\", num_classes=2)\n",
    "    return acc\n",
    "\n",
    "def torchmetrics_spef(preds, labels):\n",
    "    metric = MulticlassSpecificity(num_classes=2)\n",
    "    spef = metric(preds, labels)\n",
    "    return spef\n",
    "\n",
    "def torchmetrics_auc(preds, labels):\n",
    "    auc = torchmetrics.functional.auroc(preds, labels, task=\"multiclass\", num_classes=2)\n",
    "    return auc\n",
    "\n",
    "def confusion_matrix(preds, labels):\n",
    "    conf_matrix = torch.zeros(2, 2)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    for p, t in zip(preds, labels):\n",
    "        conf_matrix[t, p] += 1 \n",
    "    return conf_matrix\n",
    "def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Input\n",
    "    - cm : computer the value of confusion matrix\n",
    "    - normalize : True: %, False: 123\n",
    "    \"\"\"\n",
    "    classes = ['0:ASD','1:TC']\n",
    "    if normalize:\n",
    "        cm = cm.numpy()\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else '.0f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def correct_num(preds, labels):\n",
    "    \"\"\"Accuracy, auc with masking.Acc of the masked samples\"\"\"\n",
    "    correct_prediction = np.equal(np.argmax(preds, 1), labels).astype(np.float32)\n",
    "    return np.sum(correct_prediction)\n",
    "\n",
    "def prf(preds, labels, is_logit=True):\n",
    "    ''' input: logits, labels  ''' \n",
    "    pred_lab= np.argmax(preds, 1)\n",
    "    p,r,f,s  = precision_recall_fscore_support(labels, pred_lab, average='binary')\n",
    "    return [p,r,f]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae068d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "no_cuda: False\n",
      "seed: 46\n",
      "epochs: 200\n",
      "lr: 0.001\n",
      "weight_decay: 5e-05\n",
      "hidden: 16\n",
      "dropout: 0.2\n",
      "atlas: cc400\n",
      "num_features: 20000\n",
      "folds: 10\n",
      "connectivity: correlation\n",
      "max_degree: 3\n",
      "ngl: 8\n",
      "edropout: 0.3\n",
      "train: 1\n",
      "ckpt_path: ../folds/rois_cc400_pth_feature\n",
      "early_stopping: True\n",
      "early_stopping_patience: 20\n",
      "cuda: False\n",
      "  Loading dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_21384\\2141194207.py:217: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the 1-fold Training, Validation, and Test Sets:900,100,112\n",
      " Starting the 1-1 Fold:：\n",
      "Fitting estimator with 76636 features.\n",
      "Fitting estimator with 76536 features.\n",
      "Fitting estimator with 76436 features.\n",
      "Fitting estimator with 76336 features.\n",
      "Fitting estimator with 76236 features.\n",
      "Fitting estimator with 76136 features.\n",
      "Fitting estimator with 76036 features.\n",
      "Fitting estimator with 75936 features.\n",
      "Fitting estimator with 75836 features.\n",
      "Fitting estimator with 75736 features.\n",
      "Fitting estimator with 75636 features.\n",
      "Fitting estimator with 75536 features.\n",
      "Fitting estimator with 75436 features.\n",
      "Fitting estimator with 75336 features.\n",
      "Fitting estimator with 75236 features.\n",
      "Fitting estimator with 75136 features.\n",
      "Fitting estimator with 75036 features.\n",
      "Fitting estimator with 74936 features.\n",
      "Fitting estimator with 74836 features.\n",
      "Fitting estimator with 74736 features.\n",
      "Fitting estimator with 74636 features.\n",
      "Fitting estimator with 74536 features.\n",
      "Fitting estimator with 74436 features.\n",
      "Fitting estimator with 74336 features.\n",
      "Fitting estimator with 74236 features.\n",
      "Fitting estimator with 74136 features.\n",
      "Fitting estimator with 74036 features.\n",
      "Fitting estimator with 73936 features.\n",
      "Fitting estimator with 73836 features.\n",
      "Fitting estimator with 73736 features.\n",
      "Fitting estimator with 73636 features.\n",
      "Fitting estimator with 73536 features.\n",
      "Fitting estimator with 73436 features.\n",
      "Fitting estimator with 73336 features.\n",
      "Fitting estimator with 73236 features.\n",
      "Fitting estimator with 73136 features.\n",
      "Fitting estimator with 73036 features.\n",
      "Fitting estimator with 72936 features.\n",
      "Fitting estimator with 72836 features.\n",
      "Fitting estimator with 72736 features.\n",
      "Fitting estimator with 72636 features.\n",
      "Fitting estimator with 72536 features.\n",
      "Fitting estimator with 72436 features.\n",
      "Fitting estimator with 72336 features.\n",
      "Fitting estimator with 72236 features.\n",
      "Fitting estimator with 72136 features.\n",
      "Fitting estimator with 72036 features.\n",
      "Fitting estimator with 71936 features.\n",
      "Fitting estimator with 71836 features.\n",
      "Fitting estimator with 71736 features.\n",
      "Fitting estimator with 71636 features.\n",
      "Fitting estimator with 71536 features.\n",
      "Fitting estimator with 71436 features.\n",
      "Fitting estimator with 71336 features.\n",
      "Fitting estimator with 71236 features.\n",
      "Fitting estimator with 71136 features.\n",
      "Fitting estimator with 71036 features.\n",
      "Fitting estimator with 70936 features.\n",
      "Fitting estimator with 70836 features.\n",
      "Fitting estimator with 70736 features.\n",
      "Fitting estimator with 70636 features.\n",
      "Fitting estimator with 70536 features.\n",
      "Fitting estimator with 70436 features.\n",
      "Fitting estimator with 70336 features.\n",
      "Fitting estimator with 70236 features.\n",
      "Fitting estimator with 70136 features.\n",
      "Fitting estimator with 70036 features.\n",
      "Fitting estimator with 69936 features.\n",
      "Fitting estimator with 69836 features.\n",
      "Fitting estimator with 69736 features.\n",
      "Fitting estimator with 69636 features.\n",
      "Fitting estimator with 69536 features.\n",
      "Fitting estimator with 69436 features.\n",
      "Fitting estimator with 69336 features.\n",
      "Fitting estimator with 69236 features.\n",
      "Fitting estimator with 69136 features.\n",
      "Fitting estimator with 69036 features.\n",
      "Fitting estimator with 68936 features.\n",
      "Fitting estimator with 68836 features.\n",
      "Fitting estimator with 68736 features.\n",
      "Fitting estimator with 68636 features.\n",
      "Fitting estimator with 68536 features.\n",
      "Fitting estimator with 68436 features.\n",
      "Fitting estimator with 68336 features.\n",
      "Fitting estimator with 68236 features.\n",
      "Fitting estimator with 68136 features.\n",
      "Fitting estimator with 68036 features.\n",
      "Fitting estimator with 67936 features.\n",
      "Fitting estimator with 67836 features.\n",
      "Fitting estimator with 67736 features.\n",
      "Fitting estimator with 67636 features.\n",
      "Fitting estimator with 67536 features.\n",
      "Fitting estimator with 67436 features.\n",
      "Fitting estimator with 67336 features.\n",
      "Fitting estimator with 67236 features.\n",
      "Fitting estimator with 67136 features.\n",
      "Fitting estimator with 67036 features.\n",
      "Fitting estimator with 66936 features.\n",
      "Fitting estimator with 66836 features.\n",
      "Fitting estimator with 66736 features.\n",
      "Fitting estimator with 66636 features.\n",
      "Fitting estimator with 66536 features.\n",
      "Fitting estimator with 66436 features.\n",
      "Fitting estimator with 66336 features.\n",
      "Fitting estimator with 66236 features.\n",
      "Fitting estimator with 66136 features.\n",
      "Fitting estimator with 66036 features.\n",
      "Fitting estimator with 65936 features.\n",
      "Fitting estimator with 65836 features.\n",
      "Fitting estimator with 65736 features.\n",
      "Fitting estimator with 65636 features.\n",
      "Fitting estimator with 65536 features.\n",
      "Fitting estimator with 65436 features.\n",
      "Fitting estimator with 65336 features.\n",
      "Fitting estimator with 65236 features.\n",
      "Fitting estimator with 65136 features.\n",
      "Fitting estimator with 65036 features.\n",
      "Fitting estimator with 64936 features.\n",
      "Fitting estimator with 64836 features.\n",
      "Fitting estimator with 64736 features.\n",
      "Fitting estimator with 64636 features.\n",
      "Fitting estimator with 64536 features.\n",
      "Fitting estimator with 64436 features.\n",
      "Fitting estimator with 64336 features.\n",
      "Fitting estimator with 64236 features.\n",
      "Fitting estimator with 64136 features.\n",
      "Fitting estimator with 64036 features.\n",
      "Fitting estimator with 63936 features.\n",
      "Fitting estimator with 63836 features.\n",
      "Fitting estimator with 63736 features.\n",
      "Fitting estimator with 63636 features.\n",
      "Fitting estimator with 63536 features.\n",
      "Fitting estimator with 63436 features.\n",
      "Fitting estimator with 63336 features.\n",
      "Fitting estimator with 63236 features.\n",
      "Fitting estimator with 63136 features.\n",
      "Fitting estimator with 63036 features.\n",
      "Fitting estimator with 62936 features.\n",
      "Fitting estimator with 62836 features.\n",
      "Fitting estimator with 62736 features.\n",
      "Fitting estimator with 62636 features.\n",
      "Fitting estimator with 62536 features.\n",
      "Fitting estimator with 62436 features.\n",
      "Fitting estimator with 62336 features.\n",
      "Fitting estimator with 62236 features.\n",
      "Fitting estimator with 62136 features.\n",
      "Fitting estimator with 62036 features.\n",
      "Fitting estimator with 61936 features.\n",
      "Fitting estimator with 61836 features.\n",
      "Fitting estimator with 61736 features.\n",
      "Fitting estimator with 61636 features.\n",
      "Fitting estimator with 61536 features.\n",
      "Fitting estimator with 61436 features.\n",
      "Fitting estimator with 61336 features.\n",
      "Fitting estimator with 61236 features.\n",
      "Fitting estimator with 61136 features.\n",
      "Fitting estimator with 61036 features.\n",
      "Fitting estimator with 60936 features.\n",
      "Fitting estimator with 60836 features.\n",
      "Fitting estimator with 60736 features.\n",
      "Fitting estimator with 60636 features.\n",
      "Fitting estimator with 60536 features.\n",
      "Fitting estimator with 60436 features.\n",
      "Fitting estimator with 60336 features.\n",
      "Fitting estimator with 60236 features.\n",
      "Fitting estimator with 60136 features.\n",
      "Fitting estimator with 60036 features.\n",
      "Fitting estimator with 59936 features.\n",
      "Fitting estimator with 59836 features.\n",
      "Fitting estimator with 59736 features.\n",
      "Fitting estimator with 59636 features.\n",
      "Fitting estimator with 59536 features.\n",
      "Fitting estimator with 59436 features.\n",
      "Fitting estimator with 59336 features.\n",
      "Fitting estimator with 59236 features.\n",
      "Fitting estimator with 59136 features.\n",
      "Fitting estimator with 59036 features.\n",
      "Fitting estimator with 58936 features.\n",
      "Fitting estimator with 58836 features.\n",
      "Fitting estimator with 58736 features.\n",
      "Fitting estimator with 58636 features.\n",
      "Fitting estimator with 58536 features.\n",
      "Fitting estimator with 58436 features.\n",
      "Fitting estimator with 58336 features.\n",
      "Fitting estimator with 58236 features.\n",
      "Fitting estimator with 58136 features.\n",
      "Fitting estimator with 58036 features.\n",
      "Fitting estimator with 57936 features.\n",
      "Fitting estimator with 57836 features.\n",
      "Fitting estimator with 57736 features.\n",
      "Fitting estimator with 57636 features.\n",
      "Fitting estimator with 57536 features.\n",
      "Fitting estimator with 57436 features.\n",
      "Fitting estimator with 57336 features.\n",
      "Fitting estimator with 57236 features.\n",
      "Fitting estimator with 57136 features.\n",
      "Fitting estimator with 57036 features.\n",
      "Fitting estimator with 56936 features.\n",
      "Fitting estimator with 56836 features.\n",
      "Fitting estimator with 56736 features.\n",
      "Fitting estimator with 56636 features.\n",
      "Fitting estimator with 56536 features.\n",
      "Fitting estimator with 56436 features.\n",
      "Fitting estimator with 56336 features.\n",
      "Fitting estimator with 56236 features.\n",
      "Fitting estimator with 56136 features.\n",
      "Fitting estimator with 56036 features.\n",
      "Fitting estimator with 55936 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 55836 features.\n",
      "Fitting estimator with 55736 features.\n",
      "Fitting estimator with 55636 features.\n",
      "Fitting estimator with 55536 features.\n",
      "Fitting estimator with 55436 features.\n",
      "Fitting estimator with 55336 features.\n",
      "Fitting estimator with 55236 features.\n",
      "Fitting estimator with 55136 features.\n",
      "Fitting estimator with 55036 features.\n",
      "Fitting estimator with 54936 features.\n",
      "Fitting estimator with 54836 features.\n",
      "Fitting estimator with 54736 features.\n",
      "Fitting estimator with 54636 features.\n",
      "Fitting estimator with 54536 features.\n",
      "Fitting estimator with 54436 features.\n",
      "Fitting estimator with 54336 features.\n",
      "Fitting estimator with 54236 features.\n",
      "Fitting estimator with 54136 features.\n",
      "Fitting estimator with 54036 features.\n",
      "Fitting estimator with 53936 features.\n",
      "Fitting estimator with 53836 features.\n",
      "Fitting estimator with 53736 features.\n",
      "Fitting estimator with 53636 features.\n",
      "Fitting estimator with 53536 features.\n",
      "Fitting estimator with 53436 features.\n",
      "Fitting estimator with 53336 features.\n",
      "Fitting estimator with 53236 features.\n",
      "Fitting estimator with 53136 features.\n",
      "Fitting estimator with 53036 features.\n",
      "Fitting estimator with 52936 features.\n",
      "Fitting estimator with 52836 features.\n",
      "Fitting estimator with 52736 features.\n",
      "Fitting estimator with 52636 features.\n",
      "Fitting estimator with 52536 features.\n",
      "Fitting estimator with 52436 features.\n",
      "Fitting estimator with 52336 features.\n",
      "Fitting estimator with 52236 features.\n",
      "Fitting estimator with 52136 features.\n",
      "Fitting estimator with 52036 features.\n",
      "Fitting estimator with 51936 features.\n",
      "Fitting estimator with 51836 features.\n",
      "Fitting estimator with 51736 features.\n",
      "Fitting estimator with 51636 features.\n",
      "Fitting estimator with 51536 features.\n",
      "Fitting estimator with 51436 features.\n",
      "Fitting estimator with 51336 features.\n",
      "Fitting estimator with 51236 features.\n",
      "Fitting estimator with 51136 features.\n",
      "Fitting estimator with 51036 features.\n",
      "Fitting estimator with 50936 features.\n",
      "Fitting estimator with 50836 features.\n",
      "Fitting estimator with 50736 features.\n",
      "Fitting estimator with 50636 features.\n",
      "Fitting estimator with 50536 features.\n",
      "Fitting estimator with 50436 features.\n",
      "Fitting estimator with 50336 features.\n",
      "Fitting estimator with 50236 features.\n",
      "Fitting estimator with 50136 features.\n",
      "Fitting estimator with 50036 features.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import io\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# from dataloader import dataloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "if hasattr(sys.stdout, 'buffer'):\n",
    "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.no_cuda = False\n",
    "        self.seed = 46\n",
    "        self.epochs = 200\n",
    "        self.lr = 0.001\n",
    "        self.weight_decay = 5e-5\n",
    "        self.hidden = 16\n",
    "        self.dropout = 0.2\n",
    "        self.atlas = 'cc400'\n",
    "        self.num_features = 20000\n",
    "        self.folds = 10\n",
    "        self.connectivity = 'correlation'\n",
    "        self.max_degree = 3\n",
    "        self.ngl = 8\n",
    "        self.edropout = 0.3\n",
    "        self.train = 1\n",
    "        self.ckpt_path = '../folds/rois_cc400_pth_feature'\n",
    "        self.early_stopping = True\n",
    "        self.early_stopping_patience = 20\n",
    "\n",
    "# Instantiate Args class\n",
    "args = Args()\n",
    "\n",
    "# Check if CUDA is available\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Create params dictionary\n",
    "params = vars(args)\n",
    "\n",
    "# Print Hyperparameters\n",
    "print('Hyperparameters:')\n",
    "for key, value in params.items():\n",
    "    print(key + \":\", value)\n",
    "\n",
    "corrects = np.zeros(args.folds, dtype=np.int32) \n",
    "accs = np.zeros(args.folds, dtype=np.float32) \n",
    "aucs = np.zeros(args.folds, dtype=np.float32)\n",
    "prfs = np.zeros([args.folds,3], dtype=np.float32) # Save Precision, Recall, F1\n",
    "test_num = np.zeros(args.folds, dtype=np.float32)\n",
    "\n",
    "\n",
    "print('  Loading dataset ...')\n",
    "dataloader = dataloader()\n",
    "raw_features, y, nonimg = dataloader.load_data(params) \n",
    "cv_splits = dataloader.data_split(args.folds)\n",
    "features=raw_features\n",
    "\n",
    "t1 = time.time()\n",
    "count=1;\n",
    "for i in range(args.folds):\n",
    "    \n",
    "    \n",
    "    \n",
    "    t_start = time.time()\n",
    "    train_ind, test_ind = cv_splits[i]\n",
    "\n",
    "    train_ind, valid_ind = train_test_split(train_ind, test_size=0.1, random_state = 24)\n",
    "    \n",
    "    cv_splits[i] = (train_ind, valid_ind)\n",
    "    cv_splits[i] = cv_splits[i] + (test_ind,)\n",
    "    print('Size of the {}-fold Training, Validation, and Test Sets:{},{},{}' .format(i+1, len(cv_splits[i][0]), len(cv_splits[i][1]), len(cv_splits[i][2])))\n",
    "\n",
    "    if args.train == 1:\n",
    "        for j in range(args.folds):\n",
    "            print(' Starting the {}-{} Fold:：'.format(i+1,j+1))\n",
    "            node_ftr = dataloader.get_node_features(train_ind)\n",
    "            edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "            edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "            \n",
    "            model = GCN(input_dim = args.num_features,\n",
    "                        nhid = args.hidden, \n",
    "                        num_classes = 2, \n",
    "                        ngl = args.ngl, \n",
    "                        dropout = args.dropout, \n",
    "                        edge_dropout = args.edropout, \n",
    "                        edgenet_input_dim = 2*nonimg.shape[1])\n",
    "            optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "            \n",
    "#             if args.cuda:\n",
    "            model\n",
    "            features = torch.tensor(node_ftr, dtype=torch.float32)\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "            edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "            labels = torch.tensor(y, dtype=torch.long)\n",
    "            fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "                \n",
    "            acc = 0\n",
    "            best_val_loss = float('inf') # early stoppping: Initialized to positive infinity\n",
    "            current_patience = 0 # early stopping: Used to record the epochs of the current early stopping\n",
    "            \n",
    "            epoch_store = []\n",
    "            acc_train_store =[]        \n",
    "            pre_train_store =[]\n",
    "            recall_train_store =[]\n",
    "            F1_train_store =[]\n",
    "            AUC_train_store =[]\n",
    "            acc_val_store=[]\n",
    "            pre_val_store=[]\n",
    "            recall_val_store=[]\n",
    "            F1_val_store=[]\n",
    "            AUC_val_store=[]\n",
    "            \n",
    "            for epoch in range(args.epochs):\n",
    "                # train\n",
    "                model.train()\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    optimizer.zero_grad()\n",
    "                    output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "                    loss_train = torch.nn.CrossEntropyLoss()(output[train_ind], labels[train_ind])\n",
    "                    loss_train.backward()\n",
    "                    optimizer.step()\n",
    "                acc_train = torchmetrics_accuracy(output[train_ind], labels[train_ind])\n",
    "                auc_train = torchmetrics_auc(output[train_ind], labels[train_ind])\n",
    "                logits_train = output[train_ind].detach().cpu().numpy()\n",
    "                prf_train = prf(logits_train, y[train_ind])\n",
    "\n",
    "                \n",
    "                # valid\n",
    "                model.eval()\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "                loss_val = torch.nn.CrossEntropyLoss()(output[valid_ind], labels[valid_ind])\n",
    "                acc_val = torchmetrics_accuracy(output[valid_ind], labels[valid_ind])\n",
    "                auc_val = torchmetrics_auc(output[valid_ind], labels[valid_ind])\n",
    "                logits_val = output[valid_ind].detach().cpu().numpy()\n",
    "                prf_val = prf(logits_val, y[valid_ind])\n",
    "\n",
    "                \n",
    "                print('Epoch:{:04d}'.format(epoch+1))\n",
    "                print('acc_train:{:.4f}'.format(acc_train),\n",
    "                      'pre_train:{:.4f}'.format(prf_train[0]),\n",
    "                      'recall_train:{:.4f}'.format(prf_train[1]),\n",
    "                      'F1_train:{:.4f}'.format(prf_train[2]),\n",
    "                      'AUC_train:{:.4f}'.format(auc_train))\n",
    "                print('acc_val:{:.4f}'.format(acc_val),\n",
    "                      'pre_val:{:.4f}'.format(prf_val[0]),\n",
    "                      'recall_val:{:.4f}'.format(prf_val[1]),\n",
    "                      'F1_val:{:4f}'.format(prf_val[2]),\n",
    "                      'AUC_val:{:.4f}'.format(auc_val))\n",
    "                \n",
    "                epoch_store.append(epoch+1)\n",
    "                acc_train_store.append(acc_train)       \n",
    "                pre_train_store.append(prf_train[0])\n",
    "                recall_train_store.append(prf_train[1])\n",
    "                F1_train_store.append(prf_train[2])\n",
    "                AUC_train_store.append(auc_train)\n",
    "                acc_val_store.append(acc_val)\n",
    "                pre_val_store.append(prf_val[0])\n",
    "                recall_val_store.append(prf_val[1])\n",
    "                F1_val_store.append(prf_val[2])\n",
    "                AUC_val_store.append(auc_val)\n",
    "                \n",
    "                # save pth\n",
    "                if acc_val > acc and epoch > 50:\n",
    "                    acc = acc_val\n",
    "                    if args.ckpt_path != '':\n",
    "                        if not os.path.exists(args.ckpt_path):\n",
    "                            os.makedirs(args.ckpt_path)\n",
    "                        torch.save(model.state_dict(), fold_model_path)\n",
    "                \n",
    "                # Early Stopping\n",
    "                if epoch > 50 and args.early_stopping == True:\n",
    "                    if loss_val < best_val_loss:\n",
    "                        best_val_loss = loss_val\n",
    "                        current_patience = 0\n",
    "                    else:\n",
    "                        current_patience += 1\n",
    "                    if current_patience >= args.early_stopping_patience:\n",
    "                        print('Early Stopping!!! epoch：{}'.format(epoch))\n",
    "                        break\n",
    "        print(\"===================================================================\",i,\"_\",j)\n",
    "        data  = { \n",
    "              \"epoch\" : epoch_store ,\n",
    "              \"acc_train\" : acc_train_store ,        \n",
    "              \"pre_train\" : pre_train_store ,\n",
    "              \"recall_train\" : recall_train_store ,\n",
    "              \"F1_train\" : F1_train_store ,\n",
    "              \"AUC_train\" : AUC_train_store ,\n",
    "              \"acc_val\" : acc_val_store,\n",
    "               \"pre_val\" : pre_val_store ,\n",
    "              \"recall_val\" : recall_val_store ,\n",
    "              \"F1_val\" : F1_val_store ,\n",
    "              \"AUC_val\" : AUC_val_store  \n",
    "        }\n",
    "        \n",
    "        \n",
    "        epoch_file_path =  f'../files/rois_cc400_feature/file_{i}_{j}_{count}.csv'\n",
    "        data_file = pd.DataFrame(data);\n",
    "        data_file.to_csv(epoch_file_path , index=False);\n",
    "        count=count+1;\n",
    "        # test\n",
    "        print(\"Loading the Model for the {}-th Fold:... ...\".format(i+1),\n",
    "              \"Size of samples in the test set:{}\".format(len(test_ind)))\n",
    "        model.load_state_dict(torch.load(fold_model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "        acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "        auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "        logits_test = output[test_ind].detach().cpu().numpy()\n",
    "        correct_test = correct_num(logits_test, y[test_ind])\n",
    "        prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "        t_end = time.time()\n",
    "        t = t_end - t_start\n",
    "        print('Fold {} Results:'.format(i+1),\n",
    "              'test acc:{:.4f}'.format(acc_test),\n",
    "              'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "              'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "              'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "              'test_AUC:{:.4f}'.format(auc_test),\n",
    "              'time:{:.3f}s'.format(t))\n",
    "        \n",
    "        correct = correct_test\n",
    "        aucs[i] = auc_test\n",
    "        prfs[i] = prf_test\n",
    "        corrects[i] = correct\n",
    "        test_num[i] = len(test_ind)\n",
    "    \n",
    "    \n",
    "    if args.train == 0:\n",
    "        node_ftr = dataloader.get_node_features(train_ind)\n",
    "        edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "        edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "        \n",
    "        model = GCN(input_dim = args.num_features,\n",
    "                    nhid = args.hidden, \n",
    "                    num_classes = 2, \n",
    "                    ngl = args.ngl, \n",
    "                    dropout = args.dropout, \n",
    "                    edge_dropout = args.edropout, \n",
    "                    edgenet_input_dim = 2*nonimg.shape[1])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "#         if args.cuda\n",
    "        model\n",
    "        features = torch.tensor(node_ftr, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "        labels = torch.tensor(y, dtype=torch.long)\n",
    "        fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "        \n",
    "        model.load_state_dict(torch.load(fold_model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "        acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "        auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "        logits_test = output[test_ind].detach().cpu().numpy()\n",
    "        correct_test = correct_num(logits_test, y[test_ind])\n",
    "        prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "        t_end = time.time()\n",
    "        t = t_end - t_start\n",
    "        print('Fold {} Results:'.format(i+1),\n",
    "              'test acc:{:.4f}'.format(acc_test),\n",
    "              'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "              'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "              'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "              'test_AUC:{:.4f}'.format(auc_test),\n",
    "              'time:{:.3f}s'.format(t))\n",
    "        \n",
    "        correct = correct_test\n",
    "        aucs[i] = auc_test\n",
    "        prfs[i] = prf_test\n",
    "        corrects[i] = correct\n",
    "        test_num[i] = len(test_ind)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print('\\r\\n======Finish Results for Nested 10-fold cross-validation======')\n",
    "Nested10kCV_acc = np.sum(corrects) / np.sum(test_num)\n",
    "Nested10kCV_auc = np.mean(aucs)\n",
    "Nested10kCV_precision, Nested10kCV_recall, Nested10kCV_F1 = np.mean(prfs, axis=0)\n",
    "print('Test:',\n",
    "      'acc:{}'.format(Nested10kCV_acc),\n",
    "      'precision:{}'.format(Nested10kCV_precision),\n",
    "      'recall:{}'.format(Nested10kCV_recall),\n",
    "      'F1:{}'.format(Nested10kCV_F1),\n",
    "      'AUC:{}'.format(Nested10kCV_auc))\n",
    "print('Total duration:{}'.format(t2 - t1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00efe6af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import io\n",
    "# import sys\n",
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# from models import GCN\n",
    "\n",
    "# from metrics import torchmetrics_accuracy, torchmetrics_auc, correct_num, prf\n",
    "\n",
    "# # from dataloader import dataloader\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import auc\n",
    "\n",
    "# if hasattr(sys.stdout, 'buffer'):\n",
    "#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "# # Training settings\n",
    "# # parser = argparse.ArgumentParser()\n",
    "# # parser.add_argument('--no_cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "# # parser.add_argument('--seed', type=int, default=46, help='Random seed.')\n",
    "# # parser.add_argument('--epochs', type=int, default=200, help='Number of epochs to train.')\n",
    "# # parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')\n",
    "# # parser.add_argument('--weight_decay', type=float, default=5e-5, help='Weight decay (L2 loss on parameters).')\n",
    "# # parser.add_argument('--hidden', type=int, default=16, help='Number of hidden units.')\n",
    "# # parser.add_argument('--dropout', type=float, default=0.2, help='Dropout rate (1 - keep probability).')\n",
    "# # parser.add_argument('--atlas', default='cc200', help='atlas for network construction (node definition) default: ho, see preprocessed-connectomes-project.org/abide/Pipelines.html for more options ')\n",
    "# # parser.add_argument('--num_features', default=2000, type=int, help='Number of features to keep for the feature selection step (default: 2000)')\n",
    "# # parser.add_argument('--folds', default=10, type=int, help='For cross validation, specifies which fold will be used. All folds are used if set to 11 (default: 11)')\n",
    "# # parser.add_argument('--connectivity', default='correlation', help='Type of connectivity used for network construction (default: correlation, options: correlation, partial correlation, tangent)')\n",
    "# # parser.add_argument('--max_degree', type=int, default=3, help='Maximum Chebyshev polynomial degree.')\n",
    "# # parser.add_argument('--ngl', default=8, type=int, help='number of gcn hidden layders (default: 8)')\n",
    "# # parser.add_argument('--edropout', type=float, default=0.3, help='edge dropout rate')\n",
    "# # parser.add_argument('--train', default=1, type=int, help='train(default: 1) or evaluate(0)')\n",
    "# # parser.add_argument('--ckpt_path', type=str, default='./pth', help='checkpoint path to save trained models')\n",
    "# # parser.add_argument('--early_stopping', action='store_true', default=True, help='early stopping switch')\n",
    "# # parser.add_argument('--early_stopping_patience', type=int, default=20, help='early stoppng epochs')\n",
    "\n",
    "# # args = parser.parse_args()\n",
    "# # args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# # np.random.seed(args.seed)\n",
    "# # torch.manual_seed(args.seed)\n",
    "# # if args.cuda:\n",
    "# #     torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "# # params = dict()\n",
    "# # params['no_cuda'] = args.no_cuda\n",
    "# # params['seed'] = args.seed\n",
    "# # params['epochs'] = args.epochs\n",
    "# # params['lr'] = args.lr\n",
    "# # params['weight_decay'] = args.weight_decay\n",
    "# # params['hidden'] = args.hidden\n",
    "# # params['dropout'] = args.dropout\n",
    "# # params['atlas'] = args.atlas\n",
    "# # params['num_features'] = args.num_features\n",
    "# # params['folds'] = args.folds\n",
    "# # params['connectivity'] = args.connectivity\n",
    "# # params['max_degree'] = args.max_degree\n",
    "# # params['ngl'] = args.ngl\n",
    "# # params['edropout'] = args.edropout\n",
    "# # params['train'] = args.train\n",
    "# # params['ckpt_path'] = args.ckpt_path\n",
    "# # params['early_stopping'] = args.early_stopping\n",
    "# # params['early_stopping_patience'] = args.early_stopping_patience\n",
    "\n",
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.no_cuda = False\n",
    "#         self.seed = 46\n",
    "#         self.epochs = 200\n",
    "#         self.lr = 0.001\n",
    "#         self.weight_decay = 5e-5\n",
    "#         self.hidden = 16\n",
    "#         self.dropout = 0.2\n",
    "#         self.atlas = 'cc400'\n",
    "#         self.num_features = 2000\n",
    "#         self.folds = 10\n",
    "#         self.connectivity = 'correlation'\n",
    "#         self.max_degree = 3\n",
    "#         self.ngl = 8\n",
    "#         self.edropout = 0.3\n",
    "#         self.train = 0\n",
    "#         self.ckpt_path = './pth'\n",
    "#         self.early_stopping = True\n",
    "#         self.early_stopping_patience = 20\n",
    "\n",
    "# # Instantiate Args class\n",
    "# args = Args()\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# # Set random seeds\n",
    "# np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# if args.cuda:\n",
    "#     torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# # Create params dictionary\n",
    "# params = vars(args)\n",
    "\n",
    "# # Print Hyperparameters\n",
    "# print('Hyperparameters:')\n",
    "# for key, value in params.items():\n",
    "#     print(key + \":\", value)\n",
    "\n",
    "# corrects = np.zeros(args.folds, dtype=np.int32) \n",
    "# accs = np.zeros(args.folds, dtype=np.float32) \n",
    "# aucs = np.zeros(args.folds, dtype=np.float32)\n",
    "# prfs = np.zeros([args.folds,3], dtype=np.float32) # Save Precision, Recall, F1\n",
    "# test_num = np.zeros(args.folds, dtype=np.float32)\n",
    "\n",
    "# print('  Loading dataset ...')\n",
    "# dataloader = dataloader()\n",
    "# raw_features, y, nonimg = dataloader.load_data(params) \n",
    "# cv_splits = dataloader.data_split(args.folds)\n",
    "# features=raw_features\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# for i in range(args.folds):\n",
    "#     t_start = time.time()\n",
    "#     train_ind, test_ind = cv_splits[i]\n",
    "\n",
    "#     train_ind, valid_ind = train_test_split(train_ind, test_size=0.1, random_state = 24)\n",
    "    \n",
    "#     cv_splits[i] = (train_ind, valid_ind)\n",
    "#     cv_splits[i] = cv_splits[i] + (test_ind,)\n",
    "#     print('Size of the {}-fold Training, Validation, and Test Sets:{},{},{}' .format(i+1, len(cv_splits[i][0]), len(cv_splits[i][1]), len(cv_splits[i][2])))\n",
    "\n",
    "#     if args.train == 1:\n",
    "#         for j in range(args.folds):\n",
    "#             print(' Starting the {}-{} Fold:：'.format(i+1,j+1))\n",
    "#             node_ftr = dataloader.get_node_features(train_ind)\n",
    "#             edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "#             edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "            \n",
    "#             model = GCN(input_dim = args.num_features,\n",
    "#                         nhid = args.hidden, \n",
    "#                         num_classes = 2, \n",
    "#                         ngl = args.ngl, \n",
    "#                         dropout = args.dropout, \n",
    "#                         edge_dropout = args.edropout, \n",
    "#                         edgenet_input_dim = 2*nonimg.shape[1])\n",
    "#             optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "            \n",
    "# #             if args.cuda:\n",
    "#             model\n",
    "#             features = torch.tensor(node_ftr, dtype=torch.float32)\n",
    "#             edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "#             edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "#             labels = torch.tensor(y, dtype=torch.long)\n",
    "#             fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "                \n",
    "#             acc = 0\n",
    "#             best_val_loss = float('inf') # early stoppping: Initialized to positive infinity\n",
    "#             current_patience = 0 # early stopping: Used to record the epochs of the current early stopping\n",
    "\n",
    "#             for epoch in range(args.epochs):\n",
    "#                 # train\n",
    "#                 model.train()\n",
    "#                 with torch.set_grad_enabled(True):\n",
    "#                     optimizer.zero_grad()\n",
    "#                     output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#                     loss_train = torch.nn.CrossEntropyLoss()(output[train_ind], labels[train_ind])\n",
    "#                     loss_train.backward()\n",
    "#                     optimizer.step()\n",
    "#                 acc_train = torchmetrics_accuracy(output[train_ind], labels[train_ind])\n",
    "#                 auc_train = torchmetrics_auc(output[train_ind], labels[train_ind])\n",
    "#                 logits_train = output[train_ind].detach().cpu().numpy()\n",
    "#                 prf_train = prf(logits_train, y[train_ind])\n",
    "\n",
    "                \n",
    "#                 # valid\n",
    "#                 model.eval()\n",
    "#                 with torch.set_grad_enabled(False):\n",
    "#                     output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#                 loss_val = torch.nn.CrossEntropyLoss()(output[valid_ind], labels[valid_ind])\n",
    "#                 acc_val = torchmetrics_accuracy(output[valid_ind], labels[valid_ind])\n",
    "#                 auc_val = torchmetrics_auc(output[valid_ind], labels[valid_ind])\n",
    "#                 logits_val = output[valid_ind].detach().cpu().numpy()\n",
    "#                 prf_val = prf(logits_val, y[valid_ind])\n",
    "\n",
    "                \n",
    "#                 print('Epoch:{:04d}'.format(epoch+1))\n",
    "#                 print('acc_train:{:.4f}'.format(acc_train),\n",
    "#                       'pre_train:{:.4f}'.format(prf_train[0]),\n",
    "#                       'recall_train:{:.4f}'.format(prf_train[1]),\n",
    "#                       'F1_train:{:.4f}'.format(prf_train[2]),\n",
    "#                       'AUC_train:{:.4f}'.format(auc_train))\n",
    "#                 print('acc_val:{:.4f}'.format(acc_val),\n",
    "#                       'pre_val:{:.4f}'.format(prf_val[0]),\n",
    "#                       'recall_val:{:.4f}'.format(prf_val[1]),\n",
    "#                       'F1_val:{:4f}'.format(prf_val[2]),\n",
    "#                       'AUC_val:{:.4f}'.format(auc_val))\n",
    "                \n",
    "#                 # save pth\n",
    "#                 if acc_val > acc and epoch > 50:\n",
    "#                     acc = acc_val\n",
    "#                     if args.ckpt_path != '':\n",
    "#                         if not os.path.exists(args.ckpt_path):\n",
    "#                             os.makedirs(args.ckpt_path)\n",
    "#                         torch.save(model.state_dict(), fold_model_path)\n",
    "                \n",
    "#                 # Early Stopping\n",
    "#                 if epoch > 50 and args.early_stopping == True:\n",
    "#                     if loss_val < best_val_loss:\n",
    "#                         best_val_loss = loss_val\n",
    "#                         current_patience = 0\n",
    "#                     else:\n",
    "#                         current_patience += 1\n",
    "#                     if current_patience >= args.early_stopping_patience:\n",
    "#                         print('Early Stopping!!! epoch：{}'.format(epoch))\n",
    "#                         break\n",
    "                        \n",
    "#         # test\n",
    "#         print(\"Loading the Model for the {}-th Fold:... ...\".format(i+1),\n",
    "#               \"Size of samples in the test set:{}\".format(len(test_ind)))\n",
    "#         model.load_state_dict(torch.load(fold_model_path))\n",
    "#         model.eval()\n",
    "        \n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#         acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "#         auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "#         logits_test = output[test_ind].detach().cpu().numpy()\n",
    "#         correct_test = correct_num(logits_test, y[test_ind])\n",
    "#         prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "#         t_end = time.time()\n",
    "#         t = t_end - t_start\n",
    "#         print('Fold {} Results:'.format(i+1),\n",
    "#               'test acc:{:.4f}'.format(acc_test),\n",
    "#               'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "#               'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "#               'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "#               'test_AUC:{:.4f}'.format(auc_test),\n",
    "#               'time:{:.3f}s'.format(t))\n",
    "        \n",
    "#         correct = correct_test\n",
    "#         aucs[i] = auc_test\n",
    "#         prfs[i] = prf_test\n",
    "#         corrects[i] = correct\n",
    "#         test_num[i] = len(test_ind)\n",
    "    \n",
    "    \n",
    "#     if args.train == 0:\n",
    "#         node_ftr = dataloader.get_node_features(train_ind)\n",
    "#         edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "#         edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "        \n",
    "#         model = GCN(input_dim = args.num_features,\n",
    "#                     nhid = args.hidden, \n",
    "#                     num_classes = 2, \n",
    "#                     ngl = args.ngl, \n",
    "#                     dropout = args.dropout, \n",
    "#                     edge_dropout = args.edropout, \n",
    "#                     edgenet_input_dim = 2*nonimg.shape[1])\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "# #         if args.cuda\n",
    "#         model\n",
    "#         features = torch.tensor(node_ftr, dtype=torch.float)\n",
    "#         edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "#         edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "#         labels = torch.tensor(y, dtype=torch.long)\n",
    "#         fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "        \n",
    "#         model.load_state_dict(torch.load(fold_model_path))\n",
    "#         model.eval()\n",
    "        \n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#         acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "#         auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "#         logits_test = output[test_ind].detach().cpu().numpy()\n",
    "#         correct_test = correct_num(logits_test, y[test_ind])\n",
    "#         prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "#         t_end = time.time()\n",
    "#         t = t_end - t_start\n",
    "#         print('Fold {} Results:'.format(i+1),\n",
    "#               'test acc:{:.4f}'.format(acc_test),\n",
    "#               'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "#               'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "#               'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "#               'test_AUC:{:.4f}'.format(auc_test),\n",
    "#               'time:{:.3f}s'.format(t))\n",
    "        \n",
    "#         correct = correct_test\n",
    "#         aucs[i] = auc_test\n",
    "#         prfs[i] = prf_test\n",
    "#         corrects[i] = correct\n",
    "#         test_num[i] = len(test_ind)\n",
    "\n",
    "# t2 = time.time()\n",
    "\n",
    "# print('\\r\\n======Finish Results for Nested 10-fold cross-validation======')\n",
    "# Nested10kCV_acc = np.sum(corrects) / np.sum(test_num)\n",
    "# Nested10kCV_auc = np.mean(aucs)\n",
    "# Nested10kCV_precision, Nested10kCV_recall, Nested10kCV_F1 = np.mean(prfs, axis=0)\n",
    "# print('Test:',\n",
    "#       'acc:{}'.format(Nested10kCV_acc),\n",
    "#       'precision:{}'.format(Nested10kCV_precision),\n",
    "#       'recall:{}'.format(Nested10kCV_recall),\n",
    "#       'F1:{}'.format(Nested10kCV_F1),\n",
    "#       'AUC:{}'.format(Nested10kCV_auc))\n",
    "# print('Total duration:{}'.format(t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from __future__ import division\n",
    "# from __future__ import print_function\n",
    "\n",
    "# import os\n",
    "# import time\n",
    "# import argparse\n",
    "# import numpy as np\n",
    "# import io\n",
    "# import sys\n",
    "\n",
    "# import torch\n",
    "# import torch.optim as optim\n",
    "\n",
    "# from models import GCN\n",
    "\n",
    "# from metrics import torchmetrics_accuracy, torchmetrics_auc, correct_num, prf\n",
    "\n",
    "# # from dataloader import dataloader\n",
    "\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import auc\n",
    "\n",
    "# if hasattr(sys.stdout, 'buffer'):\n",
    "#     sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "# # Training settings\n",
    "# # parser = argparse.ArgumentParser()\n",
    "# # parser.add_argument('--no_cuda', action='store_true', default=False, help='Disables CUDA training.')\n",
    "# # parser.add_argument('--seed', type=int, default=46, help='Random seed.')\n",
    "# # parser.add_argument('--epochs', type=int, default=200, help='Number of epochs to train.')\n",
    "# # parser.add_argument('--lr', type=float, default=0.001, help='Initial learning rate.')\n",
    "# # parser.add_argument('--weight_decay', type=float, default=5e-5, help='Weight decay (L2 loss on parameters).')\n",
    "# # parser.add_argument('--hidden', type=int, default=16, help='Number of hidden units.')\n",
    "# # parser.add_argument('--dropout', type=float, default=0.2, help='Dropout rate (1 - keep probability).')\n",
    "# # parser.add_argument('--atlas', default='cc200', help='atlas for network construction (node definition) default: ho, see preprocessed-connectomes-project.org/abide/Pipelines.html for more options ')\n",
    "# # parser.add_argument('--num_features', default=2000, type=int, help='Number of features to keep for the feature selection step (default: 2000)')\n",
    "# # parser.add_argument('--folds', default=10, type=int, help='For cross validation, specifies which fold will be used. All folds are used if set to 11 (default: 11)')\n",
    "# # parser.add_argument('--connectivity', default='correlation', help='Type of connectivity used for network construction (default: correlation, options: correlation, partial correlation, tangent)')\n",
    "# # parser.add_argument('--max_degree', type=int, default=3, help='Maximum Chebyshev polynomial degree.')\n",
    "# # parser.add_argument('--ngl', default=8, type=int, help='number of gcn hidden layders (default: 8)')\n",
    "# # parser.add_argument('--edropout', type=float, default=0.3, help='edge dropout rate')\n",
    "# # parser.add_argument('--train', default=1, type=int, help='train(default: 1) or evaluate(0)')\n",
    "# # parser.add_argument('--ckpt_path', type=str, default='./pth', help='checkpoint path to save trained models')\n",
    "# # parser.add_argument('--early_stopping', action='store_true', default=True, help='early stopping switch')\n",
    "# # parser.add_argument('--early_stopping_patience', type=int, default=20, help='early stoppng epochs')\n",
    "\n",
    "# # args = parser.parse_args()\n",
    "# # args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# # np.random.seed(args.seed)\n",
    "# # torch.manual_seed(args.seed)\n",
    "# # if args.cuda:\n",
    "# #     torch.cuda.manual_seed(args.seed)\n",
    "    \n",
    "# # params = dict()\n",
    "# # params['no_cuda'] = args.no_cuda\n",
    "# # params['seed'] = args.seed\n",
    "# # params['epochs'] = args.epochs\n",
    "# # params['lr'] = args.lr\n",
    "# # params['weight_decay'] = args.weight_decay\n",
    "# # params['hidden'] = args.hidden\n",
    "# # params['dropout'] = args.dropout\n",
    "# # params['atlas'] = args.atlas\n",
    "# # params['num_features'] = args.num_features\n",
    "# # params['folds'] = args.folds\n",
    "# # params['connectivity'] = args.connectivity\n",
    "# # params['max_degree'] = args.max_degree\n",
    "# # params['ngl'] = args.ngl\n",
    "# # params['edropout'] = args.edropout\n",
    "# # params['train'] = args.train\n",
    "# # params['ckpt_path'] = args.ckpt_path\n",
    "# # params['early_stopping'] = args.early_stopping\n",
    "# # params['early_stopping_patience'] = args.early_stopping_patience\n",
    "\n",
    "# class Args:\n",
    "#     def __init__(self):\n",
    "#         self.no_cuda = False\n",
    "#         self.seed = 46\n",
    "#         self.epochs = 200\n",
    "#         self.lr = 0.001\n",
    "#         self.weight_decay = 5e-5\n",
    "#         self.hidden = 16\n",
    "#         self.dropout = 0.2\n",
    "#         self.atlas = 'cc400'\n",
    "#         self.num_features = 2000\n",
    "#         self.folds = 10\n",
    "#         self.connectivity = 'correlation'\n",
    "#         self.max_degree = 3\n",
    "#         self.ngl = 8\n",
    "#         self.edropout = 0.3\n",
    "#         self.train = 0\n",
    "#         self.ckpt_path ='./pth'\n",
    "#         self.early_stopping = True\n",
    "#         self.early_stopping_patience = 20\n",
    "\n",
    "# # Instantiate Args class\n",
    "# args = Args()\n",
    "\n",
    "# # Check if CUDA is available\n",
    "# args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# # Set random seeds\n",
    "# np.random.seed(args.seed)\n",
    "# torch.manual_seed(args.seed)\n",
    "# if args.cuda:\n",
    "#     torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# # Create params dictionary\n",
    "# params = vars(args)\n",
    "\n",
    "# # Print Hyperparameters\n",
    "# print('Hyperparameters:')\n",
    "# for key, value in params.items():\n",
    "#     print(key + \":\", value)\n",
    "\n",
    "# corrects = np.zeros(args.folds, dtype=np.int32) \n",
    "# accs = np.zeros(args.folds, dtype=np.float32) \n",
    "# aucs = np.zeros(args.folds, dtype=np.float32)\n",
    "# prfs = np.zeros([args.folds,3], dtype=np.float32) # Save Precision, Recall, F1\n",
    "# test_num = np.zeros(args.folds, dtype=np.float32)\n",
    "\n",
    "# print('  Loading dataset ...')\n",
    "# dataloader = dataloader()\n",
    "# raw_features, y, nonimg = dataloader.load_data(params) \n",
    "# cv_splits = dataloader.data_split(args.folds)\n",
    "# features=raw_features\n",
    "\n",
    "# t1 = time.time()\n",
    "\n",
    "# for i in range(args.folds):\n",
    "#     t_start = time.time()\n",
    "#     train_ind, test_ind = cv_splits[i]\n",
    "\n",
    "#     train_ind, valid_ind = train_test_split(train_ind, test_size=0.1, random_state = 24)\n",
    "    \n",
    "#     cv_splits[i] = (train_ind, valid_ind)\n",
    "#     cv_splits[i] = cv_splits[i] + (test_ind,)\n",
    "#     print('Size of the {}-fold Training, Validation, and Test Sets:{},{},{}' .format(i+1, len(cv_splits[i][0]), len(cv_splits[i][1]), len(cv_splits[i][2])))\n",
    "\n",
    "#     if args.train == 1:\n",
    "#         for j in range(args.folds):\n",
    "#             print(' Starting the {}-{} Fold:：'.format(i+1,j+1))\n",
    "#             node_ftr = dataloader.get_node_features(train_ind)\n",
    "#             edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "#             edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "            \n",
    "#             model = GCN(input_dim = args.num_features,\n",
    "#                         nhid = args.hidden, \n",
    "#                         num_classes = 2, \n",
    "#                         ngl = args.ngl, \n",
    "#                         dropout = args.dropout, \n",
    "#                         edge_dropout = args.edropout, \n",
    "#                         edgenet_input_dim = 2*nonimg.shape[1])\n",
    "#             optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "            \n",
    "# #             if args.cuda:\n",
    "#             model\n",
    "#             features = torch.tensor(node_ftr, dtype=torch.float32)\n",
    "#             edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "#             edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "#             labels = torch.tensor(y, dtype=torch.long)\n",
    "#             fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "                \n",
    "#             acc = 0\n",
    "#             best_val_loss = float('inf') # early stoppping: Initialized to positive infinity\n",
    "#             current_patience = 0 # early stopping: Used to record the epochs of the current early stopping\n",
    "\n",
    "#             for epoch in range(args.epochs):\n",
    "#                 # train\n",
    "#                 model.train()\n",
    "#                 with torch.set_grad_enabled(True):\n",
    "#                     optimizer.zero_grad()\n",
    "#                     output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#                     loss_train = torch.nn.CrossEntropyLoss()(output[train_ind], labels[train_ind])\n",
    "#                     loss_train.backward()\n",
    "#                     optimizer.step()\n",
    "#                 acc_train = torchmetrics_accuracy(output[train_ind], labels[train_ind])\n",
    "#                 auc_train = torchmetrics_auc(output[train_ind], labels[train_ind])\n",
    "#                 logits_train = output[train_ind].detach().cpu().numpy()\n",
    "#                 prf_train = prf(logits_train, y[train_ind])\n",
    "\n",
    "                \n",
    "#                 # valid\n",
    "#                 model.eval()\n",
    "#                 with torch.set_grad_enabled(False):\n",
    "#                     output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#                 loss_val = torch.nn.CrossEntropyLoss()(output[valid_ind], labels[valid_ind])\n",
    "#                 acc_val = torchmetrics_accuracy(output[valid_ind], labels[valid_ind])\n",
    "#                 auc_val = torchmetrics_auc(output[valid_ind], labels[valid_ind])\n",
    "#                 logits_val = output[valid_ind].detach().cpu().numpy()\n",
    "#                 prf_val = prf(logits_val, y[valid_ind])\n",
    "\n",
    "                \n",
    "#                 print('Epoch:{:04d}'.format(epoch+1))\n",
    "#                 print('acc_train:{:.4f}'.format(acc_train),\n",
    "#                       'pre_train:{:.4f}'.format(prf_train[0]),\n",
    "#                       'recall_train:{:.4f}'.format(prf_train[1]),\n",
    "#                       'F1_train:{:.4f}'.format(prf_train[2]),\n",
    "#                       'AUC_train:{:.4f}'.format(auc_train))\n",
    "#                 print('acc_val:{:.4f}'.format(acc_val),\n",
    "#                       'pre_val:{:.4f}'.format(prf_val[0]),\n",
    "#                       'recall_val:{:.4f}'.format(prf_val[1]),\n",
    "#                       'F1_val:{:4f}'.format(prf_val[2]),\n",
    "#                       'AUC_val:{:.4f}'.format(auc_val))\n",
    "                \n",
    "#                 # save pth\n",
    "#                 if acc_val > acc and epoch > 50:\n",
    "#                     acc = acc_val\n",
    "#                     if args.ckpt_path != '':\n",
    "#                         if not os.path.exists(args.ckpt_path):\n",
    "#                             os.makedirs(args.ckpt_path)\n",
    "#                         torch.save(model.state_dict(), fold_model_path)\n",
    "                \n",
    "#                 # Early Stopping\n",
    "#                 if epoch > 50 and args.early_stopping == True:\n",
    "#                     if loss_val < best_val_loss:\n",
    "#                         best_val_loss = loss_val\n",
    "#                         current_patience = 0\n",
    "#                     else:\n",
    "#                         current_patience += 1\n",
    "#                     if current_patience >= args.early_stopping_patience:\n",
    "#                         print('Early Stopping!!! epoch：{}'.format(epoch))\n",
    "#                         break\n",
    "                        \n",
    "#         # test\n",
    "#         print(\"Loading the Model for the {}-th Fold:... ...\".format(i+1),\n",
    "#               \"Size of samples in the test set:{}\".format(len(test_ind)))\n",
    "#         model.load_state_dict(torch.load(fold_model_path))\n",
    "#         model.eval()\n",
    "        \n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#         acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "#         auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "#         logits_test = output[test_ind].detach().cpu().numpy()\n",
    "#         correct_test = correct_num(logits_test, y[test_ind])\n",
    "#         prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "#         t_end = time.time()\n",
    "#         t = t_end - t_start\n",
    "#         print('Fold {} Results:'.format(i+1),\n",
    "#               'test acc:{:.4f}'.format(acc_test),\n",
    "#               'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "#               'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "#               'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "#               'test_AUC:{:.4f}'.format(auc_test),\n",
    "#               'time:{:.3f}s'.format(t))\n",
    "        \n",
    "#         correct = correct_test\n",
    "#         aucs[i] = auc_test\n",
    "#         prfs[i] = prf_test\n",
    "#         corrects[i] = correct\n",
    "#         test_num[i] = len(test_ind)\n",
    "    \n",
    "    \n",
    "#     if args.train == 0:\n",
    "#         node_ftr = dataloader.get_node_features(train_ind)\n",
    "#         edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "#         edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "        \n",
    "#         model = GCN(input_dim = args.num_features,\n",
    "#                     nhid = args.hidden, \n",
    "#                     num_classes = 2, \n",
    "#                     ngl = args.ngl, \n",
    "#                     dropout = args.dropout, \n",
    "#                     edge_dropout = args.edropout, \n",
    "#                     edgenet_input_dim = 2*nonimg.shape[1])\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "# #         if args.cuda\n",
    "#         model\n",
    "#         features = torch.tensor(node_ftr, dtype=torch.float)\n",
    "#         edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "#         edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "#         labels = torch.tensor(y, dtype=torch.long)\n",
    "#         fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "        \n",
    "#         model.load_state_dict(torch.load(fold_model_path))\n",
    "#         model.eval()\n",
    "        \n",
    "#         with torch.set_grad_enabled(False):\n",
    "#             output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "#         acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "#         auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "#         logits_test = output[test_ind].detach().cpu().numpy()\n",
    "#         correct_test = correct_num(logits_test, y[test_ind])\n",
    "#         prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "#         t_end = time.time()\n",
    "#         t = t_end - t_start\n",
    "#         print('Fold {} Results:'.format(i+1),\n",
    "#               'test acc:{:.4f}'.format(acc_test),\n",
    "#               'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "#               'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "#               'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "#               'test_AUC:{:.4f}'.format(auc_test),\n",
    "#               'time:{:.3f}s'.format(t))\n",
    "        \n",
    "#         correct = correct_test\n",
    "#         aucs[i] = auc_test\n",
    "#         prfs[i] = prf_test\n",
    "#         corrects[i] = correct\n",
    "#         test_num[i] = len(test_ind)\n",
    "\n",
    "# t2 = time.time()\n",
    "\n",
    "# print('\\r\\n======Finish Results for Nested 10-fold cross-validation======')\n",
    "# Nested10kCV_acc = np.sum(corrects) / np.sum(test_num)\n",
    "# Nested10kCV_auc = np.mean(aucs)\n",
    "# Nested10kCV_precision, Nested10kCV_recall, Nested10kCV_F1 = np.mean(prfs, axis=0)\n",
    "# print('Test:',\n",
    "#       'acc:{}'.format(Nested10kCV_acc),\n",
    "#       'precision:{}'.format(Nested10kCV_precision),\n",
    "#       'recall:{}'.format(Nested10kCV_recall),\n",
    "#       'F1:{}'.format(Nested10kCV_F1),\n",
    "#       'AUC:{}'.format(Nested10kCV_auc))\n",
    "# print('Total duration:{}'.format(t2 - t1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03838a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
