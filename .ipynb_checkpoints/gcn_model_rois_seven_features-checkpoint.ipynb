{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5d23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from nilearn import connectome\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "# Reading and computing the input data\n",
    "\n",
    "# Selected pipeline\n",
    "pipeline = 'cpac'\n",
    "\n",
    "# Input data variables\n",
    "root_folder = '../ABIDE/'\n",
    "data_folder = os.path.join(root_folder, 'ABIDE_pcp/cpac/filt_noglobal')\n",
    "phenotype = os.path.join(root_folder, 'ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv')\n",
    "\n",
    "\n",
    "def fetch_filenames(subject_IDs, file_type):\n",
    "\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        file_type    : must be one of the available file types\n",
    "\n",
    "    returns:\n",
    "\n",
    "        filenames    : list of filetypes (same length as subject_list)\n",
    "    \"\"\"\n",
    "\n",
    "    import glob\n",
    "\n",
    "    # Specify file mappings for the possible file types\n",
    "    filemapping = {'func_preproc': '_func_preproc.nii.gz',\n",
    "                   'rois_ho': '_rois_ho.1D'}\n",
    "\n",
    "    # The list to be filled\n",
    "    filenames = []\n",
    "\n",
    "    # Fill list with requested file paths\n",
    "    for i in range(len(subject_IDs)):\n",
    "        os.chdir(data_folder)  # os.path.join(data_folder, subject_IDs[i]))\n",
    "        try:\n",
    "            filenames.append(glob.glob('*' + subject_IDs[i] + filemapping[file_type])[0])\n",
    "        except IndexError:\n",
    "            # Return N/A if subject ID is not found\n",
    "            filenames.append('N/A')\n",
    "\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# Get timeseries arrays for list of subjects\n",
    "def get_timeseries(subject_list, atlas_name):\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        atlas_name   : the atlas based on which the timeseries are generated e.g. aal, cc200\n",
    "\n",
    "    returns:\n",
    "        time_series  : list of timeseries arrays, each of shape (timepoints x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    timeseries = []\n",
    "    for i in range(len(subject_list)):\n",
    "        subject_folder = os.path.join(data_folder, subject_list[i])\n",
    "        ro_file = [f for f in os.listdir(subject_folder) if f.endswith('_rois_' + atlas_name + '.1D')]\n",
    "        fl = os.path.join(subject_folder, ro_file[0])\n",
    "        print(\"Reading timeseries file %s\" %fl)\n",
    "        timeseries.append(np.loadtxt(fl, skiprows=0))\n",
    "\n",
    "    return timeseries\n",
    "\n",
    "\n",
    "# Compute connectivity matrices\n",
    "def subject_connectivity(timeseries, subject, atlas_name, kind, save=True, save_path=data_folder):\n",
    "    \"\"\"\n",
    "        timeseries   : timeseries table for subject (timepoints x regions)\n",
    "        subject      : the subject ID\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        save         : save the connectivity matrix to a file\n",
    "        save_path    : specify path to save the matrix if different from subject folder\n",
    "\n",
    "    returns:\n",
    "        connectivity : connectivity matrix (regions x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Estimating %s matrix for subject %s\" % (kind, subject))\n",
    "\n",
    "    if kind in ['tangent', 'partial correlation', 'correlation']:\n",
    "        conn_measure = connectome.ConnectivityMeasure(kind=kind)\n",
    "        connectivity = conn_measure.fit_transform([timeseries])[0]\n",
    "\n",
    "    if save:\n",
    "        subject_file = os.path.join(save_path, subject,\n",
    "                                    subject + '_' + atlas_name + '_' + kind.replace(' ', '_') + '.mat')\n",
    "        sio.savemat(subject_file, {'connectivity': connectivity})\n",
    "\n",
    "    return connectivity\n",
    "\n",
    "\n",
    "# Get the list of subject IDs\n",
    "def get_ids(num_subjects=None):\n",
    "    \"\"\"\n",
    "\n",
    "    return:\n",
    "        subject_IDs    : list of all subject IDs\n",
    "    \"\"\"\n",
    "\n",
    "    subject_IDs = np.genfromtxt(os.path.join(data_folder, 'subject_IDs.txt'), dtype=str)\n",
    "\n",
    "    if num_subjects is not None:\n",
    "        subject_IDs = subject_IDs[:num_subjects]\n",
    "\n",
    "    return subject_IDs\n",
    "\n",
    "\n",
    "# Get phenotype values for a list of subjects\n",
    "def get_subject_score(subject_list, score):\n",
    "    scores_dict = {}\n",
    "\n",
    "    with open(phenotype) as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            if row['SUB_ID'] in subject_list:\n",
    "                scores_dict[row['SUB_ID']] = row[score]\n",
    "\n",
    "    return scores_dict\n",
    "\n",
    "\n",
    "# Dimensionality reduction step for the feature vector using a ridge classifier\n",
    "def feature_selection(matrix, labels, train_ind, fnum):\n",
    "    \"\"\"\n",
    "        matrix       : feature matrix (num_subjects x num_features)\n",
    "        labels       : ground truth labels (num_subjects x 1)\n",
    "        train_ind    : indices of the training samples\n",
    "        fnum         : size of the feature vector after feature selection\n",
    "\n",
    "    return:\n",
    "        x_data      : feature matrix of lower dimension (num_subjects x fnum)\n",
    "    \"\"\"\n",
    "\n",
    "    estimator = RidgeClassifier()\n",
    "    selector = RFE(estimator, n_features_to_select=fnum, step=100, verbose=1)\n",
    "\n",
    "    featureX = matrix[train_ind, :]\n",
    "    featureY = labels[train_ind]\n",
    "    selector = selector.fit(featureX, featureY.ravel())\n",
    "    x_data = selector.transform(matrix)\n",
    "\n",
    "    print(\"Number of labeled samples %d\" % len(train_ind))\n",
    "    print(\"Number of features selected %d\" % x_data.shape[1])\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "# Make sure each site is represented in the training set when selecting a subset of the training set\n",
    "def site_percentage(train_ind, perc, subject_list):\n",
    "    \"\"\"\n",
    "        train_ind    : indices of the training samples\n",
    "        perc         : percentage of training set used\n",
    "        subject_list : list of subject IDs\n",
    "\n",
    "    return:\n",
    "        labeled_indices      : indices of the subset of training samples\n",
    "    \"\"\"\n",
    "\n",
    "    train_list = subject_list[train_ind]\n",
    "    sites = get_subject_score(train_list, score='SITE_ID')\n",
    "    unique = np.unique(list(sites.values())).tolist()\n",
    "    site = np.array([unique.index(sites[train_list[x]]) for x in range(len(train_list))])\n",
    "\n",
    "    labeled_indices = []\n",
    "\n",
    "    for i in np.unique(site):\n",
    "        id_in_site = np.argwhere(site == i).flatten()\n",
    "\n",
    "        num_nodes = len(id_in_site)\n",
    "        labeled_num = int(round(perc * num_nodes))\n",
    "        labeled_indices.extend(train_ind[id_in_site[:labeled_num]])\n",
    "\n",
    "    return labeled_indices\n",
    "\n",
    "\n",
    "# Load precomputed fMRI connectivity networks\n",
    "def get_networks(subject_list, kind, atlas_name=\"aal\", variable='connectivity'):\n",
    "    \"\"\"\n",
    "        subject_list : list of subject IDs\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        variable     : variable name in the .mat file that has been used to save the precomputed networks\n",
    "\n",
    "\n",
    "    return:\n",
    "        matrix      : feature matrix of connectivity networks (num_subjects x network_size)\n",
    "    \"\"\"\n",
    "\n",
    "    all_networks1 = []\n",
    "    all_networks2 = []\n",
    "    all_networks3 = []\n",
    "    all_networks4 = []\n",
    "    all_networks5 = []\n",
    "    all_networks6 = []\n",
    "    all_networks7 = []\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_ez/matrix_rois_ez_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks1.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_ez/matrix_rois_ez_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks1.append(matrix)\n",
    "            \n",
    "    \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_cc400/matrix_rois_cc400_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks2.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_cc400/matrix_rois_cc400_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks2.append(matrix)\n",
    "            \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_aal/matrix_rois_aal_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks3.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_aal/matrix_rois_aal_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks3.append(matrix)\n",
    "            \n",
    "    \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_ho/matrix_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks4.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_ho/matrix_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks4.append(matrix)\n",
    "            \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_cc200/matrix_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks5.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_cc200/matrix_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks5.append(matrix)\n",
    "            \n",
    "            \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_dosenbach160/matrix_rois_dosenbach160_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks6.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_dosenbach160/matrix_rois_dosenbach160_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks6.append(matrix)\n",
    "            \n",
    "            \n",
    "            \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_tt/matrix_rois_tt_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks7.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_tt/matrix_rois_tt_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks7.append(matrix)            \n",
    "            \n",
    "            \n",
    "\n",
    "    idx1 = np.triu_indices_from(all_networks1[0], 1)\n",
    "    norm_networks1 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks1]\n",
    "    vec_networks1 = [mat[idx1] for mat in norm_networks1]\n",
    "    matrix1 = np.vstack(vec_networks1)\n",
    "    \n",
    "    idx2 = np.triu_indices_from(all_networks2[0], 1)\n",
    "    norm_networks2 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks2]\n",
    "    vec_networks2 = [mat[idx2] for mat in norm_networks2]\n",
    "    matrix2 = np.vstack(vec_networks2)\n",
    "    \n",
    "    idx3 = np.triu_indices_from(all_networks3[0], 1)\n",
    "    norm_networks3 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks3]\n",
    "    vec_networks3 = [mat[idx3] for mat in norm_networks3]\n",
    "    matrix3 = np.vstack(vec_networks3)\n",
    "    \n",
    "    idx4 = np.triu_indices_from(all_networks4[0], 1)\n",
    "    norm_networks4 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks4]\n",
    "    vec_networks4 = [mat[idx4] for mat in norm_networks4]\n",
    "    matrix4 = np.vstack(vec_networks4)\n",
    "    \n",
    "    idx5 = np.triu_indices_from(all_networks5[0], 1)\n",
    "    norm_networks5 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks5]\n",
    "    vec_networks5 = [mat[idx5] for mat in norm_networks5]\n",
    "    matrix5 = np.vstack(vec_networks5)\n",
    "    \n",
    "    idx6 = np.triu_indices_from(all_networks6[0], 1)\n",
    "    norm_networks6 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks6]\n",
    "    vec_networks6 = [mat[idx6] for mat in norm_networks6]\n",
    "    matrix6 = np.vstack(vec_networks6)\n",
    "    \n",
    "    idx7 = np.triu_indices_from(all_networks7[0], 1)\n",
    "    norm_networks7 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks7]\n",
    "    vec_networks7 = [mat[idx7] for mat in norm_networks7]\n",
    "    matrix7 = np.vstack(vec_networks7)\n",
    "    \n",
    "    matrix_a =np.concatenate((matrix1,matrix2), axis=1)\n",
    "    matrix_b =np.concatenate((matrix3,matrix4), axis=1)\n",
    "    matrix_c =np.concatenate((matrix5,matrix6), axis=1)\n",
    "    \n",
    "    matrix_a_b = np.concatenate((matrix_a,matrix_b), axis=1)\n",
    "    matrix_c_7 = np.concatenate((matrix_c,matrix7), axis=1)\n",
    "    matrix = np.concatenate((matrix_a_b,matrix_c_7), axis=1)                           \n",
    "    \n",
    "    print(len(matrix[0]));\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# Construct the adjacency matrix of the population from phenotypic scores\n",
    "def create_affinity_graph_from_scores(scores, pd_dict):\n",
    "    num_nodes = len(pd_dict[scores[0]]) \n",
    "    graph = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for l in scores:\n",
    "        label_dict = pd_dict[l]\n",
    "\n",
    "        if l in ['AGE_AT_SCAN', 'FIQ']:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    try:\n",
    "                        val = abs(float(label_dict[k]) - float(label_dict[j]))\n",
    "                        if val < 2:\n",
    "                            graph[k, j] += 1\n",
    "                            graph[j, k] += 1\n",
    "                    except ValueError:  # missing label\n",
    "                        pass\n",
    "\n",
    "        else:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    if label_dict[k] == label_dict[j]:\n",
    "                        graph[k, j] += 1\n",
    "                        graph[j, k] += 1\n",
    "\n",
    "    return graph\n",
    "\n",
    "def get_static_affinity_adj(features, pd_dict):\n",
    "    pd_affinity = create_affinity_graph_from_scores(['SEX', 'SITE_ID'], pd_dict) \n",
    "    distv = distance.pdist(features, metric='correlation') \n",
    "    dist = distance.squareform(distv)  \n",
    "    sigma = np.mean(dist)\n",
    "    feature_sim = np.exp(- dist ** 2 / (2 * sigma ** 2))\n",
    "    adj = pd_affinity * feature_sim  \n",
    "\n",
    "    return adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e2f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10988\\3119403776.py:8: DeprecationWarning: Please use `eigsh` from the `scipy.sparse.linalg` namespace, the `scipy.sparse.linalg.eigen` namespace is deprecated.\n",
      "  from scipy.sparse.linalg.eigen import eigsh\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.spatial import distance\n",
    "from scipy.sparse.linalg.eigen import eigsh\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def get_train_test_masks(labels, idx_train, idx_val, idx_test):\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def load_data(subject_IDs, params): \n",
    "    \n",
    "    # labels\n",
    "    num_classes = 2\n",
    "    num_nodes = len(subject_IDs)\n",
    "    \n",
    "    # 初始化y_data(), y\n",
    "    y_data = np.zeros([num_nodes, num_classes])\n",
    "    y = np.zeros([num_nodes, 1])\n",
    "    \n",
    "    labels = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "    features = get_networks(subject_IDs, kind=params['connectivity'], atlas_name=params['atlas'])\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        y_data[i, int(labels[subject_IDs[i]]) - 1] = 1 # (871,2)\n",
    "        y[i] = int(labels[subject_IDs[i]]) # (871,)\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    cv_splits = list(skf.split(features, np.squeeze(y)))\n",
    "    train = cv_splits[params['folds']][0]\n",
    "    test = cv_splits[params['folds']][1]\n",
    "    val = test\n",
    "    \n",
    "    print('Number of train sample:{}' .format(len(train)))\n",
    "        \n",
    "    y_train, y_val, y_test, train_mask, val_mask, test_mask = get_train_test_masks(y_data, train, val, test)\n",
    "    \n",
    "    y_data = torch.LongTensor(np.where(y_data)[1])\n",
    "    y = torch.LongTensor(y)\n",
    "    y_train = torch.LongTensor(y_train[1])\n",
    "    y_val = torch.LongTensor(y_val[1])\n",
    "    y_test = torch.LongTensor(y_test[1])\n",
    "    \n",
    "    train = torch.LongTensor(train)\n",
    "    val = torch.LongTensor(val)\n",
    "    test = torch.LongTensor(test)\n",
    "    train_mask = torch.LongTensor(train_mask)\n",
    "    val_mask = torch.LongTensor(val_mask)\n",
    "    test_mask = torch.LongTensor(test_mask)\n",
    "    \n",
    "    # Eigenvector\n",
    "    labeled_ind = site_percentage(train, params['num_training'], subject_IDs)\n",
    "    x_data = feature_selection(features, y, labeled_ind, params['num_features'])\n",
    "    features = preprocess_features(sp.coo_matrix(x_data).tolil())\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    \n",
    "    # Adjacency matrix\n",
    "    graph = create_affinity_graph_from_scores(['SEX', 'SITE_ID'], subject_IDs)\n",
    "    distv = distance.pdist(x_data, metric='correlation')\n",
    "    dist = distance.squareform(distv)\n",
    "    sigma = np.mean(dist)\n",
    "    sparse_graph = np.exp(- dist ** 2 / (2 * sigma ** 2))\n",
    "    final_graph = graph * sparse_graph\n",
    "\n",
    "    return final_graph, features, y, y_data, y_train, y_val, y_test, train, val, test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        coords = torch.from_numpy(coords)\n",
    "        values = torch.from_numpy(values)\n",
    "        shape = torch.tensor(shape)\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return adj_normalized\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(sp.eye(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return t_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7980a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# from utils import preprocess_features\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "class dataloader():\n",
    "    def __init__(self): \n",
    "        self.pd_dict = {}\n",
    "        self.node_ftr_dim = 2000\n",
    "        self.num_classes = 2 \n",
    "\n",
    "    def load_data(self, params, connectivity='correlation', atlas='ho'):\n",
    "        ''' load multimodal data from ABIDE\n",
    "        return: imaging features (raw), labels, non-image data\n",
    "        '''\n",
    "        subject_IDs = get_ids()\n",
    "        labels = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "        num_nodes = len(subject_IDs)\n",
    "\n",
    "        sites = get_subject_score(subject_IDs, score='SITE_ID')\n",
    "        unique = np.unique(list(sites.values())).tolist()\n",
    "        ages = get_subject_score(subject_IDs, score='AGE_AT_SCAN')\n",
    "        genders = get_subject_score(subject_IDs, score='SEX') \n",
    "\n",
    "        y_onehot = np.zeros([num_nodes, self.num_classes])\n",
    "        y = np.zeros([num_nodes])\n",
    "        site = np.zeros([num_nodes], dtype=int)\n",
    "        age = np.zeros([num_nodes], dtype=np.float32)\n",
    "        gender = np.zeros([num_nodes], dtype=int)\n",
    "        for i in range(num_nodes):\n",
    "            y_onehot[i, int(labels[subject_IDs[i]])-1] = 1\n",
    "            y[i] = int(labels[subject_IDs[i]])\n",
    "            site[i] = unique.index(sites[subject_IDs[i]])\n",
    "            age[i] = float(ages[subject_IDs[i]])\n",
    "            gender[i] = genders[subject_IDs[i]]\n",
    "        \n",
    "        self.y = y -1  \n",
    "\n",
    "        self.raw_features = get_networks(subject_IDs, kind=connectivity, atlas_name=atlas)\n",
    "\n",
    "        phonetic_data = np.zeros([num_nodes, 3], dtype=np.float32)\n",
    "        phonetic_data[:,0] = site \n",
    "        phonetic_data[:,1] = gender \n",
    "        phonetic_data[:,2] = age \n",
    "\n",
    "        self.pd_dict['SITE_ID'] = np.copy(phonetic_data[:,0])\n",
    "        self.pd_dict['SEX'] = np.copy(phonetic_data[:,1])\n",
    "        self.pd_dict['AGE_AT_SCAN'] = np.copy(phonetic_data[:,2]) \n",
    "        \n",
    "        return self.raw_features, self.y, phonetic_data\n",
    "\n",
    "    def data_split(self, n_folds):\n",
    "        # split data by k-fold CV\n",
    "        skf = StratifiedKFold(n_splits=n_folds)\n",
    "        cv_splits = list(skf.split(self.raw_features, self.y))\n",
    "        return cv_splits \n",
    "\n",
    "    def get_node_features(self, train_ind):\n",
    "        '''preprocess node features for wl-deepgcn\n",
    "        '''\n",
    "        node_ftr = feature_selection(self.raw_features, self.y, train_ind, self.node_ftr_dim)\n",
    "        self.node_ftr = preprocess_features(node_ftr) \n",
    "        return self.node_ftr\n",
    "\n",
    "    def get_WL_inputs(self, nonimg):\n",
    "        '''get WL inputs for wl-deepgcn \n",
    "        '''\n",
    "        # construct edge network inputs \n",
    "        n = self.node_ftr.shape[0] \n",
    "        num_edge = n*(1+n)//2 - n  # n*(n-1)//2,HO=6105\n",
    "        pd_ftr_dim = nonimg.shape[1]\n",
    "        edge_index = np.zeros([2, num_edge], dtype=np.int64) \n",
    "        edgenet_input = np.zeros([num_edge, 2*pd_ftr_dim], dtype=np.float32)  \n",
    "        aff_score = np.zeros(num_edge, dtype=np.float32)\n",
    "        # static affinity score used to pre-prune edges \n",
    "        aff_adj = get_static_affinity_adj(self.node_ftr, self.pd_dict)  \n",
    "        flatten_ind = 0 \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                edge_index[:,flatten_ind] = [i,j]\n",
    "                edgenet_input[flatten_ind]  = np.concatenate((nonimg[i], nonimg[j]))\n",
    "                aff_score[flatten_ind] = aff_adj[i][j]  \n",
    "                flatten_ind +=1\n",
    "\n",
    "        assert flatten_ind == num_edge, \"Error in computing edge input\"\n",
    "        \n",
    "        keep_ind = np.where(aff_score > 1.1)[0]  \n",
    "        edge_index = edge_index[:, keep_ind]\n",
    "        edgenet_input = edgenet_input[keep_ind]\n",
    "\n",
    "        return edge_index, edgenet_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc69f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn import Linear as Lin, Sequential as Seq\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class WL(torch.nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.3):\n",
    "        super(WL, self).__init__()\n",
    "        h1=256\n",
    "        h2=128\n",
    "        self.parser =nn.Sequential(\n",
    "                nn.Linear(input_dim, h1, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.BatchNorm1d(h1),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(h1, h2, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.BatchNorm1d(h2),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(h2, h2, bias=True),\n",
    "                )\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-8)\n",
    "        self.input_dim = input_dim\n",
    "        self.model_init()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.elu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:,0:self.input_dim]\n",
    "        x2 = x[:,self.input_dim:]\n",
    "        h1 = self.parser(x1) \n",
    "        h2 = self.parser(x2) \n",
    "        p = (self.cos(h1,h2) + 1)*0.5\n",
    "        return p\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Lin):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db8243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear as Lin, Sequential as Seq\n",
    "import torch_geometric as tg\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, nhid):\n",
    "        super(MLP,self).__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "            torch.nn.Linear(input_dim,nhid))\n",
    "        \n",
    "    def forward(self, features):\n",
    "        output = self.cls(features)\n",
    "        return output\n",
    "            \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, nhid, num_classes, ngl, dropout, edge_dropout, edgenet_input_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        K=3   \n",
    "        hidden = [nhid for i in range(ngl)] \n",
    "        self.dropout = dropout\n",
    "        self.edge_dropout = edge_dropout \n",
    "        bias = False \n",
    "        self.relu = torch.nn.ReLU(inplace=True) \n",
    "        self.ngl = ngl \n",
    "        self.gconv = nn.ModuleList()\n",
    "        for i in range(ngl):\n",
    "            in_channels = input_dim if i==0  else hidden[i-1]\n",
    "            self.gconv.append(tg.nn.ChebConv(in_channels, hidden[i], K, normalization='sym', bias=bias)) \n",
    "          \n",
    "        self.cls = nn.Sequential(\n",
    "                torch.nn.Linear(16, 128),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(128), \n",
    "                torch.nn.Linear(128, num_classes))\n",
    "\n",
    "        self.edge_net = WL(input_dim=edgenet_input_dim//2, dropout=dropout)\n",
    "        self.model_init()\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Lin):\n",
    "                torch.nn.init.kaiming_normal_(m.weight) # He init\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n",
    "    def forward(self, features, edge_index, edgenet_input, enforce_edropout=False): \n",
    "        if self.edge_dropout>0:\n",
    "            if enforce_edropout or self.training:\n",
    "                one_mask = torch.ones([edgenet_input.shape[0],1])\n",
    "                self.drop_mask = F.dropout(one_mask, self.edge_dropout, True)\n",
    "                self.bool_mask = torch.squeeze(self.drop_mask.type(torch.bool))\n",
    "                edge_index = edge_index[:, self.bool_mask] \n",
    "                edgenet_input = edgenet_input[self.bool_mask] # Weights\n",
    "            \n",
    "        edge_weight = torch.squeeze(self.edge_net(edgenet_input))\n",
    "        \n",
    "\n",
    "        # GCN residual connection\n",
    "        # input layer\n",
    "        features = F.dropout(features, self.dropout, self.training)\n",
    "        x = self.relu(self.gconv[0](features, edge_index, edge_weight)) \n",
    "        x_temp = x\n",
    "        \n",
    "        # hidden layers\n",
    "        for i in range(1, self.ngl - 1): # self.ngl→7\n",
    "            x = F.dropout(x_temp, self.dropout, self.training)\n",
    "            x = self.relu(self.gconv[i](x, edge_index, edge_weight)) \n",
    "            x_temp = x_temp + x # ([871,64])\n",
    "\n",
    "        # output layer\n",
    "        x = F.dropout(x_temp, self.dropout, self.training)\n",
    "        x = self.relu(self.gconv[self.ngl - 1](x, edge_index, edge_weight))\n",
    "        x_temp = x_temp + x\n",
    "\n",
    "        output = x # Final output is not cumulative\n",
    "        output = self.cls(output) \n",
    "        \n",
    "        return output, edge_weight\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165d107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5856d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassSpecificity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def torchmetrics_accuracy(preds, labels):\n",
    "    acc = torchmetrics.functional.accuracy(preds, labels,task=\"multiclass\", num_classes=2)\n",
    "    return acc\n",
    "\n",
    "def torchmetrics_spef(preds, labels):\n",
    "    metric = MulticlassSpecificity(num_classes=2)\n",
    "    spef = metric(preds, labels)\n",
    "    return spef\n",
    "\n",
    "def torchmetrics_auc(preds, labels):\n",
    "    auc = torchmetrics.functional.auroc(preds, labels, task=\"multiclass\", num_classes=2)\n",
    "    return auc\n",
    "\n",
    "def confusion_matrix(preds, labels):\n",
    "    conf_matrix = torch.zeros(2, 2)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    for p, t in zip(preds, labels):\n",
    "        conf_matrix[t, p] += 1 \n",
    "    return conf_matrix\n",
    "def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Input\n",
    "    - cm : computer the value of confusion matrix\n",
    "    - normalize : True: %, False: 123\n",
    "    \"\"\"\n",
    "    classes = ['0:ASD','1:TC']\n",
    "    if normalize:\n",
    "        cm = cm.numpy()\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else '.0f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def correct_num(preds, labels):\n",
    "    \"\"\"Accuracy, auc with masking.Acc of the masked samples\"\"\"\n",
    "    correct_prediction = np.equal(np.argmax(preds, 1), labels).astype(np.float32)\n",
    "    return np.sum(correct_prediction)\n",
    "\n",
    "def prf(preds, labels, is_logit=True):\n",
    "    ''' input: logits, labels  ''' \n",
    "    pred_lab= np.argmax(preds, 1)\n",
    "    p,r,f,s  = precision_recall_fscore_support(labels, pred_lab, average='binary')\n",
    "    return [p,r,f]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae068d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ba9682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "no_cuda: False\n",
      "seed: 46\n",
      "epochs: 1\n",
      "lr: 0.001\n",
      "weight_decay: 5e-05\n",
      "hidden: 16\n",
      "dropout: 0.2\n",
      "atlas: ez\n",
      "num_features: 2000\n",
      "folds: 2\n",
      "connectivity: correlation\n",
      "max_degree: 3\n",
      "ngl: 8\n",
      "edropout: 0.3\n",
      "train: 1\n",
      "ckpt_path: ../folds/rois_seven_pth\n",
      "early_stopping: True\n",
      "early_stopping_patience: 20\n",
      "cuda: False\n",
      "  Loading dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10988\\4081386945.py:290: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks1 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks1]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10988\\4081386945.py:295: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks2 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks2]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10988\\4081386945.py:300: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks3 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks3]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10988\\4081386945.py:305: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks4 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks4]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10988\\4081386945.py:310: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks5 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks5]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10988\\4081386945.py:315: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks6 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks6]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10988\\4081386945.py:320: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks7 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131503\n",
      "Size of the 1-fold Training, Validation, and Test Sets:500,56,556\n",
      " Starting the 1-1 Fold:：\n",
      "Fitting estimator with 131503 features.\n",
      "Fitting estimator with 131403 features.\n",
      "Fitting estimator with 131303 features.\n",
      "Fitting estimator with 131203 features.\n",
      "Fitting estimator with 131103 features.\n",
      "Fitting estimator with 131003 features.\n",
      "Fitting estimator with 130903 features.\n",
      "Fitting estimator with 130803 features.\n",
      "Fitting estimator with 130703 features.\n",
      "Fitting estimator with 130603 features.\n",
      "Fitting estimator with 130503 features.\n",
      "Fitting estimator with 130403 features.\n",
      "Fitting estimator with 130303 features.\n",
      "Fitting estimator with 130203 features.\n",
      "Fitting estimator with 130103 features.\n",
      "Fitting estimator with 130003 features.\n",
      "Fitting estimator with 129903 features.\n",
      "Fitting estimator with 129803 features.\n",
      "Fitting estimator with 129703 features.\n",
      "Fitting estimator with 129603 features.\n",
      "Fitting estimator with 129503 features.\n",
      "Fitting estimator with 129403 features.\n",
      "Fitting estimator with 129303 features.\n",
      "Fitting estimator with 129203 features.\n",
      "Fitting estimator with 129103 features.\n",
      "Fitting estimator with 129003 features.\n",
      "Fitting estimator with 128903 features.\n",
      "Fitting estimator with 128803 features.\n",
      "Fitting estimator with 128703 features.\n",
      "Fitting estimator with 128603 features.\n",
      "Fitting estimator with 128503 features.\n",
      "Fitting estimator with 128403 features.\n",
      "Fitting estimator with 128303 features.\n",
      "Fitting estimator with 128203 features.\n",
      "Fitting estimator with 128103 features.\n",
      "Fitting estimator with 128003 features.\n",
      "Fitting estimator with 127903 features.\n",
      "Fitting estimator with 127803 features.\n",
      "Fitting estimator with 127703 features.\n",
      "Fitting estimator with 127603 features.\n",
      "Fitting estimator with 127503 features.\n",
      "Fitting estimator with 127403 features.\n",
      "Fitting estimator with 127303 features.\n",
      "Fitting estimator with 127203 features.\n",
      "Fitting estimator with 127103 features.\n",
      "Fitting estimator with 127003 features.\n",
      "Fitting estimator with 126903 features.\n",
      "Fitting estimator with 126803 features.\n",
      "Fitting estimator with 126703 features.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 97\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args\u001b[38;5;241m.\u001b[39mfolds):\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m Starting the \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m Fold:：\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m,j\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 97\u001b[0m     node_ftr \u001b[38;5;241m=\u001b[39m dataloader\u001b[38;5;241m.\u001b[39mget_node_features(train_ind)\n\u001b[0;32m     98\u001b[0m     edge_index, edgenet_input \u001b[38;5;241m=\u001b[39m dataloader\u001b[38;5;241m.\u001b[39mget_WL_inputs(nonimg)\n\u001b[0;32m     99\u001b[0m     edgenet_input \u001b[38;5;241m=\u001b[39m (edgenet_input \u001b[38;5;241m-\u001b[39m edgenet_input\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)) \u001b[38;5;241m/\u001b[39m edgenet_input\u001b[38;5;241m.\u001b[39mstd(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 62\u001b[0m, in \u001b[0;36mdataloader.get_node_features\u001b[1;34m(self, train_ind)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_node_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_ind):\n\u001b[0;32m     60\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''preprocess node features for wl-deepgcn\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    '''\u001b[39;00m\n\u001b[1;32m---> 62\u001b[0m     node_ftr \u001b[38;5;241m=\u001b[39m feature_selection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw_features, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39my, train_ind, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_ftr_dim)\n\u001b[0;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_ftr \u001b[38;5;241m=\u001b[39m preprocess_features(node_ftr) \n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnode_ftr\n",
      "Cell \u001b[1;32mIn[1], line 151\u001b[0m, in \u001b[0;36mfeature_selection\u001b[1;34m(matrix, labels, train_ind, fnum)\u001b[0m\n\u001b[0;32m    149\u001b[0m featureX \u001b[38;5;241m=\u001b[39m matrix[train_ind, :]\n\u001b[0;32m    150\u001b[0m featureY \u001b[38;5;241m=\u001b[39m labels[train_ind]\n\u001b[1;32m--> 151\u001b[0m selector \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mfit(featureX, featureY\u001b[38;5;241m.\u001b[39mravel())\n\u001b[0;32m    152\u001b[0m x_data \u001b[38;5;241m=\u001b[39m selector\u001b[38;5;241m.\u001b[39mtransform(matrix)\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of labeled samples \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_ind))\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_selection\\_rfe.py:264\u001b[0m, in \u001b[0;36mRFE.fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Fit the RFE model and then the underlying estimator on the selected features.\u001b[39;00m\n\u001b[0;32m    245\u001b[0m \n\u001b[0;32m    246\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;124;03m    Fitted estimator.\u001b[39;00m\n\u001b[0;32m    262\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    263\u001b[0m _raise_for_unsupported_routing(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfit\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[1;32m--> 264\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\feature_selection\\_rfe.py:311\u001b[0m, in \u001b[0;36mRFE._fit\u001b[1;34m(self, X, y, step_score, **fit_params)\u001b[0m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting estimator with \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m features.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m np\u001b[38;5;241m.\u001b[39msum(support_))\n\u001b[1;32m--> 311\u001b[0m estimator\u001b[38;5;241m.\u001b[39mfit(X[:, features], y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    313\u001b[0m \u001b[38;5;66;03m# Get importance and rank them\u001b[39;00m\n\u001b[0;32m    314\u001b[0m importances \u001b[38;5;241m=\u001b[39m _get_feature_importances(\n\u001b[0;32m    315\u001b[0m     estimator,\n\u001b[0;32m    316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimportance_getter,\n\u001b[0;32m    317\u001b[0m     transform_func\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msquare\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    318\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1457\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorator\u001b[39m(fit_method):\n\u001b[1;32m-> 1457\u001b[0m     \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(fit_method)\n\u001b[0;32m   1458\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m   1459\u001b[0m         global_skip_validation \u001b[38;5;241m=\u001b[39m get_config()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip_parameter_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1461\u001b[0m         \u001b[38;5;66;03m# we don't want to validate again for each call to partial_fit\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import io\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# from dataloader import dataloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "if hasattr(sys.stdout, 'buffer'):\n",
    "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.no_cuda = False\n",
    "        self.seed = 46\n",
    "        self.epochs = 1\n",
    "        self.lr = 0.001\n",
    "        self.weight_decay = 5e-5\n",
    "        self.hidden = 16\n",
    "        self.dropout = 0.2\n",
    "        self.atlas = 'ez'\n",
    "        self.num_features = 2000\n",
    "        self.folds = 2\n",
    "        self.connectivity = 'correlation'\n",
    "        self.max_degree = 3\n",
    "        self.ngl = 8\n",
    "        self.edropout = 0.3\n",
    "        self.train = 1\n",
    "        self.ckpt_path = '../folds/rois_seven_pth'\n",
    "        self.early_stopping = True\n",
    "        self.early_stopping_patience = 20\n",
    "\n",
    "# Instantiate Args class\n",
    "args = Args()\n",
    "\n",
    "# Check if CUDA is available\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Create params dictionary\n",
    "params = vars(args)\n",
    "\n",
    "# Print Hyperparameters\n",
    "print('Hyperparameters:')\n",
    "for key, value in params.items():\n",
    "    print(key + \":\", value)\n",
    "\n",
    "corrects = np.zeros(args.folds, dtype=np.int32) \n",
    "accs = np.zeros(args.folds, dtype=np.float32) \n",
    "aucs = np.zeros(args.folds, dtype=np.float32)\n",
    "prfs = np.zeros([args.folds,3], dtype=np.float32) # Save Precision, Recall, F1\n",
    "test_num = np.zeros(args.folds, dtype=np.float32)\n",
    "\n",
    "\n",
    "print('  Loading dataset ...')\n",
    "dataloader = dataloader()\n",
    "raw_features, y, nonimg = dataloader.load_data(params) \n",
    "cv_splits = dataloader.data_split(args.folds)\n",
    "features=raw_features\n",
    "\n",
    "t1 = time.time()\n",
    "count=1;\n",
    "for i in range(args.folds):\n",
    "    \n",
    "    \n",
    "    \n",
    "    t_start = time.time()\n",
    "    train_ind, test_ind = cv_splits[i]\n",
    "\n",
    "    train_ind, valid_ind = train_test_split(train_ind, test_size=0.1, random_state = 24)\n",
    "    \n",
    "    cv_splits[i] = (train_ind, valid_ind)\n",
    "    cv_splits[i] = cv_splits[i] + (test_ind,)\n",
    "    print('Size of the {}-fold Training, Validation, and Test Sets:{},{},{}' .format(i+1, len(cv_splits[i][0]), len(cv_splits[i][1]), len(cv_splits[i][2])))\n",
    "\n",
    "    if args.train == 1:\n",
    "        for j in range(args.folds):\n",
    "            print(' Starting the {}-{} Fold:：'.format(i+1,j+1))\n",
    "            node_ftr = dataloader.get_node_features(train_ind)\n",
    "            edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "            edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "            \n",
    "            model = GCN(input_dim = args.num_features,\n",
    "                        nhid = args.hidden, \n",
    "                        num_classes = 2, \n",
    "                        ngl = args.ngl, \n",
    "                        dropout = args.dropout, \n",
    "                        edge_dropout = args.edropout, \n",
    "                        edgenet_input_dim = 2*nonimg.shape[1])\n",
    "            optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "            \n",
    "#             if args.cuda:\n",
    "            model\n",
    "            features = torch.tensor(node_ftr, dtype=torch.float32)\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "            edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "            labels = torch.tensor(y, dtype=torch.long)\n",
    "            fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "                \n",
    "            acc = 0\n",
    "            best_val_loss = float('inf') # early stoppping: Initialized to positive infinity\n",
    "            current_patience = 0 # early stopping: Used to record the epochs of the current early stopping\n",
    "            \n",
    "            epoch_store = []\n",
    "            acc_train_store =[]        \n",
    "            pre_train_store =[]\n",
    "            recall_train_store =[]\n",
    "            F1_train_store =[]\n",
    "            AUC_train_store =[]\n",
    "            acc_val_store=[]\n",
    "            pre_val_store=[]\n",
    "            recall_val_store=[]\n",
    "            F1_val_store=[]\n",
    "            AUC_val_store=[]\n",
    "            \n",
    "            for epoch in range(args.epochs):\n",
    "                # train\n",
    "                model.train()\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    optimizer.zero_grad()\n",
    "                    output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "                    loss_train = torch.nn.CrossEntropyLoss()(output[train_ind], labels[train_ind])\n",
    "                    loss_train.backward()\n",
    "                    optimizer.step()\n",
    "                acc_train = torchmetrics_accuracy(output[train_ind], labels[train_ind])\n",
    "                auc_train = torchmetrics_auc(output[train_ind], labels[train_ind])\n",
    "                logits_train = output[train_ind].detach().cpu().numpy()\n",
    "                prf_train = prf(logits_train, y[train_ind])\n",
    "\n",
    "                \n",
    "                # valid\n",
    "                model.eval()\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "                loss_val = torch.nn.CrossEntropyLoss()(output[valid_ind], labels[valid_ind])\n",
    "                acc_val = torchmetrics_accuracy(output[valid_ind], labels[valid_ind])\n",
    "                auc_val = torchmetrics_auc(output[valid_ind], labels[valid_ind])\n",
    "                logits_val = output[valid_ind].detach().cpu().numpy()\n",
    "                prf_val = prf(logits_val, y[valid_ind])\n",
    "\n",
    "                \n",
    "                print('Epoch:{:04d}'.format(epoch+1))\n",
    "                print('acc_train:{:.4f}'.format(acc_train),\n",
    "                      'pre_train:{:.4f}'.format(prf_train[0]),\n",
    "                      'recall_train:{:.4f}'.format(prf_train[1]),\n",
    "                      'F1_train:{:.4f}'.format(prf_train[2]),\n",
    "                      'AUC_train:{:.4f}'.format(auc_train))\n",
    "                print('acc_val:{:.4f}'.format(acc_val),\n",
    "                      'pre_val:{:.4f}'.format(prf_val[0]),\n",
    "                      'recall_val:{:.4f}'.format(prf_val[1]),\n",
    "                      'F1_val:{:4f}'.format(prf_val[2]),\n",
    "                      'AUC_val:{:.4f}'.format(auc_val))\n",
    "                \n",
    "                epoch_store.append(epoch+1)\n",
    "                acc_train_store.append(acc_train)       \n",
    "                pre_train_store.append(prf_train[0])\n",
    "                recall_train_store.append(prf_train[1])\n",
    "                F1_train_store.append(prf_train[2])\n",
    "                AUC_train_store.append(auc_train)\n",
    "                acc_val_store.append(acc_val)\n",
    "                pre_val_store.append(prf_val[0])\n",
    "                recall_val_store.append(prf_val[1])\n",
    "                F1_val_store.append(prf_val[2])\n",
    "                AUC_val_store.append(auc_val)\n",
    "                \n",
    "                # save pth\n",
    "                if acc_val > acc and epoch > 50:\n",
    "                    acc = acc_val\n",
    "                    if args.ckpt_path != '':\n",
    "                        if not os.path.exists(args.ckpt_path):\n",
    "                            os.makedirs(args.ckpt_path)\n",
    "                        torch.save(model.state_dict(), fold_model_path)\n",
    "                \n",
    "                # Early Stopping\n",
    "                if epoch > 50 and args.early_stopping == True:\n",
    "                    if loss_val < best_val_loss:\n",
    "                        best_val_loss = loss_val\n",
    "                        current_patience = 0\n",
    "                    else:\n",
    "                        current_patience += 1\n",
    "                    if current_patience >= args.early_stopping_patience:\n",
    "                        print('Early Stopping!!! epoch：{}'.format(epoch))\n",
    "                        break\n",
    "        print(\"===================================================================\",i,\"_\",j)\n",
    "        data  = { \n",
    "              \"epoch\" : epoch_store ,\n",
    "              \"acc_train\" : acc_train_store ,        \n",
    "              \"pre_train\" : pre_train_store ,\n",
    "              \"recall_train\" : recall_train_store ,\n",
    "              \"F1_train\" : F1_train_store ,\n",
    "              \"AUC_train\" : AUC_train_store ,\n",
    "              \"acc_val\" : acc_val_store,\n",
    "               \"pre_val\" : pre_val_store ,\n",
    "              \"recall_val\" : recall_val_store ,\n",
    "              \"F1_val\" : F1_val_store ,\n",
    "              \"AUC_val\" : AUC_val_store  \n",
    "        }\n",
    "        \n",
    "        \n",
    "        epoch_file_path =  f'../files/rois_seven/file_{i}_{j}_{count}.csv'\n",
    "        data_file = pd.DataFrame(data);\n",
    "        data_file.to_csv(epoch_file_path , index=False);\n",
    "        count=count+1;\n",
    "        # test\n",
    "        print(\"Loading the Model for the {}-th Fold:... ...\".format(i+1),\n",
    "              \"Size of samples in the test set:{}\".format(len(test_ind)))\n",
    "        model.load_state_dict(torch.load(fold_model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "        acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "        auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "        logits_test = output[test_ind].detach().cpu().numpy()\n",
    "        correct_test = correct_num(logits_test, y[test_ind])\n",
    "        prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "        t_end = time.time()\n",
    "        t = t_end - t_start\n",
    "        print('Fold {} Results:'.format(i+1),\n",
    "              'test acc:{:.4f}'.format(acc_test),\n",
    "              'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "              'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "              'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "              'test_AUC:{:.4f}'.format(auc_test),\n",
    "              'time:{:.3f}s'.format(t))\n",
    "        \n",
    "        correct = correct_test\n",
    "        aucs[i] = auc_test\n",
    "        prfs[i] = prf_test\n",
    "        corrects[i] = correct\n",
    "        test_num[i] = len(test_ind)\n",
    "    \n",
    "    \n",
    "    if args.train == 0:\n",
    "        node_ftr = dataloader.get_node_features(train_ind)\n",
    "        edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "        edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "        \n",
    "        model = GCN(input_dim = args.num_features,\n",
    "                    nhid = args.hidden, \n",
    "                    num_classes = 2, \n",
    "                    ngl = args.ngl, \n",
    "                    dropout = args.dropout, \n",
    "                    edge_dropout = args.edropout, \n",
    "                    edgenet_input_dim = 2*nonimg.shape[1])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "#         if args.cuda\n",
    "        model\n",
    "        features = torch.tensor(node_ftr, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "        labels = torch.tensor(y, dtype=torch.long)\n",
    "        fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "        \n",
    "        model.load_state_dict(torch.load(fold_model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "        acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "        auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "        logits_test = output[test_ind].detach().cpu().numpy()\n",
    "        correct_test = correct_num(logits_test, y[test_ind])\n",
    "        prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "        t_end = time.time()\n",
    "        t = t_end - t_start\n",
    "        print('Fold {} Results:'.format(i+1),\n",
    "              'test acc:{:.4f}'.format(acc_test),\n",
    "              'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "              'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "              'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "              'test_AUC:{:.4f}'.format(auc_test),\n",
    "              'time:{:.3f}s'.format(t))\n",
    "        \n",
    "        correct = correct_test\n",
    "        aucs[i] = auc_test\n",
    "        prfs[i] = prf_test\n",
    "        corrects[i] = correct\n",
    "        test_num[i] = len(test_ind)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print('\\r\\n======Finish Results for Nested 10-fold cross-validation======')\n",
    "Nested10kCV_acc = np.sum(corrects) / np.sum(test_num)\n",
    "Nested10kCV_auc = np.mean(aucs)\n",
    "Nested10kCV_precision, Nested10kCV_recall, Nested10kCV_F1 = np.mean(prfs, axis=0)\n",
    "print('Test:',\n",
    "      'acc:{}'.format(Nested10kCV_acc),\n",
    "      'precision:{}'.format(Nested10kCV_precision),\n",
    "      'recall:{}'.format(Nested10kCV_recall),\n",
    "      'F1:{}'.format(Nested10kCV_F1),\n",
    "      'AUC:{}'.format(Nested10kCV_auc))\n",
    "print('Total duration:{}'.format(t2 - t1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845c726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03838a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
