{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc5d23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from nilearn import connectome\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "# Reading and computing the input data\n",
    "\n",
    "# Selected pipeline\n",
    "pipeline = 'cpac'\n",
    "\n",
    "# Input data variables\n",
    "root_folder = '../ABIDE/'\n",
    "data_folder = os.path.join(root_folder, 'ABIDE_pcp/cpac/filt_noglobal')\n",
    "phenotype = os.path.join(root_folder, 'ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv')\n",
    "\n",
    "\n",
    "def fetch_filenames(subject_IDs, file_type):\n",
    "\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        file_type    : must be one of the available file types\n",
    "\n",
    "    returns:\n",
    "\n",
    "        filenames    : list of filetypes (same length as subject_list)\n",
    "    \"\"\"\n",
    "\n",
    "    import glob\n",
    "\n",
    "    # Specify file mappings for the possible file types\n",
    "    filemapping = {'func_preproc': '_func_preproc.nii.gz',\n",
    "                   'rois_ho': '_rois_ho.1D'}\n",
    "\n",
    "    # The list to be filled\n",
    "    filenames = []\n",
    "\n",
    "    # Fill list with requested file paths\n",
    "    for i in range(len(subject_IDs)):\n",
    "        os.chdir(data_folder)  # os.path.join(data_folder, subject_IDs[i]))\n",
    "        try:\n",
    "            filenames.append(glob.glob('*' + subject_IDs[i] + filemapping[file_type])[0])\n",
    "        except IndexError:\n",
    "            # Return N/A if subject ID is not found\n",
    "            filenames.append('N/A')\n",
    "\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# Get timeseries arrays for list of subjects\n",
    "def get_timeseries(subject_list, atlas_name):\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        atlas_name   : the atlas based on which the timeseries are generated e.g. aal, cc200\n",
    "\n",
    "    returns:\n",
    "        time_series  : list of timeseries arrays, each of shape (timepoints x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    timeseries = []\n",
    "    for i in range(len(subject_list)):\n",
    "        subject_folder = os.path.join(data_folder, subject_list[i])\n",
    "        ro_file = [f for f in os.listdir(subject_folder) if f.endswith('_rois_' + atlas_name + '.1D')]\n",
    "        fl = os.path.join(subject_folder, ro_file[0])\n",
    "        print(\"Reading timeseries file %s\" %fl)\n",
    "        timeseries.append(np.loadtxt(fl, skiprows=0))\n",
    "\n",
    "    return timeseries\n",
    "\n",
    "\n",
    "# Compute connectivity matrices\n",
    "def subject_connectivity(timeseries, subject, atlas_name, kind, save=True, save_path=data_folder):\n",
    "    \"\"\"\n",
    "        timeseries   : timeseries table for subject (timepoints x regions)\n",
    "        subject      : the subject ID\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        save         : save the connectivity matrix to a file\n",
    "        save_path    : specify path to save the matrix if different from subject folder\n",
    "\n",
    "    returns:\n",
    "        connectivity : connectivity matrix (regions x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Estimating %s matrix for subject %s\" % (kind, subject))\n",
    "\n",
    "    if kind in ['tangent', 'partial correlation', 'correlation']:\n",
    "        conn_measure = connectome.ConnectivityMeasure(kind=kind)\n",
    "        connectivity = conn_measure.fit_transform([timeseries])[0]\n",
    "\n",
    "    if save:\n",
    "        subject_file = os.path.join(save_path, subject,\n",
    "                                    subject + '_' + atlas_name + '_' + kind.replace(' ', '_') + '.mat')\n",
    "        sio.savemat(subject_file, {'connectivity': connectivity})\n",
    "\n",
    "    return connectivity\n",
    "\n",
    "\n",
    "# Get the list of subject IDs\n",
    "def get_ids(num_subjects=None):\n",
    "    \"\"\"\n",
    "\n",
    "    return:\n",
    "        subject_IDs    : list of all subject IDs\n",
    "    \"\"\"\n",
    "\n",
    "    subject_IDs = np.genfromtxt(os.path.join(data_folder, 'subject_IDs.txt'), dtype=str)\n",
    "\n",
    "    if num_subjects is not None:\n",
    "        subject_IDs = subject_IDs[:num_subjects]\n",
    "\n",
    "    return subject_IDs\n",
    "\n",
    "\n",
    "# Get phenotype values for a list of subjects\n",
    "def get_subject_score(subject_list, score):\n",
    "    scores_dict = {}\n",
    "\n",
    "    with open(phenotype) as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            if row['SUB_ID'] in subject_list:\n",
    "                scores_dict[row['SUB_ID']] = row[score]\n",
    "\n",
    "    return scores_dict\n",
    "\n",
    "\n",
    "# Dimensionality reduction step for the feature vector using a ridge classifier\n",
    "def feature_selection(matrix, labels, train_ind, fnum):\n",
    "    \"\"\"\n",
    "        matrix       : feature matrix (num_subjects x num_features)\n",
    "        labels       : ground truth labels (num_subjects x 1)\n",
    "        train_ind    : indices of the training samples\n",
    "        fnum         : size of the feature vector after feature selection\n",
    "\n",
    "    return:\n",
    "        x_data      : feature matrix of lower dimension (num_subjects x fnum)\n",
    "    \"\"\"\n",
    "\n",
    "    estimator = RidgeClassifier()\n",
    "    selector = RFE(estimator, n_features_to_select=fnum, step=100, verbose=1)\n",
    "\n",
    "    featureX = matrix[train_ind, :]\n",
    "    featureY = labels[train_ind]\n",
    "    selector = selector.fit(featureX, featureY.ravel())\n",
    "    x_data = selector.transform(matrix)\n",
    "\n",
    "    print(\"Number of labeled samples %d\" % len(train_ind))\n",
    "    print(\"Number of features selected %d\" % x_data.shape[1])\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "# Make sure each site is represented in the training set when selecting a subset of the training set\n",
    "def site_percentage(train_ind, perc, subject_list):\n",
    "    \"\"\"\n",
    "        train_ind    : indices of the training samples\n",
    "        perc         : percentage of training set used\n",
    "        subject_list : list of subject IDs\n",
    "\n",
    "    return:\n",
    "        labeled_indices      : indices of the subset of training samples\n",
    "    \"\"\"\n",
    "\n",
    "    train_list = subject_list[train_ind]\n",
    "    sites = get_subject_score(train_list, score='SITE_ID')\n",
    "    unique = np.unique(list(sites.values())).tolist()\n",
    "    site = np.array([unique.index(sites[train_list[x]]) for x in range(len(train_list))])\n",
    "\n",
    "    labeled_indices = []\n",
    "\n",
    "    for i in np.unique(site):\n",
    "        id_in_site = np.argwhere(site == i).flatten()\n",
    "\n",
    "        num_nodes = len(id_in_site)\n",
    "        labeled_num = int(round(perc * num_nodes))\n",
    "        labeled_indices.extend(train_ind[id_in_site[:labeled_num]])\n",
    "\n",
    "    return labeled_indices\n",
    "\n",
    "\n",
    "# Load precomputed fMRI connectivity networks\n",
    "def get_networks(subject_list, kind, atlas_name=\"aal\", variable='connectivity'):\n",
    "    \"\"\"\n",
    "        subject_list : list of subject IDs\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        variable     : variable name in the .mat file that has been used to save the precomputed networks\n",
    "\n",
    "\n",
    "    return:\n",
    "        matrix      : feature matrix of connectivity networks (num_subjects x network_size)\n",
    "    \"\"\"\n",
    "\n",
    "    all_networks = []\n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_cc200/matrix_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_cc200/matrix_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks.append(matrix)\n",
    "            \n",
    "            \n",
    "    # all_networks=np.array(all_networks)\n",
    "\n",
    "    idx = np.triu_indices_from(all_networks[0], 1)\n",
    "    norm_networks = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks]\n",
    "    vec_networks = [mat[idx] for mat in norm_networks]\n",
    "    matrix = np.vstack(vec_networks)\n",
    "\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# Construct the adjacency matrix of the population from phenotypic scores\n",
    "def create_affinity_graph_from_scores(scores, pd_dict):\n",
    "    num_nodes = len(pd_dict[scores[0]]) \n",
    "    graph = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for l in scores:\n",
    "        label_dict = pd_dict[l]\n",
    "\n",
    "        if l in ['AGE_AT_SCAN', 'FIQ']:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    try:\n",
    "                        val = abs(float(label_dict[k]) - float(label_dict[j]))\n",
    "                        if val < 2:\n",
    "                            graph[k, j] += 1\n",
    "                            graph[j, k] += 1\n",
    "                    except ValueError:  # missing label\n",
    "                        pass\n",
    "\n",
    "        else:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    if label_dict[k] == label_dict[j]:\n",
    "                        graph[k, j] += 1\n",
    "                        graph[j, k] += 1\n",
    "\n",
    "    return graph\n",
    "\n",
    "def get_static_affinity_adj(features, pd_dict):\n",
    "    pd_affinity = create_affinity_graph_from_scores(['SEX', 'SITE_ID'], pd_dict) \n",
    "    distv = distance.pdist(features, metric='correlation') \n",
    "    dist = distance.squareform(distv)  \n",
    "    sigma = np.mean(dist)\n",
    "    feature_sim = np.exp(- dist ** 2 / (2 * sigma ** 2))\n",
    "    adj = pd_affinity * feature_sim  \n",
    "\n",
    "    return adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "50e2f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_2196\\3119403776.py:8: DeprecationWarning: Please use `eigsh` from the `scipy.sparse.linalg` namespace, the `scipy.sparse.linalg.eigen` namespace is deprecated.\n",
      "  from scipy.sparse.linalg.eigen import eigsh\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.spatial import distance\n",
    "from scipy.sparse.linalg.eigen import eigsh\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def get_train_test_masks(labels, idx_train, idx_val, idx_test):\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def load_data(subject_IDs, params): \n",
    "    \n",
    "    # labels\n",
    "    num_classes = 2\n",
    "    num_nodes = len(subject_IDs)\n",
    "    \n",
    "    # 初始化y_data(), y\n",
    "    y_data = np.zeros([num_nodes, num_classes])\n",
    "    y = np.zeros([num_nodes, 1])\n",
    "    \n",
    "    labels = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "    features = get_networks(subject_IDs, kind=params['connectivity'], atlas_name=params['atlas'])\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        y_data[i, int(labels[subject_IDs[i]]) - 1] = 1 # (871,2)\n",
    "        y[i] = int(labels[subject_IDs[i]]) # (871,)\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    cv_splits = list(skf.split(features, np.squeeze(y)))\n",
    "    train = cv_splits[params['folds']][0]\n",
    "    test = cv_splits[params['folds']][1]\n",
    "    val = test\n",
    "    \n",
    "    print('Number of train sample:{}' .format(len(train)))\n",
    "        \n",
    "    y_train, y_val, y_test, train_mask, val_mask, test_mask = get_train_test_masks(y_data, train, val, test)\n",
    "    \n",
    "    y_data = torch.LongTensor(np.where(y_data)[1])\n",
    "    y = torch.LongTensor(y)\n",
    "    y_train = torch.LongTensor(y_train[1])\n",
    "    y_val = torch.LongTensor(y_val[1])\n",
    "    y_test = torch.LongTensor(y_test[1])\n",
    "    \n",
    "    train = torch.LongTensor(train)\n",
    "    val = torch.LongTensor(val)\n",
    "    test = torch.LongTensor(test)\n",
    "    train_mask = torch.LongTensor(train_mask)\n",
    "    val_mask = torch.LongTensor(val_mask)\n",
    "    test_mask = torch.LongTensor(test_mask)\n",
    "    \n",
    "    # Eigenvector\n",
    "    labeled_ind = site_percentage(train, params['num_training'], subject_IDs)\n",
    "    x_data = feature_selection(features, y, labeled_ind, params['num_features'])\n",
    "    features = preprocess_features(sp.coo_matrix(x_data).tolil())\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    \n",
    "    # Adjacency matrix\n",
    "    graph = create_affinity_graph_from_scores(['SEX', 'SITE_ID'], subject_IDs)\n",
    "    distv = distance.pdist(x_data, metric='correlation')\n",
    "    dist = distance.squareform(distv)\n",
    "    sigma = np.mean(dist)\n",
    "    sparse_graph = np.exp(- dist ** 2 / (2 * sigma ** 2))\n",
    "    final_graph = graph * sparse_graph\n",
    "\n",
    "    return final_graph, features, y, y_data, y_train, y_val, y_test, train, val, test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        coords = torch.from_numpy(coords)\n",
    "        values = torch.from_numpy(values)\n",
    "        shape = torch.tensor(shape)\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return adj_normalized\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(sp.eye(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return t_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7980a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# from utils import preprocess_features\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "class dataloader():\n",
    "    def __init__(self): \n",
    "        self.pd_dict = {}\n",
    "        self.node_ftr_dim = 2000\n",
    "        self.num_classes = 2 \n",
    "\n",
    "    def load_data(self, params, connectivity='correlation', atlas='ho'):\n",
    "        ''' load multimodal data from ABIDE\n",
    "        return: imaging features (raw), labels, non-image data\n",
    "        '''\n",
    "        subject_IDs = get_ids()\n",
    "        labels = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "        num_nodes = len(subject_IDs)\n",
    "\n",
    "        sites = get_subject_score(subject_IDs, score='SITE_ID')\n",
    "        unique = np.unique(list(sites.values())).tolist()\n",
    "        ages = get_subject_score(subject_IDs, score='AGE_AT_SCAN')\n",
    "        genders = get_subject_score(subject_IDs, score='SEX') \n",
    "\n",
    "        y_onehot = np.zeros([num_nodes, self.num_classes])\n",
    "        y = np.zeros([num_nodes])\n",
    "        site = np.zeros([num_nodes], dtype=int)\n",
    "        age = np.zeros([num_nodes], dtype=np.float32)\n",
    "        gender = np.zeros([num_nodes], dtype=int)\n",
    "        for i in range(num_nodes):\n",
    "            y_onehot[i, int(labels[subject_IDs[i]])-1] = 1\n",
    "            y[i] = int(labels[subject_IDs[i]])\n",
    "            site[i] = unique.index(sites[subject_IDs[i]])\n",
    "            age[i] = float(ages[subject_IDs[i]])\n",
    "            gender[i] = genders[subject_IDs[i]]\n",
    "        \n",
    "        self.y = y -1  \n",
    "\n",
    "        self.raw_features = get_networks(subject_IDs, kind=connectivity, atlas_name=atlas)\n",
    "\n",
    "        phonetic_data = np.zeros([num_nodes, 3], dtype=np.float32)\n",
    "        phonetic_data[:,0] = site \n",
    "        phonetic_data[:,1] = gender \n",
    "        phonetic_data[:,2] = age \n",
    "\n",
    "        self.pd_dict['SITE_ID'] = np.copy(phonetic_data[:,0])\n",
    "        self.pd_dict['SEX'] = np.copy(phonetic_data[:,1])\n",
    "        self.pd_dict['AGE_AT_SCAN'] = np.copy(phonetic_data[:,2]) \n",
    "        \n",
    "        return self.raw_features, self.y, phonetic_data\n",
    "\n",
    "    def data_split(self, n_folds):\n",
    "        # split data by k-fold CV\n",
    "        skf = StratifiedKFold(n_splits=n_folds)\n",
    "        cv_splits = list(skf.split(self.raw_features, self.y))\n",
    "        return cv_splits \n",
    "\n",
    "    def get_node_features(self, train_ind):\n",
    "        '''preprocess node features for wl-deepgcn\n",
    "        '''\n",
    "        node_ftr = feature_selection(self.raw_features, self.y, train_ind, self.node_ftr_dim)\n",
    "        self.node_ftr = preprocess_features(node_ftr) \n",
    "        return self.node_ftr\n",
    "\n",
    "    def get_WL_inputs(self, nonimg):\n",
    "        '''get WL inputs for wl-deepgcn \n",
    "        '''\n",
    "        # construct edge network inputs \n",
    "        n = self.node_ftr.shape[0] \n",
    "        num_edge = n*(1+n)//2 - n  # n*(n-1)//2,HO=6105\n",
    "        pd_ftr_dim = nonimg.shape[1]\n",
    "        edge_index = np.zeros([2, num_edge], dtype=np.int64) \n",
    "        edgenet_input = np.zeros([num_edge, 2*pd_ftr_dim], dtype=np.float32)  \n",
    "        aff_score = np.zeros(num_edge, dtype=np.float32)\n",
    "        # static affinity score used to pre-prune edges \n",
    "        aff_adj = get_static_affinity_adj(self.node_ftr, self.pd_dict)  \n",
    "        flatten_ind = 0 \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                edge_index[:,flatten_ind] = [i,j]\n",
    "                edgenet_input[flatten_ind]  = np.concatenate((nonimg[i], nonimg[j]))\n",
    "                aff_score[flatten_ind] = aff_adj[i][j]  \n",
    "                flatten_ind +=1\n",
    "\n",
    "        assert flatten_ind == num_edge, \"Error in computing edge input\"\n",
    "        \n",
    "        keep_ind = np.where(aff_score > 1.1)[0]  \n",
    "        edge_index = edge_index[:, keep_ind]\n",
    "        edgenet_input = edgenet_input[keep_ind]\n",
    "\n",
    "        return edge_index, edgenet_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abc69f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn import Linear as Lin, Sequential as Seq\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class WL(torch.nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.3):\n",
    "        super(WL, self).__init__()\n",
    "        h1=256\n",
    "        h2=128\n",
    "        self.parser =nn.Sequential(\n",
    "                nn.Linear(input_dim, h1, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.BatchNorm1d(h1),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(h1, h2, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.BatchNorm1d(h2),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(h2, h2, bias=True),\n",
    "                )\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-8)\n",
    "        self.input_dim = input_dim\n",
    "        self.model_init()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.elu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:,0:self.input_dim]\n",
    "        x2 = x[:,self.input_dim:]\n",
    "        h1 = self.parser(x1) \n",
    "        h2 = self.parser(x2) \n",
    "        p = (self.cos(h1,h2) + 1)*0.5\n",
    "        return p\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Lin):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6db8243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear as Lin, Sequential as Seq\n",
    "import torch_geometric as tg\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, nhid):\n",
    "        super(MLP,self).__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "            torch.nn.Linear(input_dim,nhid))\n",
    "        \n",
    "    def forward(self, features):\n",
    "        output = self.cls(features)\n",
    "        return output\n",
    "            \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, nhid, num_classes, ngl, dropout, edge_dropout, edgenet_input_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        K=3   \n",
    "        hidden = [nhid for i in range(ngl)] \n",
    "        self.dropout = dropout\n",
    "        self.edge_dropout = edge_dropout \n",
    "        bias = False \n",
    "        self.relu = torch.nn.ReLU(inplace=True) \n",
    "        self.ngl = ngl \n",
    "        self.gconv = nn.ModuleList()\n",
    "        for i in range(ngl):\n",
    "            in_channels = input_dim if i==0  else hidden[i-1]\n",
    "            self.gconv.append(tg.nn.ChebConv(in_channels, hidden[i], K, normalization='sym', bias=bias)) \n",
    "          \n",
    "        self.cls = nn.Sequential(\n",
    "                torch.nn.Linear(16, 128),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(128), \n",
    "                torch.nn.Linear(128, num_classes))\n",
    "\n",
    "        self.edge_net = WL(input_dim=edgenet_input_dim//2, dropout=dropout)\n",
    "        self.model_init()\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Lin):\n",
    "                torch.nn.init.kaiming_normal_(m.weight) # He init\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n",
    "    def forward(self, features, edge_index, edgenet_input, enforce_edropout=False): \n",
    "        if self.edge_dropout>0:\n",
    "            if enforce_edropout or self.training:\n",
    "                one_mask = torch.ones([edgenet_input.shape[0],1])\n",
    "                self.drop_mask = F.dropout(one_mask, self.edge_dropout, True)\n",
    "                self.bool_mask = torch.squeeze(self.drop_mask.type(torch.bool))\n",
    "                edge_index = edge_index[:, self.bool_mask] \n",
    "                edgenet_input = edgenet_input[self.bool_mask] # Weights\n",
    "            \n",
    "        edge_weight = torch.squeeze(self.edge_net(edgenet_input))\n",
    "        \n",
    "\n",
    "        # GCN residual connection\n",
    "        # input layer\n",
    "        features = F.dropout(features, self.dropout, self.training)\n",
    "        x = self.relu(self.gconv[0](features, edge_index, edge_weight)) \n",
    "        x_temp = x\n",
    "        \n",
    "        # hidden layers\n",
    "        for i in range(1, self.ngl - 1): # self.ngl→7\n",
    "            x = F.dropout(x_temp, self.dropout, self.training)\n",
    "            x = self.relu(self.gconv[i](x, edge_index, edge_weight)) \n",
    "            x_temp = x_temp + x # ([871,64])\n",
    "\n",
    "        # output layer\n",
    "        x = F.dropout(x_temp, self.dropout, self.training)\n",
    "        x = self.relu(self.gconv[self.ngl - 1](x, edge_index, edge_weight))\n",
    "        x_temp = x_temp + x\n",
    "\n",
    "        output = x # Final output is not cumulative\n",
    "        output = self.cls(output) \n",
    "        \n",
    "        return output, edge_weight\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "165d107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5856d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassSpecificity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def torchmetrics_accuracy(preds, labels):\n",
    "    acc = torchmetrics.functional.accuracy(preds, labels,task=\"multiclass\", num_classes=2)\n",
    "    return acc\n",
    "\n",
    "def torchmetrics_spef(preds, labels):\n",
    "    metric = MulticlassSpecificity(num_classes=2)\n",
    "    spef = metric(preds, labels)\n",
    "    return spef\n",
    "\n",
    "def torchmetrics_auc(preds, labels):\n",
    "    auc = torchmetrics.functional.auroc(preds, labels, task=\"multiclass\", num_classes=2)\n",
    "    return auc\n",
    "\n",
    "def confusion_matrix(preds, labels):\n",
    "    conf_matrix = torch.zeros(2, 2)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    for p, t in zip(preds, labels):\n",
    "        conf_matrix[t, p] += 1 \n",
    "    return conf_matrix\n",
    "def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Input\n",
    "    - cm : computer the value of confusion matrix\n",
    "    - normalize : True: %, False: 123\n",
    "    \"\"\"\n",
    "    classes = ['0:ASD','1:TC']\n",
    "    if normalize:\n",
    "        cm = cm.numpy()\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else '.0f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def correct_num(preds, labels):\n",
    "    \"\"\"Accuracy, auc with masking.Acc of the masked samples\"\"\"\n",
    "    correct_prediction = np.equal(np.argmax(preds, 1), labels).astype(np.float32)\n",
    "    return np.sum(correct_prediction)\n",
    "\n",
    "def prf(preds, labels, is_logit=True):\n",
    "    ''' input: logits, labels  ''' \n",
    "    pred_lab= np.argmax(preds, 1)\n",
    "    p,r,f,s  = precision_recall_fscore_support(labels, pred_lab, average='binary')\n",
    "    return [p,r,f]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ae068d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ba9682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "no_cuda: False\n",
      "seed: 46\n",
      "epochs: 200\n",
      "lr: 0.001\n",
      "weight_decay: 5e-05\n",
      "hidden: 16\n",
      "dropout: 0.2\n",
      "atlas: ho\n",
      "num_features: 2000\n",
      "folds: 10\n",
      "connectivity: correlation\n",
      "max_degree: 3\n",
      "ngl: 8\n",
      "edropout: 0.3\n",
      "train: 1\n",
      "ckpt_path: ../folds/rois_cc200_pth\n",
      "early_stopping: True\n",
      "early_stopping_patience: 20\n",
      "cuda: False\n",
      "  Loading dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_2196\\3229437123.py:216: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the 1-fold Training, Validation, and Test Sets:900,100,112\n",
      " Starting the 1-1 Fold:：\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.4822 pre_train:0.4989 recall_train:0.4989 F1_train:0.4989 AUC_train:0.4661\n",
      "acc_val:0.6000 pre_val:0.5694 recall_val:0.8200 F1_val:0.672131 AUC_val:0.6832\n",
      "Epoch:0002\n",
      "acc_train:0.5256 pre_train:0.5468 recall_train:0.4774 F1_train:0.5098 AUC_train:0.5268\n",
      "acc_val:0.6000 pre_val:0.5893 recall_val:0.6600 F1_val:0.622642 AUC_val:0.6776\n",
      "Epoch:0003\n",
      "acc_train:0.5356 pre_train:0.5644 recall_train:0.4430 F1_train:0.4964 AUC_train:0.5383\n",
      "acc_val:0.6000 pre_val:0.5926 recall_val:0.6400 F1_val:0.615385 AUC_val:0.6728\n",
      "Epoch:0004\n",
      "acc_train:0.5267 pre_train:0.5419 recall_train:0.5419 F1_train:0.5419 AUC_train:0.5438\n",
      "acc_val:0.6000 pre_val:0.6000 recall_val:0.6000 F1_val:0.600000 AUC_val:0.6716\n",
      "Epoch:0005\n",
      "acc_train:0.5544 pre_train:0.5796 recall_train:0.5011 F1_train:0.5375 AUC_train:0.5712\n",
      "acc_val:0.6200 pre_val:0.6250 recall_val:0.6000 F1_val:0.612245 AUC_val:0.6812\n",
      "Epoch:0006\n",
      "acc_train:0.5578 pre_train:0.5835 recall_train:0.5032 F1_train:0.5404 AUC_train:0.5944\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6820\n",
      "Epoch:0007\n",
      "acc_train:0.5633 pre_train:0.5933 recall_train:0.4925 F1_train:0.5382 AUC_train:0.5771\n",
      "acc_val:0.6400 pre_val:0.6458 recall_val:0.6200 F1_val:0.632653 AUC_val:0.6856\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0008\n",
      "acc_train:0.5911 pre_train:0.6240 recall_train:0.5247 F1_train:0.5701 AUC_train:0.6045\n",
      "acc_val:0.6100 pre_val:0.6078 recall_val:0.6200 F1_val:0.613861 AUC_val:0.6836\n",
      "Epoch:0009\n",
      "acc_train:0.5833 pre_train:0.6056 recall_train:0.5548 F1_train:0.5791 AUC_train:0.6230\n",
      "acc_val:0.6100 pre_val:0.6038 recall_val:0.6400 F1_val:0.621359 AUC_val:0.6820\n",
      "Epoch:0010\n",
      "acc_train:0.5900 pre_train:0.6096 recall_train:0.5742 F1_train:0.5914 AUC_train:0.6283\n",
      "acc_val:0.6300 pre_val:0.6066 recall_val:0.7400 F1_val:0.666667 AUC_val:0.6848\n",
      "Epoch:0011\n",
      "acc_train:0.5522 pre_train:0.5692 recall_train:0.5484 F1_train:0.5586 AUC_train:0.5916\n",
      "acc_val:0.6300 pre_val:0.6066 recall_val:0.7400 F1_val:0.666667 AUC_val:0.6856\n",
      "Epoch:0012\n",
      "acc_train:0.6044 pre_train:0.6307 recall_train:0.5656 F1_train:0.5964 AUC_train:0.6109\n",
      "acc_val:0.6200 pre_val:0.6111 recall_val:0.6600 F1_val:0.634615 AUC_val:0.6896\n",
      "Epoch:0013\n",
      "acc_train:0.5878 pre_train:0.6169 recall_train:0.5333 F1_train:0.5721 AUC_train:0.6162\n",
      "acc_val:0.6300 pre_val:0.6226 recall_val:0.6600 F1_val:0.640777 AUC_val:0.6912\n",
      "Epoch:0014\n",
      "acc_train:0.6178 pre_train:0.6263 recall_train:0.6452 F1_train:0.6356 AUC_train:0.6509\n",
      "acc_val:0.6100 pre_val:0.6078 recall_val:0.6200 F1_val:0.613861 AUC_val:0.6904\n",
      "Epoch:0015\n",
      "acc_train:0.6067 pre_train:0.6344 recall_train:0.5634 F1_train:0.5968 AUC_train:0.6424\n",
      "acc_val:0.6400 pre_val:0.6458 recall_val:0.6200 F1_val:0.632653 AUC_val:0.6916\n",
      "Epoch:0016\n",
      "acc_train:0.6267 pre_train:0.6625 recall_train:0.5656 F1_train:0.6102 AUC_train:0.6653\n",
      "acc_val:0.6300 pre_val:0.6383 recall_val:0.6000 F1_val:0.618557 AUC_val:0.6940\n",
      "Epoch:0017\n",
      "acc_train:0.5956 pre_train:0.6166 recall_train:0.5742 F1_train:0.5947 AUC_train:0.6337\n",
      "acc_val:0.6300 pre_val:0.6383 recall_val:0.6000 F1_val:0.618557 AUC_val:0.6988\n",
      "Epoch:0018\n",
      "acc_train:0.6067 pre_train:0.6344 recall_train:0.5634 F1_train:0.5968 AUC_train:0.6459\n",
      "acc_val:0.6300 pre_val:0.6383 recall_val:0.6000 F1_val:0.618557 AUC_val:0.7028\n",
      "Epoch:0019\n",
      "acc_train:0.6022 pre_train:0.6219 recall_train:0.5871 F1_train:0.6040 AUC_train:0.6514\n",
      "acc_val:0.6300 pre_val:0.6383 recall_val:0.6000 F1_val:0.618557 AUC_val:0.7092\n",
      "Epoch:0020\n",
      "acc_train:0.6044 pre_train:0.6295 recall_train:0.5699 F1_train:0.5982 AUC_train:0.6478\n",
      "acc_val:0.6300 pre_val:0.6383 recall_val:0.6000 F1_val:0.618557 AUC_val:0.7156\n",
      "Epoch:0021\n",
      "acc_train:0.5856 pre_train:0.5891 recall_train:0.6538 F1_train:0.6198 AUC_train:0.6524\n",
      "acc_val:0.6400 pre_val:0.6458 recall_val:0.6200 F1_val:0.632653 AUC_val:0.7248\n",
      "Epoch:0022\n",
      "acc_train:0.6111 pre_train:0.6340 recall_train:0.5849 F1_train:0.6085 AUC_train:0.6888\n",
      "acc_val:0.6400 pre_val:0.6458 recall_val:0.6200 F1_val:0.632653 AUC_val:0.7316\n",
      "Epoch:0023\n",
      "acc_train:0.6111 pre_train:0.6253 recall_train:0.6172 F1_train:0.6212 AUC_train:0.6535\n",
      "acc_val:0.6300 pre_val:0.6383 recall_val:0.6000 F1_val:0.618557 AUC_val:0.7364\n",
      "Epoch:0024\n",
      "acc_train:0.6189 pre_train:0.6439 recall_train:0.5871 F1_train:0.6142 AUC_train:0.6609\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.7432\n",
      "Epoch:0025\n",
      "acc_train:0.5956 pre_train:0.6223 recall_train:0.5527 F1_train:0.5854 AUC_train:0.6587\n",
      "acc_val:0.6500 pre_val:0.6744 recall_val:0.5800 F1_val:0.623656 AUC_val:0.7472\n",
      "Epoch:0026\n",
      "acc_train:0.5944 pre_train:0.5919 recall_train:0.6925 F1_train:0.6383 AUC_train:0.6821\n",
      "acc_val:0.6400 pre_val:0.6094 recall_val:0.7800 F1_val:0.684211 AUC_val:0.7408\n",
      "Epoch:0027\n",
      "acc_train:0.6067 pre_train:0.6078 recall_train:0.6731 F1_train:0.6388 AUC_train:0.6836\n",
      "acc_val:0.6600 pre_val:0.6290 recall_val:0.7800 F1_val:0.696429 AUC_val:0.7528\n",
      "Epoch:0028\n",
      "acc_train:0.6222 pre_train:0.6528 recall_train:0.5742 F1_train:0.6110 AUC_train:0.6771\n",
      "acc_val:0.6800 pre_val:0.6667 recall_val:0.7200 F1_val:0.692308 AUC_val:0.7644\n",
      "Epoch:0029\n",
      "acc_train:0.6156 pre_train:0.6448 recall_train:0.5699 F1_train:0.6050 AUC_train:0.6671\n",
      "acc_val:0.6600 pre_val:0.6600 recall_val:0.6600 F1_val:0.660000 AUC_val:0.7684\n",
      "Epoch:0030\n",
      "acc_train:0.6456 pre_train:0.7005 recall_train:0.5484 F1_train:0.6152 AUC_train:0.6752\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.7684\n",
      "Epoch:0031\n",
      "acc_train:0.6756 pre_train:0.7357 recall_train:0.5806 F1_train:0.6490 AUC_train:0.7191\n",
      "acc_val:0.6600 pre_val:0.7000 recall_val:0.5600 F1_val:0.622222 AUC_val:0.7676\n",
      "Epoch:0032\n",
      "acc_train:0.6244 pre_train:0.6599 recall_train:0.5634 F1_train:0.6079 AUC_train:0.7046\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7748\n",
      "Epoch:0033\n",
      "acc_train:0.6811 pre_train:0.7681 recall_train:0.5484 F1_train:0.6399 AUC_train:0.7286\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7784\n",
      "Epoch:0034\n",
      "acc_train:0.6856 pre_train:0.7298 recall_train:0.6215 F1_train:0.6713 AUC_train:0.7588\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7860\n",
      "Epoch:0035\n",
      "acc_train:0.6767 pre_train:0.7486 recall_train:0.5634 F1_train:0.6429 AUC_train:0.7482\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7892\n",
      "Epoch:0036\n",
      "acc_train:0.6900 pre_train:0.7555 recall_train:0.5914 F1_train:0.6634 AUC_train:0.7653\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7932\n",
      "Epoch:0037\n",
      "acc_train:0.6822 pre_train:0.7579 recall_train:0.5656 F1_train:0.6478 AUC_train:0.7538\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.7928\n",
      "Epoch:0038\n",
      "acc_train:0.6800 pre_train:0.7411 recall_train:0.5849 F1_train:0.6538 AUC_train:0.7887\n",
      "acc_val:0.6900 pre_val:0.7317 recall_val:0.6000 F1_val:0.659341 AUC_val:0.7968\n",
      "Epoch:0039\n",
      "acc_train:0.6889 pre_train:0.7697 recall_train:0.5677 F1_train:0.6535 AUC_train:0.7819\n",
      "acc_val:0.7000 pre_val:0.7273 recall_val:0.6400 F1_val:0.680851 AUC_val:0.8048\n",
      "Epoch:0040\n",
      "acc_train:0.7156 pre_train:0.7580 recall_train:0.6602 F1_train:0.7057 AUC_train:0.8147\n",
      "acc_val:0.7500 pre_val:0.7451 recall_val:0.7600 F1_val:0.752475 AUC_val:0.8132\n",
      "Epoch:0041\n",
      "acc_train:0.7133 pre_train:0.7702 recall_train:0.6344 F1_train:0.6958 AUC_train:0.8121\n",
      "acc_val:0.7700 pre_val:0.7647 recall_val:0.7800 F1_val:0.772277 AUC_val:0.8168\n",
      "Epoch:0042\n",
      "acc_train:0.7067 pre_train:0.7947 recall_train:0.5828 F1_train:0.6725 AUC_train:0.8343\n",
      "acc_val:0.7600 pre_val:0.7500 recall_val:0.7800 F1_val:0.764706 AUC_val:0.8376\n",
      "Epoch:0043\n",
      "acc_train:0.7578 pre_train:0.8329 recall_train:0.6645 F1_train:0.7392 AUC_train:0.8450\n",
      "acc_val:0.7600 pre_val:0.7321 recall_val:0.8200 F1_val:0.773585 AUC_val:0.8484\n",
      "Epoch:0044\n",
      "acc_train:0.7889 pre_train:0.8235 recall_train:0.7527 F1_train:0.7865 AUC_train:0.8773\n",
      "acc_val:0.7900 pre_val:0.7636 recall_val:0.8400 F1_val:0.800000 AUC_val:0.8564\n",
      "Epoch:0045\n",
      "acc_train:0.7889 pre_train:0.7723 recall_train:0.8387 F1_train:0.8041 AUC_train:0.8740\n",
      "acc_val:0.8100 pre_val:0.7818 recall_val:0.8600 F1_val:0.819048 AUC_val:0.8692\n",
      "Epoch:0046\n",
      "acc_train:0.8178 pre_train:0.8004 recall_train:0.8624 F1_train:0.8302 AUC_train:0.8903\n",
      "acc_val:0.8000 pre_val:0.7679 recall_val:0.8600 F1_val:0.811321 AUC_val:0.8820\n",
      "Epoch:0047\n",
      "acc_train:0.8322 pre_train:0.8165 recall_train:0.8710 F1_train:0.8429 AUC_train:0.8907\n",
      "acc_val:0.8000 pre_val:0.7586 recall_val:0.8800 F1_val:0.814815 AUC_val:0.8740\n",
      "Epoch:0048\n",
      "acc_train:0.8311 pre_train:0.8254 recall_train:0.8538 F1_train:0.8393 AUC_train:0.9032\n",
      "acc_val:0.8000 pre_val:0.7500 recall_val:0.9000 F1_val:0.818182 AUC_val:0.8768\n",
      "Epoch:0049\n",
      "acc_train:0.8200 pre_train:0.7965 recall_train:0.8753 F1_train:0.8340 AUC_train:0.9021\n",
      "acc_val:0.7800 pre_val:0.7258 recall_val:0.9000 F1_val:0.803571 AUC_val:0.8732\n",
      "Epoch:0050\n",
      "acc_train:0.8200 pre_train:0.7821 recall_train:0.9032 F1_train:0.8383 AUC_train:0.9054\n",
      "acc_val:0.7700 pre_val:0.7143 recall_val:0.9000 F1_val:0.796460 AUC_val:0.8780\n",
      "Epoch:0051\n",
      "acc_train:0.8156 pre_train:0.7805 recall_train:0.8946 F1_train:0.8337 AUC_train:0.9112\n",
      "acc_val:0.7800 pre_val:0.7414 recall_val:0.8600 F1_val:0.796296 AUC_val:0.8748\n",
      "Epoch:0052\n",
      "acc_train:0.8256 pre_train:0.7917 recall_train:0.8989 F1_train:0.8419 AUC_train:0.9164\n",
      "acc_val:0.8000 pre_val:0.7679 recall_val:0.8600 F1_val:0.811321 AUC_val:0.8784\n",
      "Epoch:0053\n",
      "acc_train:0.8356 pre_train:0.7952 recall_train:0.9183 F1_train:0.8523 AUC_train:0.9191\n",
      "acc_val:0.8100 pre_val:0.7818 recall_val:0.8600 F1_val:0.819048 AUC_val:0.8748\n",
      "Epoch:0054\n",
      "acc_train:0.8189 pre_train:0.7871 recall_train:0.8903 F1_train:0.8355 AUC_train:0.9204\n",
      "acc_val:0.7700 pre_val:0.7455 recall_val:0.8200 F1_val:0.780952 AUC_val:0.8636\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0055\n",
      "acc_train:0.8133 pre_train:0.7895 recall_train:0.8710 F1_train:0.8282 AUC_train:0.9094\n",
      "acc_val:0.7900 pre_val:0.7736 recall_val:0.8200 F1_val:0.796117 AUC_val:0.8652\n",
      "Epoch:0056\n",
      "acc_train:0.8289 pre_train:0.7973 recall_train:0.8968 F1_train:0.8441 AUC_train:0.9178\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8724\n",
      "Epoch:0057\n",
      "acc_train:0.8389 pre_train:0.8125 recall_train:0.8946 F1_train:0.8516 AUC_train:0.9053\n",
      "acc_val:0.8300 pre_val:0.8235 recall_val:0.8400 F1_val:0.831683 AUC_val:0.8764\n",
      "Epoch:0058\n",
      "acc_train:0.8567 pre_train:0.8360 recall_train:0.8989 F1_train:0.8663 AUC_train:0.9311\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8768\n",
      "Epoch:0059\n",
      "acc_train:0.8656 pre_train:0.8413 recall_train:0.9118 F1_train:0.8751 AUC_train:0.9298\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8764\n",
      "Epoch:0060\n",
      "acc_train:0.8589 pre_train:0.8353 recall_train:0.9054 F1_train:0.8689 AUC_train:0.9243\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8776\n",
      "Epoch:0061\n",
      "acc_train:0.8844 pre_train:0.8588 recall_train:0.9290 F1_train:0.8926 AUC_train:0.9486\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8804\n",
      "Epoch:0062\n",
      "acc_train:0.8800 pre_train:0.8480 recall_train:0.9355 F1_train:0.8896 AUC_train:0.9436\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8812\n",
      "Epoch:0063\n",
      "acc_train:0.8867 pre_train:0.8511 recall_train:0.9462 F1_train:0.8961 AUC_train:0.9487\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8948\n",
      "Epoch:0064\n",
      "acc_train:0.8822 pre_train:0.8472 recall_train:0.9419 F1_train:0.8921 AUC_train:0.9487\n",
      "acc_val:0.8100 pre_val:0.7925 recall_val:0.8400 F1_val:0.815534 AUC_val:0.9040\n",
      "Epoch:0065\n",
      "acc_train:0.8767 pre_train:0.8391 recall_train:0.9419 F1_train:0.8875 AUC_train:0.9500\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.9072\n",
      "Epoch:0066\n",
      "acc_train:0.8800 pre_train:0.8439 recall_train:0.9419 F1_train:0.8902 AUC_train:0.9609\n",
      "acc_val:0.8100 pre_val:0.7818 recall_val:0.8600 F1_val:0.819048 AUC_val:0.9128\n",
      "Epoch:0067\n",
      "acc_train:0.8989 pre_train:0.8696 recall_train:0.9462 F1_train:0.9063 AUC_train:0.9565\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9132\n",
      "Epoch:0068\n",
      "acc_train:0.9111 pre_train:0.8812 recall_train:0.9570 F1_train:0.9175 AUC_train:0.9633\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9132\n",
      "Epoch:0069\n",
      "acc_train:0.8922 pre_train:0.8710 recall_train:0.9290 F1_train:0.8991 AUC_train:0.9568\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9204\n",
      "Epoch:0070\n",
      "acc_train:0.8978 pre_train:0.8679 recall_train:0.9462 F1_train:0.9053 AUC_train:0.9623\n",
      "acc_val:0.8400 pre_val:0.8148 recall_val:0.8800 F1_val:0.846154 AUC_val:0.9228\n",
      "Epoch:0071\n",
      "acc_train:0.9222 pre_train:0.9175 recall_train:0.9333 F1_train:0.9254 AUC_train:0.9701\n",
      "acc_val:0.8400 pre_val:0.8269 recall_val:0.8600 F1_val:0.843137 AUC_val:0.9288\n",
      "Epoch:0072\n",
      "acc_train:0.9078 pre_train:0.9099 recall_train:0.9118 F1_train:0.9108 AUC_train:0.9645\n",
      "acc_val:0.8400 pre_val:0.8148 recall_val:0.8800 F1_val:0.846154 AUC_val:0.9212\n",
      "Epoch:0073\n",
      "acc_train:0.9044 pre_train:0.8813 recall_train:0.9419 F1_train:0.9106 AUC_train:0.9657\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9116\n",
      "Epoch:0074\n",
      "acc_train:0.9000 pre_train:0.8728 recall_train:0.9441 F1_train:0.9070 AUC_train:0.9577\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9160\n",
      "Epoch:0075\n",
      "acc_train:0.9044 pre_train:0.8844 recall_train:0.9376 F1_train:0.9102 AUC_train:0.9647\n",
      "acc_val:0.8300 pre_val:0.8113 recall_val:0.8600 F1_val:0.834951 AUC_val:0.9212\n",
      "Epoch:0076\n",
      "acc_train:0.9200 pre_train:0.9068 recall_train:0.9419 F1_train:0.9241 AUC_train:0.9706\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9216\n",
      "Epoch:0077\n",
      "acc_train:0.9178 pre_train:0.8966 recall_train:0.9505 F1_train:0.9228 AUC_train:0.9730\n",
      "acc_val:0.8200 pre_val:0.7963 recall_val:0.8600 F1_val:0.826923 AUC_val:0.9168\n",
      "Epoch:0078\n",
      "acc_train:0.9356 pre_train:0.9078 recall_train:0.9742 F1_train:0.9398 AUC_train:0.9748\n",
      "acc_val:0.8200 pre_val:0.7857 recall_val:0.8800 F1_val:0.830189 AUC_val:0.9108\n",
      "Epoch:0079\n",
      "acc_train:0.9278 pre_train:0.9016 recall_train:0.9656 F1_train:0.9325 AUC_train:0.9679\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9130\n",
      "Epoch:0080\n",
      "acc_train:0.9300 pre_train:0.9069 recall_train:0.9634 F1_train:0.9343 AUC_train:0.9731\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9122\n",
      "Epoch:0081\n",
      "acc_train:0.9356 pre_train:0.9179 recall_train:0.9613 F1_train:0.9391 AUC_train:0.9729\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9150\n",
      "Epoch:0082\n",
      "acc_train:0.9233 pre_train:0.8976 recall_train:0.9613 F1_train:0.9283 AUC_train:0.9741\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9160\n",
      "Epoch:0083\n",
      "acc_train:0.9278 pre_train:0.9016 recall_train:0.9656 F1_train:0.9325 AUC_train:0.9831\n",
      "acc_val:0.8500 pre_val:0.8070 recall_val:0.9200 F1_val:0.859813 AUC_val:0.9108\n",
      "Epoch:0084\n",
      "acc_train:0.9356 pre_train:0.9145 recall_train:0.9656 F1_train:0.9393 AUC_train:0.9746\n",
      "acc_val:0.8500 pre_val:0.8070 recall_val:0.9200 F1_val:0.859813 AUC_val:0.9134\n",
      "Epoch:0085\n",
      "acc_train:0.9311 pre_train:0.9138 recall_train:0.9570 F1_train:0.9349 AUC_train:0.9832\n",
      "acc_val:0.8500 pre_val:0.8070 recall_val:0.9200 F1_val:0.859813 AUC_val:0.9146\n",
      "Epoch:0086\n",
      "acc_train:0.9367 pre_train:0.9232 recall_train:0.9570 F1_train:0.9398 AUC_train:0.9873\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9136\n",
      "Epoch:0087\n",
      "acc_train:0.9411 pre_train:0.9204 recall_train:0.9699 F1_train:0.9445 AUC_train:0.9864\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9122\n",
      "Epoch:0088\n",
      "acc_train:0.9400 pre_train:0.9202 recall_train:0.9677 F1_train:0.9434 AUC_train:0.9837\n",
      "acc_val:0.8600 pre_val:0.8333 recall_val:0.9000 F1_val:0.865385 AUC_val:0.9192\n",
      "Epoch:0089\n",
      "acc_train:0.9400 pre_train:0.9118 recall_train:0.9785 F1_train:0.9440 AUC_train:0.9880\n",
      "acc_val:0.8700 pre_val:0.8364 recall_val:0.9200 F1_val:0.876190 AUC_val:0.9182\n",
      "Epoch:0090\n",
      "acc_train:0.9433 pre_train:0.9207 recall_train:0.9742 F1_train:0.9467 AUC_train:0.9794\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9162\n",
      "Epoch:0091\n",
      "acc_train:0.9533 pre_train:0.9434 recall_train:0.9677 F1_train:0.9554 AUC_train:0.9893\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9166\n",
      "Epoch:0092\n",
      "acc_train:0.9533 pre_train:0.9434 recall_train:0.9677 F1_train:0.9554 AUC_train:0.9917\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9142\n",
      "Epoch:0093\n",
      "acc_train:0.9433 pre_train:0.9190 recall_train:0.9763 F1_train:0.9468 AUC_train:0.9832\n",
      "acc_val:0.8500 pre_val:0.7966 recall_val:0.9400 F1_val:0.862385 AUC_val:0.9148\n",
      "Epoch:0094\n",
      "acc_train:0.9522 pre_train:0.9414 recall_train:0.9677 F1_train:0.9544 AUC_train:0.9884\n",
      "acc_val:0.8500 pre_val:0.7869 recall_val:0.9600 F1_val:0.864865 AUC_val:0.9148\n",
      "Epoch:0095\n",
      "acc_train:0.9611 pre_train:0.9479 recall_train:0.9785 F1_train:0.9630 AUC_train:0.9921\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9128\n",
      "Epoch:0096\n",
      "acc_train:0.9533 pre_train:0.9290 recall_train:0.9849 F1_train:0.9562 AUC_train:0.9866\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9148\n",
      "Epoch:0097\n",
      "acc_train:0.9478 pre_train:0.9197 recall_train:0.9849 F1_train:0.9512 AUC_train:0.9874\n",
      "acc_val:0.8300 pre_val:0.7705 recall_val:0.9400 F1_val:0.846847 AUC_val:0.9162\n",
      "Epoch:0098\n",
      "acc_train:0.9556 pre_train:0.9493 recall_train:0.9656 F1_train:0.9574 AUC_train:0.9900\n",
      "acc_val:0.8400 pre_val:0.7833 recall_val:0.9400 F1_val:0.854545 AUC_val:0.9168\n",
      "Epoch:0099\n",
      "acc_train:0.9622 pre_train:0.9518 recall_train:0.9763 F1_train:0.9639 AUC_train:0.9885\n",
      "acc_val:0.8500 pre_val:0.7966 recall_val:0.9400 F1_val:0.862385 AUC_val:0.9168\n",
      "Epoch:0100\n",
      "acc_train:0.9644 pre_train:0.9539 recall_train:0.9785 F1_train:0.9660 AUC_train:0.9937\n",
      "acc_val:0.8600 pre_val:0.8103 recall_val:0.9400 F1_val:0.870370 AUC_val:0.9168\n",
      "Epoch:0101\n",
      "acc_train:0.9567 pre_train:0.9551 recall_train:0.9613 F1_train:0.9582 AUC_train:0.9867\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9212\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0102\n",
      "acc_train:0.9567 pre_train:0.9437 recall_train:0.9742 F1_train:0.9587 AUC_train:0.9927\n",
      "acc_val:0.8800 pre_val:0.8519 recall_val:0.9200 F1_val:0.884615 AUC_val:0.9138\n",
      "Epoch:0103\n",
      "acc_train:0.9700 pre_train:0.9601 recall_train:0.9828 F1_train:0.9713 AUC_train:0.9926\n",
      "acc_val:0.8900 pre_val:0.8679 recall_val:0.9200 F1_val:0.893204 AUC_val:0.9130\n",
      "Epoch:0104\n",
      "acc_train:0.9644 pre_train:0.9558 recall_train:0.9763 F1_train:0.9660 AUC_train:0.9928\n",
      "acc_val:0.8900 pre_val:0.8679 recall_val:0.9200 F1_val:0.893204 AUC_val:0.9122\n",
      "Epoch:0105\n",
      "acc_train:0.9689 pre_train:0.9719 recall_train:0.9677 F1_train:0.9698 AUC_train:0.9909\n",
      "acc_val:0.8900 pre_val:0.8545 recall_val:0.9400 F1_val:0.895238 AUC_val:0.9126\n",
      "Epoch:0106\n",
      "acc_train:0.9644 pre_train:0.9558 recall_train:0.9763 F1_train:0.9660 AUC_train:0.9905\n",
      "acc_val:0.8700 pre_val:0.8246 recall_val:0.9400 F1_val:0.878505 AUC_val:0.9196\n",
      "Epoch:0107\n",
      "acc_train:0.9656 pre_train:0.9578 recall_train:0.9763 F1_train:0.9670 AUC_train:0.9909\n",
      "acc_val:0.8500 pre_val:0.7966 recall_val:0.9400 F1_val:0.862385 AUC_val:0.9208\n",
      "Epoch:0108\n",
      "acc_train:0.9544 pre_train:0.9511 recall_train:0.9613 F1_train:0.9561 AUC_train:0.9861\n",
      "acc_val:0.8500 pre_val:0.7966 recall_val:0.9400 F1_val:0.862385 AUC_val:0.9216\n",
      "Epoch:0109\n",
      "acc_train:0.9589 pre_train:0.9553 recall_train:0.9656 F1_train:0.9604 AUC_train:0.9866\n",
      "acc_val:0.8300 pre_val:0.7705 recall_val:0.9400 F1_val:0.846847 AUC_val:0.9242\n",
      "Epoch:0110\n",
      "acc_train:0.9678 pre_train:0.9561 recall_train:0.9828 F1_train:0.9692 AUC_train:0.9949\n",
      "acc_val:0.8300 pre_val:0.7705 recall_val:0.9400 F1_val:0.846847 AUC_val:0.9252\n",
      "Epoch:0111\n",
      "acc_train:0.9689 pre_train:0.9619 recall_train:0.9785 F1_train:0.9701 AUC_train:0.9922\n",
      "acc_val:0.8200 pre_val:0.7581 recall_val:0.9400 F1_val:0.839286 AUC_val:0.9248\n",
      "Epoch:0112\n",
      "acc_train:0.9600 pre_train:0.9459 recall_train:0.9785 F1_train:0.9619 AUC_train:0.9941\n",
      "acc_val:0.8200 pre_val:0.7500 recall_val:0.9600 F1_val:0.842105 AUC_val:0.9268\n",
      "Epoch:0113\n",
      "acc_train:0.9689 pre_train:0.9505 recall_train:0.9914 F1_train:0.9705 AUC_train:0.9939\n",
      "acc_val:0.8300 pre_val:0.7619 recall_val:0.9600 F1_val:0.849558 AUC_val:0.9268\n",
      "Epoch:0114\n",
      "acc_train:0.9589 pre_train:0.9385 recall_train:0.9849 F1_train:0.9612 AUC_train:0.9915\n",
      "acc_val:0.8300 pre_val:0.7619 recall_val:0.9600 F1_val:0.849558 AUC_val:0.9288\n",
      "Epoch:0115\n",
      "acc_train:0.9767 pre_train:0.9625 recall_train:0.9935 F1_train:0.9778 AUC_train:0.9966\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9320\n",
      "Epoch:0116\n",
      "acc_train:0.9589 pre_train:0.9367 recall_train:0.9871 F1_train:0.9613 AUC_train:0.9926\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9312\n",
      "Epoch:0117\n",
      "acc_train:0.9644 pre_train:0.9501 recall_train:0.9828 F1_train:0.9662 AUC_train:0.9929\n",
      "acc_val:0.8300 pre_val:0.7619 recall_val:0.9600 F1_val:0.849558 AUC_val:0.9270\n",
      "Epoch:0118\n",
      "acc_train:0.9733 pre_train:0.9682 recall_train:0.9806 F1_train:0.9744 AUC_train:0.9914\n",
      "acc_val:0.8300 pre_val:0.7619 recall_val:0.9600 F1_val:0.849558 AUC_val:0.9220\n",
      "Epoch:0119\n",
      "acc_train:0.9800 pre_train:0.9745 recall_train:0.9871 F1_train:0.9808 AUC_train:0.9967\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9196\n",
      "Epoch:0120\n",
      "acc_train:0.9789 pre_train:0.9765 recall_train:0.9828 F1_train:0.9796 AUC_train:0.9947\n",
      "acc_val:0.8200 pre_val:0.7500 recall_val:0.9600 F1_val:0.842105 AUC_val:0.9218\n",
      "Epoch:0121\n",
      "acc_train:0.9678 pre_train:0.9504 recall_train:0.9892 F1_train:0.9694 AUC_train:0.9968\n",
      "acc_val:0.8200 pre_val:0.7500 recall_val:0.9600 F1_val:0.842105 AUC_val:0.9184\n",
      "Epoch:0122\n",
      "acc_train:0.9711 pre_train:0.9563 recall_train:0.9892 F1_train:0.9725 AUC_train:0.9963\n",
      "acc_val:0.8200 pre_val:0.7500 recall_val:0.9600 F1_val:0.842105 AUC_val:0.9200\n",
      "Epoch:0123\n",
      "acc_train:0.9778 pre_train:0.9626 recall_train:0.9957 F1_train:0.9789 AUC_train:0.9964\n",
      "acc_val:0.8100 pre_val:0.7385 recall_val:0.9600 F1_val:0.834783 AUC_val:0.9186\n",
      "Early Stopping!!! epoch：122\n",
      " Starting the 1-2 Fold:：\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.4789 pre_train:0.4938 recall_train:0.3441 F1_train:0.4056 AUC_train:0.4362\n",
      "acc_val:0.4900 pre_val:0.4935 recall_val:0.7600 F1_val:0.598425 AUC_val:0.4692\n",
      "Epoch:0002\n",
      "acc_train:0.4722 pre_train:0.4848 recall_train:0.3419 F1_train:0.4010 AUC_train:0.4584\n",
      "acc_val:0.4900 pre_val:0.4941 recall_val:0.8400 F1_val:0.622222 AUC_val:0.4860\n",
      "Epoch:0003\n",
      "acc_train:0.5522 pre_train:0.6006 recall_train:0.3978 F1_train:0.4787 AUC_train:0.5645\n",
      "acc_val:0.5800 pre_val:0.5465 recall_val:0.9400 F1_val:0.691176 AUC_val:0.5720\n",
      "Epoch:0004\n",
      "acc_train:0.5589 pre_train:0.6441 recall_train:0.3269 F1_train:0.4337 AUC_train:0.5852\n",
      "acc_val:0.5500 pre_val:0.5301 recall_val:0.8800 F1_val:0.661654 AUC_val:0.5968\n",
      "Epoch:0005\n",
      "acc_train:0.5722 pre_train:0.6709 recall_train:0.3376 F1_train:0.4492 AUC_train:0.5949\n",
      "acc_val:0.5500 pre_val:0.5352 recall_val:0.7600 F1_val:0.628099 AUC_val:0.5988\n",
      "Epoch:0006\n",
      "acc_train:0.5656 pre_train:0.6504 recall_train:0.3441 F1_train:0.4501 AUC_train:0.5995\n",
      "acc_val:0.5700 pre_val:0.5538 recall_val:0.7200 F1_val:0.626087 AUC_val:0.5936\n",
      "Epoch:0007\n",
      "acc_train:0.5811 pre_train:0.6341 recall_train:0.4473 F1_train:0.5246 AUC_train:0.5752\n",
      "acc_val:0.5200 pre_val:0.5122 recall_val:0.8400 F1_val:0.636364 AUC_val:0.6000\n",
      "Epoch:0008\n",
      "acc_train:0.6111 pre_train:0.6898 recall_train:0.4495 F1_train:0.5443 AUC_train:0.6212\n",
      "acc_val:0.5400 pre_val:0.5263 recall_val:0.8000 F1_val:0.634921 AUC_val:0.6120\n",
      "Epoch:0009\n",
      "acc_train:0.5656 pre_train:0.6381 recall_train:0.3677 F1_train:0.4666 AUC_train:0.6103\n",
      "acc_val:0.5400 pre_val:0.5286 recall_val:0.7400 F1_val:0.616667 AUC_val:0.6248\n",
      "Epoch:0010\n",
      "acc_train:0.5667 pre_train:0.6147 recall_train:0.4323 F1_train:0.5076 AUC_train:0.5637\n",
      "acc_val:0.5700 pre_val:0.5507 recall_val:0.7600 F1_val:0.638655 AUC_val:0.6388\n",
      "Epoch:0011\n",
      "acc_train:0.6100 pre_train:0.6717 recall_train:0.4796 F1_train:0.5596 AUC_train:0.6021\n",
      "acc_val:0.6100 pre_val:0.5797 recall_val:0.8000 F1_val:0.672269 AUC_val:0.6616\n",
      "Epoch:0012\n",
      "acc_train:0.5933 pre_train:0.6634 recall_train:0.4323 F1_train:0.5234 AUC_train:0.6060\n",
      "acc_val:0.7100 pre_val:0.6721 recall_val:0.8200 F1_val:0.738739 AUC_val:0.6822\n",
      "Epoch:0013\n",
      "acc_train:0.5944 pre_train:0.6592 recall_train:0.4452 F1_train:0.5315 AUC_train:0.6359\n",
      "acc_val:0.6800 pre_val:0.6607 recall_val:0.7400 F1_val:0.698113 AUC_val:0.6624\n",
      "Epoch:0014\n",
      "acc_train:0.6144 pre_train:0.6735 recall_train:0.4925 F1_train:0.5689 AUC_train:0.6220\n",
      "acc_val:0.6700 pre_val:0.6545 recall_val:0.7200 F1_val:0.685714 AUC_val:0.6668\n",
      "Epoch:0015\n",
      "acc_train:0.6089 pre_train:0.6677 recall_train:0.4839 F1_train:0.5611 AUC_train:0.6146\n",
      "acc_val:0.6900 pre_val:0.6792 recall_val:0.7200 F1_val:0.699029 AUC_val:0.6756\n",
      "Epoch:0016\n",
      "acc_train:0.6278 pre_train:0.6738 recall_train:0.5419 F1_train:0.6007 AUC_train:0.6499\n",
      "acc_val:0.7000 pre_val:0.7000 recall_val:0.7000 F1_val:0.700000 AUC_val:0.6824\n",
      "Epoch:0017\n",
      "acc_train:0.4911 pre_train:0.5055 recall_train:0.6903 F1_train:0.5836 AUC_train:0.5299\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.9600 F1_val:0.657534 AUC_val:0.6964\n",
      "Epoch:0018\n",
      "acc_train:0.6000 pre_train:0.6241 recall_train:0.5677 F1_train:0.5946 AUC_train:0.6200\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.9600 F1_val:0.657534 AUC_val:0.6936\n",
      "Epoch:0019\n",
      "acc_train:0.6089 pre_train:0.6523 recall_train:0.5204 F1_train:0.5789 AUC_train:0.6336\n",
      "acc_val:0.5500 pre_val:0.5294 recall_val:0.9000 F1_val:0.666667 AUC_val:0.7016\n",
      "Epoch:0020\n",
      "acc_train:0.6322 pre_train:0.6727 recall_train:0.5613 F1_train:0.6120 AUC_train:0.6459\n",
      "acc_val:0.6100 pre_val:0.5753 recall_val:0.8400 F1_val:0.682927 AUC_val:0.7080\n",
      "Epoch:0021\n",
      "acc_train:0.5922 pre_train:0.6283 recall_train:0.5161 F1_train:0.5667 AUC_train:0.6086\n",
      "acc_val:0.6500 pre_val:0.6190 recall_val:0.7800 F1_val:0.690265 AUC_val:0.7096\n",
      "Epoch:0022\n",
      "acc_train:0.6144 pre_train:0.6667 recall_train:0.5075 F1_train:0.5763 AUC_train:0.6441\n",
      "acc_val:0.6900 pre_val:0.6727 recall_val:0.7400 F1_val:0.704762 AUC_val:0.7128\n",
      "Epoch:0023\n",
      "acc_train:0.6278 pre_train:0.6890 recall_train:0.5097 F1_train:0.5859 AUC_train:0.6630\n",
      "acc_val:0.7100 pre_val:0.7234 recall_val:0.6800 F1_val:0.701031 AUC_val:0.7124\n",
      "Epoch:0024\n",
      "acc_train:0.5867 pre_train:0.6131 recall_train:0.5419 F1_train:0.5753 AUC_train:0.6202\n",
      "acc_val:0.7200 pre_val:0.7200 recall_val:0.7200 F1_val:0.720000 AUC_val:0.7076\n",
      "Epoch:0025\n",
      "acc_train:0.6133 pre_train:0.6512 recall_train:0.5419 F1_train:0.5915 AUC_train:0.6465\n",
      "acc_val:0.7000 pre_val:0.7174 recall_val:0.6600 F1_val:0.687500 AUC_val:0.7080\n",
      "Epoch:0026\n",
      "acc_train:0.6422 pre_train:0.6897 recall_train:0.5591 F1_train:0.6176 AUC_train:0.6601\n",
      "acc_val:0.7200 pre_val:0.7619 recall_val:0.6400 F1_val:0.695652 AUC_val:0.7116\n",
      "Epoch:0027\n",
      "acc_train:0.6278 pre_train:0.6617 recall_train:0.5720 F1_train:0.6136 AUC_train:0.6631\n",
      "acc_val:0.7300 pre_val:0.7805 recall_val:0.6400 F1_val:0.703297 AUC_val:0.7180\n",
      "Epoch:0028\n",
      "acc_train:0.5367 pre_train:0.5392 recall_train:0.7097 F1_train:0.6128 AUC_train:0.6130\n",
      "acc_val:0.7300 pre_val:0.7347 recall_val:0.7200 F1_val:0.727273 AUC_val:0.7200\n",
      "Epoch:0029\n",
      "acc_train:0.6222 pre_train:0.6822 recall_train:0.5032 F1_train:0.5792 AUC_train:0.6525\n",
      "acc_val:0.7200 pre_val:0.7619 recall_val:0.6400 F1_val:0.695652 AUC_val:0.7176\n",
      "Epoch:0030\n",
      "acc_train:0.6422 pre_train:0.7097 recall_train:0.5204 F1_train:0.6005 AUC_train:0.6815\n",
      "acc_val:0.7200 pre_val:0.7750 recall_val:0.6200 F1_val:0.688889 AUC_val:0.7112\n",
      "Epoch:0031\n",
      "acc_train:0.6489 pre_train:0.7185 recall_train:0.5269 F1_train:0.6079 AUC_train:0.6944\n",
      "acc_val:0.7000 pre_val:0.7500 recall_val:0.6000 F1_val:0.666667 AUC_val:0.7012\n",
      "Epoch:0032\n",
      "acc_train:0.6500 pre_train:0.7273 recall_train:0.5161 F1_train:0.6038 AUC_train:0.6996\n",
      "acc_val:0.6500 pre_val:0.7143 recall_val:0.5000 F1_val:0.588235 AUC_val:0.7020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0033\n",
      "acc_train:0.6344 pre_train:0.7099 recall_train:0.4946 F1_train:0.5830 AUC_train:0.7013\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.6988\n",
      "Epoch:0034\n",
      "acc_train:0.6411 pre_train:0.6994 recall_train:0.5355 F1_train:0.6066 AUC_train:0.6830\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7024\n",
      "Epoch:0035\n",
      "acc_train:0.6589 pre_train:0.7351 recall_train:0.5312 F1_train:0.6167 AUC_train:0.7046\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7084\n",
      "Epoch:0036\n",
      "acc_train:0.6267 pre_train:0.6757 recall_train:0.5333 F1_train:0.5962 AUC_train:0.6676\n",
      "acc_val:0.6900 pre_val:0.7714 recall_val:0.5400 F1_val:0.635294 AUC_val:0.7236\n",
      "Epoch:0037\n",
      "acc_train:0.6522 pre_train:0.7123 recall_train:0.5484 F1_train:0.6197 AUC_train:0.6840\n",
      "acc_val:0.6900 pre_val:0.7879 recall_val:0.5200 F1_val:0.626506 AUC_val:0.7452\n",
      "Epoch:0038\n",
      "acc_train:0.6689 pre_train:0.7420 recall_train:0.5505 F1_train:0.6321 AUC_train:0.7254\n",
      "acc_val:0.6800 pre_val:0.7812 recall_val:0.5000 F1_val:0.609756 AUC_val:0.7512\n",
      "Epoch:0039\n",
      "acc_train:0.6456 pre_train:0.6973 recall_train:0.5548 F1_train:0.6180 AUC_train:0.6748\n",
      "acc_val:0.7000 pre_val:0.8125 recall_val:0.5200 F1_val:0.634146 AUC_val:0.7344\n",
      "Epoch:0040\n",
      "acc_train:0.6422 pre_train:0.6917 recall_train:0.5548 F1_train:0.6158 AUC_train:0.6917\n",
      "acc_val:0.7000 pre_val:0.7632 recall_val:0.5800 F1_val:0.659091 AUC_val:0.7312\n",
      "Epoch:0041\n",
      "acc_train:0.6722 pre_train:0.7401 recall_train:0.5634 F1_train:0.6398 AUC_train:0.7317\n",
      "acc_val:0.7100 pre_val:0.7838 recall_val:0.5800 F1_val:0.666667 AUC_val:0.7352\n",
      "Epoch:0042\n",
      "acc_train:0.6267 pre_train:0.6650 recall_train:0.5591 F1_train:0.6075 AUC_train:0.6837\n",
      "acc_val:0.7300 pre_val:0.8286 recall_val:0.5800 F1_val:0.682353 AUC_val:0.7428\n",
      "Epoch:0043\n",
      "acc_train:0.6756 pre_train:0.7423 recall_train:0.5699 F1_train:0.6448 AUC_train:0.7301\n",
      "acc_val:0.7300 pre_val:0.8286 recall_val:0.5800 F1_val:0.682353 AUC_val:0.7460\n",
      "Epoch:0044\n",
      "acc_train:0.6544 pre_train:0.7037 recall_train:0.5720 F1_train:0.6311 AUC_train:0.7154\n",
      "acc_val:0.7200 pre_val:0.8056 recall_val:0.5800 F1_val:0.674419 AUC_val:0.7516\n",
      "Epoch:0045\n",
      "acc_train:0.6656 pre_train:0.7092 recall_train:0.5978 F1_train:0.6488 AUC_train:0.7035\n",
      "acc_val:0.7400 pre_val:0.8333 recall_val:0.6000 F1_val:0.697674 AUC_val:0.7580\n",
      "Epoch:0046\n",
      "acc_train:0.6589 pre_train:0.7194 recall_train:0.5570 F1_train:0.6279 AUC_train:0.7274\n",
      "acc_val:0.7300 pre_val:0.8108 recall_val:0.6000 F1_val:0.689655 AUC_val:0.7644\n",
      "Epoch:0047\n",
      "acc_train:0.6978 pre_train:0.7587 recall_train:0.6086 F1_train:0.6754 AUC_train:0.7669\n",
      "acc_val:0.7300 pre_val:0.8108 recall_val:0.6000 F1_val:0.689655 AUC_val:0.7696\n",
      "Epoch:0048\n",
      "acc_train:0.6711 pre_train:0.7380 recall_train:0.5634 F1_train:0.6390 AUC_train:0.7500\n",
      "acc_val:0.7200 pre_val:0.8056 recall_val:0.5800 F1_val:0.674419 AUC_val:0.7700\n",
      "Epoch:0049\n",
      "acc_train:0.7333 pre_train:0.7834 recall_train:0.6688 F1_train:0.7216 AUC_train:0.7939\n",
      "acc_val:0.7300 pre_val:0.8108 recall_val:0.6000 F1_val:0.689655 AUC_val:0.7740\n",
      "Epoch:0050\n",
      "acc_train:0.7156 pre_train:0.7960 recall_train:0.6043 F1_train:0.6870 AUC_train:0.8012\n",
      "acc_val:0.7300 pre_val:0.8108 recall_val:0.6000 F1_val:0.689655 AUC_val:0.7804\n",
      "Epoch:0051\n",
      "acc_train:0.7000 pre_train:0.7778 recall_train:0.5871 F1_train:0.6691 AUC_train:0.7886\n",
      "acc_val:0.7400 pre_val:0.8333 recall_val:0.6000 F1_val:0.697674 AUC_val:0.7944\n",
      "Epoch:0052\n",
      "acc_train:0.7244 pre_train:0.8163 recall_train:0.6022 F1_train:0.6931 AUC_train:0.8086\n",
      "acc_val:0.7300 pre_val:0.8286 recall_val:0.5800 F1_val:0.682353 AUC_val:0.8052\n",
      "Epoch:0053\n",
      "acc_train:0.7378 pre_train:0.8207 recall_train:0.6301 F1_train:0.7129 AUC_train:0.8281\n",
      "acc_val:0.7300 pre_val:0.8286 recall_val:0.5800 F1_val:0.682353 AUC_val:0.8072\n",
      "Epoch:0054\n",
      "acc_train:0.7333 pre_train:0.8134 recall_train:0.6280 F1_train:0.7087 AUC_train:0.8298\n",
      "acc_val:0.7500 pre_val:0.8571 recall_val:0.6000 F1_val:0.705882 AUC_val:0.8160\n",
      "Epoch:0055\n",
      "acc_train:0.7589 pre_train:0.8388 recall_train:0.6602 F1_train:0.7389 AUC_train:0.8419\n",
      "acc_val:0.7500 pre_val:0.8378 recall_val:0.6200 F1_val:0.712644 AUC_val:0.8136\n",
      "Epoch:0056\n",
      "acc_train:0.7989 pre_train:0.8737 recall_train:0.7140 F1_train:0.7858 AUC_train:0.8691\n",
      "acc_val:0.7200 pre_val:0.7895 recall_val:0.6000 F1_val:0.681818 AUC_val:0.8048\n",
      "Epoch:0057\n",
      "acc_train:0.8011 pre_train:0.8648 recall_train:0.7290 F1_train:0.7911 AUC_train:0.8738\n",
      "acc_val:0.6800 pre_val:0.7647 recall_val:0.5200 F1_val:0.619048 AUC_val:0.7984\n",
      "Epoch:0058\n",
      "acc_train:0.7844 pre_train:0.8379 recall_train:0.7226 F1_train:0.7760 AUC_train:0.8760\n",
      "acc_val:0.7000 pre_val:0.8125 recall_val:0.5200 F1_val:0.634146 AUC_val:0.8060\n",
      "Epoch:0059\n",
      "acc_train:0.7900 pre_train:0.8632 recall_train:0.7054 F1_train:0.7763 AUC_train:0.8795\n",
      "acc_val:0.7400 pre_val:0.8529 recall_val:0.5800 F1_val:0.690476 AUC_val:0.8104\n",
      "Epoch:0060\n",
      "acc_train:0.8111 pre_train:0.8715 recall_train:0.7441 F1_train:0.8028 AUC_train:0.8922\n",
      "acc_val:0.7400 pre_val:0.8529 recall_val:0.5800 F1_val:0.690476 AUC_val:0.8208\n",
      "Epoch:0061\n",
      "acc_train:0.8056 pre_train:0.8796 recall_train:0.7226 F1_train:0.7934 AUC_train:0.8969\n",
      "acc_val:0.7200 pre_val:0.8235 recall_val:0.5600 F1_val:0.666667 AUC_val:0.8320\n",
      "Epoch:0062\n",
      "acc_train:0.8300 pre_train:0.8750 recall_train:0.7828 F1_train:0.8263 AUC_train:0.9013\n",
      "acc_val:0.7400 pre_val:0.8529 recall_val:0.5800 F1_val:0.690476 AUC_val:0.8252\n",
      "Epoch:0063\n",
      "acc_train:0.8289 pre_train:0.8747 recall_train:0.7806 F1_train:0.8250 AUC_train:0.9036\n",
      "acc_val:0.7400 pre_val:0.8529 recall_val:0.5800 F1_val:0.690476 AUC_val:0.8284\n",
      "Epoch:0064\n",
      "acc_train:0.8367 pre_train:0.9119 recall_train:0.7570 F1_train:0.8273 AUC_train:0.9153\n",
      "acc_val:0.7400 pre_val:0.8529 recall_val:0.5800 F1_val:0.690476 AUC_val:0.8328\n",
      "Epoch:0065\n",
      "acc_train:0.8356 pre_train:0.9096 recall_train:0.7570 F1_train:0.8263 AUC_train:0.9101\n",
      "acc_val:0.7300 pre_val:0.8485 recall_val:0.5600 F1_val:0.674699 AUC_val:0.8384\n",
      "Epoch:0066\n",
      "acc_train:0.8411 pre_train:0.8985 recall_train:0.7806 F1_train:0.8354 AUC_train:0.9173\n",
      "acc_val:0.7300 pre_val:0.8485 recall_val:0.5600 F1_val:0.674699 AUC_val:0.8400\n",
      "Epoch:0067\n",
      "acc_train:0.8444 pre_train:0.8935 recall_train:0.7935 F1_train:0.8405 AUC_train:0.9289\n",
      "acc_val:0.7400 pre_val:0.8750 recall_val:0.5600 F1_val:0.682927 AUC_val:0.8384\n",
      "Epoch:0068\n",
      "acc_train:0.8544 pre_train:0.9113 recall_train:0.7957 F1_train:0.8496 AUC_train:0.9376\n",
      "acc_val:0.7300 pre_val:0.8485 recall_val:0.5600 F1_val:0.674699 AUC_val:0.8400\n",
      "Epoch:0069\n",
      "acc_train:0.8778 pre_train:0.9340 recall_train:0.8215 F1_train:0.8741 AUC_train:0.9528\n",
      "acc_val:0.7100 pre_val:0.8000 recall_val:0.5600 F1_val:0.658824 AUC_val:0.8384\n",
      "Epoch:0070\n",
      "acc_train:0.8733 pre_train:0.9488 recall_train:0.7978 F1_train:0.8668 AUC_train:0.9494\n",
      "acc_val:0.7300 pre_val:0.8108 recall_val:0.6000 F1_val:0.689655 AUC_val:0.8352\n",
      "Epoch:0071\n",
      "acc_train:0.8756 pre_train:0.9337 recall_train:0.8172 F1_train:0.8716 AUC_train:0.9450\n",
      "acc_val:0.7400 pre_val:0.8333 recall_val:0.6000 F1_val:0.697674 AUC_val:0.8376\n",
      "Epoch:0072\n",
      "acc_train:0.8911 pre_train:0.9509 recall_train:0.8323 F1_train:0.8876 AUC_train:0.9558\n",
      "acc_val:0.7600 pre_val:0.8611 recall_val:0.6200 F1_val:0.720930 AUC_val:0.8412\n",
      "Epoch:0073\n",
      "acc_train:0.8822 pre_train:0.9476 recall_train:0.8172 F1_train:0.8776 AUC_train:0.9609\n",
      "acc_val:0.7700 pre_val:0.8857 recall_val:0.6200 F1_val:0.729412 AUC_val:0.8516\n",
      "Epoch:0074\n",
      "acc_train:0.8978 pre_train:0.9770 recall_train:0.8215 F1_train:0.8925 AUC_train:0.9732\n",
      "acc_val:0.7700 pre_val:0.8857 recall_val:0.6200 F1_val:0.729412 AUC_val:0.8624\n",
      "Epoch:0075\n",
      "acc_train:0.8922 pre_train:0.9577 recall_train:0.8280 F1_train:0.8881 AUC_train:0.9655\n",
      "acc_val:0.7900 pre_val:0.8919 recall_val:0.6600 F1_val:0.758621 AUC_val:0.8648\n",
      "Epoch:0076\n",
      "acc_train:0.8967 pre_train:0.9794 recall_train:0.8172 F1_train:0.8910 AUC_train:0.9641\n",
      "acc_val:0.7800 pre_val:0.8889 recall_val:0.6400 F1_val:0.744186 AUC_val:0.8700\n",
      "Epoch:0077\n",
      "acc_train:0.8989 pre_train:0.9770 recall_train:0.8237 F1_train:0.8938 AUC_train:0.9733\n",
      "acc_val:0.7800 pre_val:0.8889 recall_val:0.6400 F1_val:0.744186 AUC_val:0.8796\n",
      "Epoch:0078\n",
      "acc_train:0.8956 pre_train:0.9720 recall_train:0.8215 F1_train:0.8904 AUC_train:0.9691\n",
      "acc_val:0.7700 pre_val:0.8857 recall_val:0.6200 F1_val:0.729412 AUC_val:0.8876\n",
      "Epoch:0079\n",
      "acc_train:0.9067 pre_train:0.9751 recall_train:0.8409 F1_train:0.9030 AUC_train:0.9721\n",
      "acc_val:0.7500 pre_val:0.8571 recall_val:0.6000 F1_val:0.705882 AUC_val:0.8912\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0080\n",
      "acc_train:0.9122 pre_train:0.9595 recall_train:0.8667 F1_train:0.9107 AUC_train:0.9742\n",
      "acc_val:0.7400 pre_val:0.8333 recall_val:0.6000 F1_val:0.697674 AUC_val:0.8992\n",
      "Epoch:0081\n",
      "acc_train:0.9189 pre_train:0.9734 recall_train:0.8667 F1_train:0.9170 AUC_train:0.9763\n",
      "acc_val:0.7600 pre_val:0.8421 recall_val:0.6400 F1_val:0.727273 AUC_val:0.9008\n",
      "Epoch:0082\n",
      "acc_train:0.9222 pre_train:0.9714 recall_train:0.8753 F1_train:0.9208 AUC_train:0.9797\n",
      "acc_val:0.7600 pre_val:0.8421 recall_val:0.6400 F1_val:0.727273 AUC_val:0.8976\n",
      "Epoch:0083\n",
      "acc_train:0.9267 pre_train:0.9739 recall_train:0.8817 F1_train:0.9255 AUC_train:0.9825\n",
      "acc_val:0.7700 pre_val:0.8462 recall_val:0.6600 F1_val:0.741573 AUC_val:0.8972\n",
      "Epoch:0084\n",
      "acc_train:0.9244 pre_train:0.9693 recall_train:0.8817 F1_train:0.9234 AUC_train:0.9797\n",
      "acc_val:0.7600 pre_val:0.8250 recall_val:0.6600 F1_val:0.733333 AUC_val:0.9004\n",
      "Epoch:0085\n",
      "acc_train:0.9200 pre_train:0.9735 recall_train:0.8688 F1_train:0.9182 AUC_train:0.9845\n",
      "acc_val:0.7700 pre_val:0.8293 recall_val:0.6800 F1_val:0.747253 AUC_val:0.9040\n",
      "Epoch:0086\n",
      "acc_train:0.9244 pre_train:0.9737 recall_train:0.8774 F1_train:0.9231 AUC_train:0.9848\n",
      "acc_val:0.7800 pre_val:0.8333 recall_val:0.7000 F1_val:0.760870 AUC_val:0.9048\n",
      "Epoch:0087\n",
      "acc_train:0.9267 pre_train:0.9650 recall_train:0.8903 F1_train:0.9262 AUC_train:0.9770\n",
      "acc_val:0.8000 pre_val:0.8571 recall_val:0.7200 F1_val:0.782609 AUC_val:0.9012\n",
      "Epoch:0088\n",
      "acc_train:0.9367 pre_train:0.9834 recall_train:0.8925 F1_train:0.9357 AUC_train:0.9892\n",
      "acc_val:0.8100 pre_val:0.8605 recall_val:0.7400 F1_val:0.795699 AUC_val:0.9020\n",
      "Epoch:0089\n",
      "acc_train:0.9433 pre_train:0.9814 recall_train:0.9075 F1_train:0.9430 AUC_train:0.9884\n",
      "acc_val:0.8200 pre_val:0.8810 recall_val:0.7400 F1_val:0.804348 AUC_val:0.9012\n",
      "Epoch:0090\n",
      "acc_train:0.9400 pre_train:0.9768 recall_train:0.9054 F1_train:0.9397 AUC_train:0.9880\n",
      "acc_val:0.8100 pre_val:0.8605 recall_val:0.7400 F1_val:0.795699 AUC_val:0.9004\n",
      "Epoch:0091\n",
      "acc_train:0.9311 pre_train:0.9832 recall_train:0.8817 F1_train:0.9297 AUC_train:0.9870\n",
      "acc_val:0.8100 pre_val:0.8605 recall_val:0.7400 F1_val:0.795699 AUC_val:0.9016\n",
      "Epoch:0092\n",
      "acc_train:0.9422 pre_train:0.9640 recall_train:0.9226 F1_train:0.9429 AUC_train:0.9898\n",
      "acc_val:0.8200 pre_val:0.8810 recall_val:0.7400 F1_val:0.804348 AUC_val:0.9036\n",
      "Epoch:0093\n",
      "acc_train:0.9456 pre_train:0.9860 recall_train:0.9075 F1_train:0.9451 AUC_train:0.9918\n",
      "acc_val:0.8200 pre_val:0.9000 recall_val:0.7200 F1_val:0.800000 AUC_val:0.9036\n",
      "Epoch:0094\n",
      "acc_train:0.9400 pre_train:0.9724 recall_train:0.9097 F1_train:0.9400 AUC_train:0.9886\n",
      "acc_val:0.8000 pre_val:0.8750 recall_val:0.7000 F1_val:0.777778 AUC_val:0.8988\n",
      "Epoch:0095\n",
      "acc_train:0.9300 pre_train:0.9674 recall_train:0.8946 F1_train:0.9296 AUC_train:0.9878\n",
      "acc_val:0.7900 pre_val:0.8537 recall_val:0.7000 F1_val:0.769231 AUC_val:0.8944\n",
      "Epoch:0096\n",
      "acc_train:0.9444 pre_train:0.9770 recall_train:0.9140 F1_train:0.9444 AUC_train:0.9919\n",
      "acc_val:0.7900 pre_val:0.8537 recall_val:0.7000 F1_val:0.769231 AUC_val:0.8952\n",
      "Epoch:0097\n",
      "acc_train:0.9356 pre_train:0.9678 recall_train:0.9054 F1_train:0.9356 AUC_train:0.9865\n",
      "acc_val:0.8100 pre_val:0.8974 recall_val:0.7000 F1_val:0.786517 AUC_val:0.9000\n",
      "Epoch:0098\n",
      "acc_train:0.9456 pre_train:0.9815 recall_train:0.9118 F1_train:0.9454 AUC_train:0.9886\n",
      "acc_val:0.7900 pre_val:0.8919 recall_val:0.6600 F1_val:0.758621 AUC_val:0.9056\n",
      "Epoch:0099\n",
      "acc_train:0.9511 pre_train:0.9907 recall_train:0.9140 F1_train:0.9508 AUC_train:0.9909\n",
      "acc_val:0.7900 pre_val:0.8919 recall_val:0.6600 F1_val:0.758621 AUC_val:0.9104\n",
      "Epoch:0100\n",
      "acc_train:0.9611 pre_train:0.9821 recall_train:0.9419 F1_train:0.9616 AUC_train:0.9911\n",
      "acc_val:0.7800 pre_val:0.8889 recall_val:0.6400 F1_val:0.744186 AUC_val:0.9088\n",
      "Epoch:0101\n",
      "acc_train:0.9444 pre_train:0.9748 recall_train:0.9161 F1_train:0.9446 AUC_train:0.9905\n",
      "acc_val:0.7800 pre_val:0.9118 recall_val:0.6200 F1_val:0.738095 AUC_val:0.9084\n",
      "Epoch:0102\n",
      "acc_train:0.9656 pre_train:0.9844 recall_train:0.9484 F1_train:0.9660 AUC_train:0.9948\n",
      "acc_val:0.7900 pre_val:0.9143 recall_val:0.6400 F1_val:0.752941 AUC_val:0.9084\n",
      "Epoch:0103\n",
      "acc_train:0.9600 pre_train:0.9886 recall_train:0.9333 F1_train:0.9602 AUC_train:0.9950\n",
      "acc_val:0.7800 pre_val:0.8889 recall_val:0.6400 F1_val:0.744186 AUC_val:0.9116\n",
      "Epoch:0104\n",
      "acc_train:0.9467 pre_train:0.9838 recall_train:0.9118 F1_train:0.9464 AUC_train:0.9938\n",
      "acc_val:0.7800 pre_val:0.8889 recall_val:0.6400 F1_val:0.744186 AUC_val:0.9100\n",
      "Epoch:0105\n",
      "acc_train:0.9478 pre_train:0.9816 recall_train:0.9161 F1_train:0.9477 AUC_train:0.9925\n",
      "acc_val:0.7800 pre_val:0.8889 recall_val:0.6400 F1_val:0.744186 AUC_val:0.9076\n",
      "Epoch:0106\n",
      "acc_train:0.9600 pre_train:0.9886 recall_train:0.9333 F1_train:0.9602 AUC_train:0.9918\n",
      "acc_val:0.7800 pre_val:0.8889 recall_val:0.6400 F1_val:0.744186 AUC_val:0.9064\n",
      "Epoch:0107\n",
      "acc_train:0.9633 pre_train:0.9977 recall_train:0.9312 F1_train:0.9633 AUC_train:0.9928\n",
      "acc_val:0.7800 pre_val:0.8889 recall_val:0.6400 F1_val:0.744186 AUC_val:0.9084\n",
      "Epoch:0108\n",
      "acc_train:0.9544 pre_train:0.9796 recall_train:0.9312 F1_train:0.9548 AUC_train:0.9906\n",
      "acc_val:0.7900 pre_val:0.8919 recall_val:0.6600 F1_val:0.758621 AUC_val:0.9076\n",
      "Epoch:0109\n",
      "acc_train:0.9633 pre_train:0.9977 recall_train:0.9312 F1_train:0.9633 AUC_train:0.9971\n",
      "acc_val:0.7800 pre_val:0.8684 recall_val:0.6600 F1_val:0.750000 AUC_val:0.9088\n",
      "Epoch:0110\n",
      "acc_train:0.9589 pre_train:0.9842 recall_train:0.9355 F1_train:0.9592 AUC_train:0.9945\n",
      "acc_val:0.7900 pre_val:0.8718 recall_val:0.6800 F1_val:0.764045 AUC_val:0.9064\n",
      "Epoch:0111\n",
      "acc_train:0.9633 pre_train:0.9887 recall_train:0.9398 F1_train:0.9636 AUC_train:0.9948\n",
      "acc_val:0.8100 pre_val:0.8974 recall_val:0.7000 F1_val:0.786517 AUC_val:0.9008\n",
      "Epoch:0112\n",
      "acc_train:0.9589 pre_train:0.9714 recall_train:0.9484 F1_train:0.9597 AUC_train:0.9937\n",
      "acc_val:0.8000 pre_val:0.8750 recall_val:0.7000 F1_val:0.777778 AUC_val:0.8986\n",
      "Early Stopping!!! epoch：111\n",
      " Starting the 1-3 Fold:：\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.5300 pre_train:0.5734 recall_train:0.3527 F1_train:0.4368 AUC_train:0.5244\n",
      "acc_val:0.5300 pre_val:0.5172 recall_val:0.9000 F1_val:0.656934 AUC_val:0.6440\n",
      "Epoch:0002\n",
      "acc_train:0.5056 pre_train:0.5250 recall_train:0.4516 F1_train:0.4855 AUC_train:0.5014\n",
      "acc_val:0.6100 pre_val:0.6279 recall_val:0.5400 F1_val:0.580645 AUC_val:0.6584\n",
      "Epoch:0003\n",
      "acc_train:0.5556 pre_train:0.6032 recall_train:0.4086 F1_train:0.4872 AUC_train:0.5563\n",
      "acc_val:0.6300 pre_val:0.6857 recall_val:0.4800 F1_val:0.564706 AUC_val:0.6732\n",
      "Epoch:0004\n",
      "acc_train:0.5367 pre_train:0.5745 recall_train:0.3978 F1_train:0.4701 AUC_train:0.5495\n",
      "acc_val:0.6300 pre_val:0.7097 recall_val:0.4400 F1_val:0.543210 AUC_val:0.6832\n",
      "Epoch:0005\n",
      "acc_train:0.5811 pre_train:0.6457 recall_train:0.4194 F1_train:0.5085 AUC_train:0.5925\n",
      "acc_val:0.6100 pre_val:0.6897 recall_val:0.4000 F1_val:0.506329 AUC_val:0.6784\n",
      "Epoch:0006\n",
      "acc_train:0.5644 pre_train:0.6034 recall_train:0.4581 F1_train:0.5208 AUC_train:0.5685\n",
      "acc_val:0.6300 pre_val:0.7407 recall_val:0.4000 F1_val:0.519481 AUC_val:0.6816\n",
      "Epoch:0007\n",
      "acc_train:0.6000 pre_train:0.6656 recall_train:0.4538 F1_train:0.5396 AUC_train:0.6101\n",
      "acc_val:0.6400 pre_val:0.7692 recall_val:0.4000 F1_val:0.526316 AUC_val:0.6828\n",
      "Epoch:0008\n",
      "acc_train:0.5344 pre_train:0.5635 recall_train:0.4387 F1_train:0.4933 AUC_train:0.5541\n",
      "acc_val:0.6300 pre_val:0.7600 recall_val:0.3800 F1_val:0.506667 AUC_val:0.6836\n",
      "Epoch:0009\n",
      "acc_train:0.5733 pre_train:0.6052 recall_train:0.5011 F1_train:0.5482 AUC_train:0.6025\n",
      "acc_val:0.6200 pre_val:0.7308 recall_val:0.3800 F1_val:0.500000 AUC_val:0.6896\n",
      "Epoch:0010\n",
      "acc_train:0.5478 pre_train:0.5707 recall_train:0.5032 F1_train:0.5349 AUC_train:0.5618\n",
      "acc_val:0.6500 pre_val:0.6829 recall_val:0.5600 F1_val:0.615385 AUC_val:0.6856\n",
      "Epoch:0011\n",
      "acc_train:0.5333 pre_train:0.5517 recall_train:0.5161 F1_train:0.5333 AUC_train:0.5551\n",
      "acc_val:0.6500 pre_val:0.6744 recall_val:0.5800 F1_val:0.623656 AUC_val:0.6852\n",
      "Epoch:0012\n",
      "acc_train:0.5867 pre_train:0.6183 recall_train:0.5226 F1_train:0.5664 AUC_train:0.5879\n",
      "acc_val:0.6200 pre_val:0.6304 recall_val:0.5800 F1_val:0.604167 AUC_val:0.6844\n",
      "Epoch:0013\n",
      "acc_train:0.5867 pre_train:0.6094 recall_train:0.5570 F1_train:0.5820 AUC_train:0.6215\n",
      "acc_val:0.6000 pre_val:0.6087 recall_val:0.5600 F1_val:0.583333 AUC_val:0.6904\n",
      "Epoch:0014\n",
      "acc_train:0.6022 pre_train:0.6375 recall_train:0.5333 F1_train:0.5808 AUC_train:0.6293\n",
      "acc_val:0.6100 pre_val:0.6122 recall_val:0.6000 F1_val:0.606061 AUC_val:0.6976\n",
      "Epoch:0015\n",
      "acc_train:0.5900 pre_train:0.6188 recall_train:0.5376 F1_train:0.5754 AUC_train:0.6170\n",
      "acc_val:0.6100 pre_val:0.6122 recall_val:0.6000 F1_val:0.606061 AUC_val:0.6972\n",
      "Epoch:0016\n",
      "acc_train:0.5978 pre_train:0.6157 recall_train:0.5892 F1_train:0.6022 AUC_train:0.6231\n",
      "acc_val:0.6100 pre_val:0.6078 recall_val:0.6200 F1_val:0.613861 AUC_val:0.6932\n",
      "Epoch:0017\n",
      "acc_train:0.6167 pre_train:0.6493 recall_train:0.5613 F1_train:0.6021 AUC_train:0.6512\n",
      "acc_val:0.6100 pre_val:0.6078 recall_val:0.6200 F1_val:0.613861 AUC_val:0.6952\n",
      "Epoch:0018\n",
      "acc_train:0.5967 pre_train:0.6262 recall_train:0.5441 F1_train:0.5823 AUC_train:0.6004\n",
      "acc_val:0.6100 pre_val:0.6078 recall_val:0.6200 F1_val:0.613861 AUC_val:0.6988\n",
      "Epoch:0019\n",
      "acc_train:0.6044 pre_train:0.6339 recall_train:0.5548 F1_train:0.5917 AUC_train:0.6289\n",
      "acc_val:0.6000 pre_val:0.5962 recall_val:0.6200 F1_val:0.607843 AUC_val:0.7020\n",
      "Epoch:0020\n",
      "acc_train:0.5778 pre_train:0.6034 recall_train:0.5333 F1_train:0.5662 AUC_train:0.5935\n",
      "acc_val:0.6000 pre_val:0.5962 recall_val:0.6200 F1_val:0.607843 AUC_val:0.7016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0021\n",
      "acc_train:0.6333 pre_train:0.6634 recall_train:0.5892 F1_train:0.6241 AUC_train:0.6603\n",
      "acc_val:0.6000 pre_val:0.5962 recall_val:0.6200 F1_val:0.607843 AUC_val:0.7004\n",
      "Epoch:0022\n",
      "acc_train:0.5967 pre_train:0.6232 recall_train:0.5548 F1_train:0.5870 AUC_train:0.6478\n",
      "acc_val:0.6000 pre_val:0.5962 recall_val:0.6200 F1_val:0.607843 AUC_val:0.7032\n",
      "Epoch:0023\n",
      "acc_train:0.6189 pre_train:0.6548 recall_train:0.5548 F1_train:0.6007 AUC_train:0.6429\n",
      "acc_val:0.6000 pre_val:0.5962 recall_val:0.6200 F1_val:0.607843 AUC_val:0.7056\n",
      "Epoch:0024\n",
      "acc_train:0.6200 pre_train:0.6534 recall_train:0.5634 F1_train:0.6051 AUC_train:0.6653\n",
      "acc_val:0.6000 pre_val:0.5962 recall_val:0.6200 F1_val:0.607843 AUC_val:0.7052\n",
      "Epoch:0025\n",
      "acc_train:0.5978 pre_train:0.6272 recall_train:0.5462 F1_train:0.5839 AUC_train:0.6358\n",
      "acc_val:0.6000 pre_val:0.5962 recall_val:0.6200 F1_val:0.607843 AUC_val:0.7076\n",
      "Epoch:0026\n",
      "acc_train:0.6100 pre_train:0.6377 recall_train:0.5677 F1_train:0.6007 AUC_train:0.6537\n",
      "acc_val:0.6000 pre_val:0.5962 recall_val:0.6200 F1_val:0.607843 AUC_val:0.7088\n",
      "Epoch:0027\n",
      "acc_train:0.6167 pre_train:0.6579 recall_train:0.5376 F1_train:0.5917 AUC_train:0.6562\n",
      "acc_val:0.6000 pre_val:0.5962 recall_val:0.6200 F1_val:0.607843 AUC_val:0.7084\n",
      "Epoch:0028\n",
      "acc_train:0.6078 pre_train:0.6284 recall_train:0.5892 F1_train:0.6082 AUC_train:0.6482\n",
      "acc_val:0.5800 pre_val:0.5800 recall_val:0.5800 F1_val:0.580000 AUC_val:0.6992\n",
      "Epoch:0029\n",
      "acc_train:0.6078 pre_train:0.6458 recall_train:0.5333 F1_train:0.5842 AUC_train:0.6597\n",
      "acc_val:0.5900 pre_val:0.5918 recall_val:0.5800 F1_val:0.585859 AUC_val:0.7000\n",
      "Epoch:0030\n",
      "acc_train:0.6167 pre_train:0.6579 recall_train:0.5376 F1_train:0.5917 AUC_train:0.6636\n",
      "acc_val:0.5800 pre_val:0.5833 recall_val:0.5600 F1_val:0.571429 AUC_val:0.7016\n",
      "Epoch:0031\n",
      "acc_train:0.6511 pre_train:0.7103 recall_train:0.5484 F1_train:0.6189 AUC_train:0.6967\n",
      "acc_val:0.6100 pre_val:0.6222 recall_val:0.5600 F1_val:0.589474 AUC_val:0.7068\n",
      "Epoch:0032\n",
      "acc_train:0.6356 pre_train:0.6789 recall_train:0.5591 F1_train:0.6132 AUC_train:0.6895\n",
      "acc_val:0.6000 pre_val:0.6190 recall_val:0.5200 F1_val:0.565217 AUC_val:0.7120\n",
      "Epoch:0033\n",
      "acc_train:0.6333 pre_train:0.6735 recall_train:0.5634 F1_train:0.6136 AUC_train:0.6923\n",
      "acc_val:0.6000 pre_val:0.6190 recall_val:0.5200 F1_val:0.565217 AUC_val:0.7152\n",
      "Epoch:0034\n",
      "acc_train:0.6056 pre_train:0.6322 recall_train:0.5656 F1_train:0.5970 AUC_train:0.6798\n",
      "acc_val:0.6000 pre_val:0.6190 recall_val:0.5200 F1_val:0.565217 AUC_val:0.7208\n",
      "Epoch:0035\n",
      "acc_train:0.6511 pre_train:0.7139 recall_train:0.5419 F1_train:0.6161 AUC_train:0.7161\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.7248\n",
      "Epoch:0036\n",
      "acc_train:0.6367 pre_train:0.6778 recall_train:0.5656 F1_train:0.6166 AUC_train:0.6864\n",
      "acc_val:0.6600 pre_val:0.7105 recall_val:0.5400 F1_val:0.613636 AUC_val:0.7312\n",
      "Epoch:0037\n",
      "acc_train:0.6356 pre_train:0.6691 recall_train:0.5828 F1_train:0.6230 AUC_train:0.6973\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7408\n",
      "Epoch:0038\n",
      "acc_train:0.6533 pre_train:0.6917 recall_train:0.5935 F1_train:0.6389 AUC_train:0.7258\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7392\n",
      "Epoch:0039\n",
      "acc_train:0.6633 pre_train:0.7143 recall_train:0.5806 F1_train:0.6406 AUC_train:0.7338\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7376\n",
      "Epoch:0040\n",
      "acc_train:0.6700 pre_train:0.7090 recall_train:0.6129 F1_train:0.6574 AUC_train:0.7385\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7388\n",
      "Epoch:0041\n",
      "acc_train:0.6556 pre_train:0.7159 recall_train:0.5527 F1_train:0.6238 AUC_train:0.7375\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7392\n",
      "Epoch:0042\n",
      "acc_train:0.6689 pre_train:0.7125 recall_train:0.6022 F1_train:0.6527 AUC_train:0.7505\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7456\n",
      "Epoch:0043\n",
      "acc_train:0.6600 pre_train:0.7065 recall_train:0.5849 F1_train:0.6400 AUC_train:0.7430\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7464\n",
      "Epoch:0044\n",
      "acc_train:0.6789 pre_train:0.7444 recall_train:0.5763 F1_train:0.6497 AUC_train:0.7411\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7496\n",
      "Epoch:0045\n",
      "acc_train:0.6467 pre_train:0.7003 recall_train:0.5527 F1_train:0.6178 AUC_train:0.7301\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7520\n",
      "Epoch:0046\n",
      "acc_train:0.6578 pre_train:0.7093 recall_train:0.5720 F1_train:0.6333 AUC_train:0.7530\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7564\n",
      "Epoch:0047\n",
      "acc_train:0.6944 pre_train:0.7582 recall_train:0.6000 F1_train:0.6699 AUC_train:0.7759\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7584\n",
      "Epoch:0048\n",
      "acc_train:0.6933 pre_train:0.7561 recall_train:0.6000 F1_train:0.6691 AUC_train:0.7751\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7644\n",
      "Epoch:0049\n",
      "acc_train:0.6822 pre_train:0.7106 recall_train:0.6495 F1_train:0.6787 AUC_train:0.7880\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7672\n",
      "Epoch:0050\n",
      "acc_train:0.6722 pre_train:0.7273 recall_train:0.5849 F1_train:0.6484 AUC_train:0.7710\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7688\n",
      "Epoch:0051\n",
      "acc_train:0.6744 pre_train:0.7216 recall_train:0.6022 F1_train:0.6565 AUC_train:0.7840\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7672\n",
      "Epoch:0052\n",
      "acc_train:0.6744 pre_train:0.6972 recall_train:0.6538 F1_train:0.6748 AUC_train:0.8010\n",
      "acc_val:0.6700 pre_val:0.7576 recall_val:0.5000 F1_val:0.602410 AUC_val:0.7672\n",
      "Epoch:0053\n",
      "acc_train:0.6989 pre_train:0.7650 recall_train:0.6022 F1_train:0.6739 AUC_train:0.8107\n",
      "acc_val:0.6700 pre_val:0.7576 recall_val:0.5000 F1_val:0.602410 AUC_val:0.7708\n",
      "Epoch:0054\n",
      "acc_train:0.7100 pre_train:0.7802 recall_train:0.6108 F1_train:0.6852 AUC_train:0.8218\n",
      "acc_val:0.6500 pre_val:0.7143 recall_val:0.5000 F1_val:0.588235 AUC_val:0.7656\n",
      "Epoch:0055\n",
      "acc_train:0.7122 pre_train:0.7384 recall_train:0.6860 F1_train:0.7113 AUC_train:0.8368\n",
      "acc_val:0.6500 pre_val:0.7143 recall_val:0.5000 F1_val:0.588235 AUC_val:0.7716\n",
      "Epoch:0056\n",
      "acc_train:0.7222 pre_train:0.8045 recall_train:0.6108 F1_train:0.6944 AUC_train:0.8430\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.7784\n",
      "Epoch:0057\n",
      "acc_train:0.7044 pre_train:0.7625 recall_train:0.6215 F1_train:0.6848 AUC_train:0.8473\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.8008\n",
      "Epoch:0058\n",
      "acc_train:0.7089 pre_train:0.7434 recall_train:0.6667 F1_train:0.7029 AUC_train:0.8400\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.8160\n",
      "Epoch:0059\n",
      "acc_train:0.7511 pre_train:0.7809 recall_train:0.7204 F1_train:0.7494 AUC_train:0.8559\n",
      "acc_val:0.6800 pre_val:0.7250 recall_val:0.5800 F1_val:0.644444 AUC_val:0.8256\n",
      "Epoch:0060\n",
      "acc_train:0.7200 pre_train:0.7566 recall_train:0.6753 F1_train:0.7136 AUC_train:0.8570\n",
      "acc_val:0.6700 pre_val:0.7073 recall_val:0.5800 F1_val:0.637363 AUC_val:0.8228\n",
      "Epoch:0061\n",
      "acc_train:0.7056 pre_train:0.7463 recall_train:0.6516 F1_train:0.6958 AUC_train:0.8500\n",
      "acc_val:0.6700 pre_val:0.7073 recall_val:0.5800 F1_val:0.637363 AUC_val:0.8256\n",
      "Epoch:0062\n",
      "acc_train:0.7544 pre_train:0.8081 recall_train:0.6882 F1_train:0.7433 AUC_train:0.8829\n",
      "acc_val:0.7400 pre_val:0.7727 recall_val:0.6800 F1_val:0.723404 AUC_val:0.8348\n",
      "Epoch:0063\n",
      "acc_train:0.7822 pre_train:0.7808 recall_train:0.8043 F1_train:0.7924 AUC_train:0.8709\n",
      "acc_val:0.7600 pre_val:0.7708 recall_val:0.7400 F1_val:0.755102 AUC_val:0.8460\n",
      "Epoch:0064\n",
      "acc_train:0.7422 pre_train:0.7877 recall_train:0.6860 F1_train:0.7333 AUC_train:0.8835\n",
      "acc_val:0.7700 pre_val:0.7455 recall_val:0.8200 F1_val:0.780952 AUC_val:0.8492\n",
      "Epoch:0065\n",
      "acc_train:0.7989 pre_train:0.7898 recall_train:0.8323 F1_train:0.8105 AUC_train:0.8854\n",
      "acc_val:0.7600 pre_val:0.7167 recall_val:0.8600 F1_val:0.781818 AUC_val:0.8436\n",
      "Epoch:0066\n",
      "acc_train:0.7044 pre_train:0.7469 recall_train:0.6473 F1_train:0.6935 AUC_train:0.8665\n",
      "acc_val:0.7400 pre_val:0.7000 recall_val:0.8400 F1_val:0.763636 AUC_val:0.8236\n",
      "Epoch:0067\n",
      "acc_train:0.8256 pre_train:0.7873 recall_train:0.9075 F1_train:0.8432 AUC_train:0.8941\n",
      "acc_val:0.7600 pre_val:0.7097 recall_val:0.8800 F1_val:0.785714 AUC_val:0.8312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0068\n",
      "acc_train:0.8211 pre_train:0.7868 recall_train:0.8968 F1_train:0.8382 AUC_train:0.9089\n",
      "acc_val:0.7500 pre_val:0.7049 recall_val:0.8600 F1_val:0.774775 AUC_val:0.8352\n",
      "Epoch:0069\n",
      "acc_train:0.8100 pre_train:0.7763 recall_train:0.8882 F1_train:0.8285 AUC_train:0.8879\n",
      "acc_val:0.7500 pre_val:0.7049 recall_val:0.8600 F1_val:0.774775 AUC_val:0.8400\n",
      "Epoch:0070\n",
      "acc_train:0.8300 pre_train:0.7727 recall_train:0.9505 F1_train:0.8525 AUC_train:0.9090\n",
      "acc_val:0.7600 pre_val:0.7097 recall_val:0.8800 F1_val:0.785714 AUC_val:0.8488\n",
      "Epoch:0071\n",
      "acc_train:0.8333 pre_train:0.7955 recall_train:0.9118 F1_train:0.8497 AUC_train:0.9123\n",
      "acc_val:0.7700 pre_val:0.7288 recall_val:0.8600 F1_val:0.788991 AUC_val:0.8564\n",
      "Epoch:0072\n",
      "acc_train:0.8356 pre_train:0.7974 recall_train:0.9140 F1_train:0.8517 AUC_train:0.9159\n",
      "acc_val:0.7800 pre_val:0.7500 recall_val:0.8400 F1_val:0.792453 AUC_val:0.8668\n",
      "Epoch:0073\n",
      "acc_train:0.8411 pre_train:0.7917 recall_train:0.9398 F1_train:0.8594 AUC_train:0.9048\n",
      "acc_val:0.7900 pre_val:0.7636 recall_val:0.8400 F1_val:0.800000 AUC_val:0.8696\n",
      "Epoch:0074\n",
      "acc_train:0.8322 pre_train:0.7962 recall_train:0.9075 F1_train:0.8482 AUC_train:0.8999\n",
      "acc_val:0.7800 pre_val:0.7593 recall_val:0.8200 F1_val:0.788462 AUC_val:0.8716\n",
      "Epoch:0075\n",
      "acc_train:0.8267 pre_train:0.7954 recall_train:0.8946 F1_train:0.8421 AUC_train:0.9112\n",
      "acc_val:0.7900 pre_val:0.7736 recall_val:0.8200 F1_val:0.796117 AUC_val:0.8812\n",
      "Epoch:0076\n",
      "acc_train:0.8611 pre_train:0.8346 recall_train:0.9118 F1_train:0.8715 AUC_train:0.9220\n",
      "acc_val:0.8000 pre_val:0.7885 recall_val:0.8200 F1_val:0.803922 AUC_val:0.8936\n",
      "Epoch:0077\n",
      "acc_train:0.8400 pre_train:0.8229 recall_train:0.8796 F1_train:0.8503 AUC_train:0.9194\n",
      "acc_val:0.8000 pre_val:0.7778 recall_val:0.8400 F1_val:0.807692 AUC_val:0.8964\n",
      "Epoch:0078\n",
      "acc_train:0.8800 pre_train:0.8426 recall_train:0.9441 F1_train:0.8905 AUC_train:0.9387\n",
      "acc_val:0.7900 pre_val:0.7636 recall_val:0.8400 F1_val:0.800000 AUC_val:0.8964\n",
      "Epoch:0079\n",
      "acc_train:0.8633 pre_train:0.8098 recall_train:0.9613 F1_train:0.8791 AUC_train:0.9422\n",
      "acc_val:0.8000 pre_val:0.7778 recall_val:0.8400 F1_val:0.807692 AUC_val:0.8984\n",
      "Epoch:0080\n",
      "acc_train:0.8589 pre_train:0.8288 recall_train:0.9161 F1_train:0.8703 AUC_train:0.9339\n",
      "acc_val:0.8000 pre_val:0.7778 recall_val:0.8400 F1_val:0.807692 AUC_val:0.8936\n",
      "Epoch:0081\n",
      "acc_train:0.8656 pre_train:0.8197 recall_train:0.9484 F1_train:0.8794 AUC_train:0.9365\n",
      "acc_val:0.7900 pre_val:0.7636 recall_val:0.8400 F1_val:0.800000 AUC_val:0.8928\n",
      "Epoch:0082\n",
      "acc_train:0.8811 pre_train:0.8377 recall_train:0.9548 F1_train:0.8925 AUC_train:0.9519\n",
      "acc_val:0.7900 pre_val:0.7636 recall_val:0.8400 F1_val:0.800000 AUC_val:0.8980\n",
      "Epoch:0083\n",
      "acc_train:0.8678 pre_train:0.8180 recall_train:0.9570 F1_train:0.8821 AUC_train:0.9413\n",
      "acc_val:0.8100 pre_val:0.7719 recall_val:0.8800 F1_val:0.822430 AUC_val:0.9024\n",
      "Epoch:0084\n",
      "acc_train:0.8856 pre_train:0.8428 recall_train:0.9570 F1_train:0.8963 AUC_train:0.9538\n",
      "acc_val:0.8200 pre_val:0.7857 recall_val:0.8800 F1_val:0.830189 AUC_val:0.9072\n",
      "Epoch:0085\n",
      "acc_train:0.8844 pre_train:0.8300 recall_train:0.9763 F1_train:0.8972 AUC_train:0.9503\n",
      "acc_val:0.8200 pre_val:0.7857 recall_val:0.8800 F1_val:0.830189 AUC_val:0.9052\n",
      "Epoch:0086\n",
      "acc_train:0.8956 pre_train:0.8659 recall_train:0.9441 F1_train:0.9033 AUC_train:0.9558\n",
      "acc_val:0.8100 pre_val:0.7818 recall_val:0.8600 F1_val:0.819048 AUC_val:0.9060\n",
      "Epoch:0087\n",
      "acc_train:0.8944 pre_train:0.8544 recall_train:0.9591 F1_train:0.9037 AUC_train:0.9570\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.9092\n",
      "Epoch:0088\n",
      "acc_train:0.9022 pre_train:0.8590 recall_train:0.9699 F1_train:0.9111 AUC_train:0.9607\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9120\n",
      "Epoch:0089\n",
      "acc_train:0.9011 pre_train:0.8615 recall_train:0.9634 F1_train:0.9096 AUC_train:0.9575\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9060\n",
      "Epoch:0090\n",
      "acc_train:0.8933 pre_train:0.8449 recall_train:0.9720 F1_train:0.9040 AUC_train:0.9620\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9092\n",
      "Epoch:0091\n",
      "acc_train:0.9044 pre_train:0.8582 recall_train:0.9763 F1_train:0.9135 AUC_train:0.9621\n",
      "acc_val:0.8400 pre_val:0.7931 recall_val:0.9200 F1_val:0.851852 AUC_val:0.9098\n",
      "Epoch:0092\n",
      "acc_train:0.9078 pre_train:0.8673 recall_train:0.9699 F1_train:0.9157 AUC_train:0.9655\n",
      "acc_val:0.8500 pre_val:0.7966 recall_val:0.9400 F1_val:0.862385 AUC_val:0.9134\n",
      "Epoch:0093\n",
      "acc_train:0.9089 pre_train:0.8634 recall_train:0.9785 F1_train:0.9173 AUC_train:0.9693\n",
      "acc_val:0.8500 pre_val:0.7966 recall_val:0.9400 F1_val:0.862385 AUC_val:0.9140\n",
      "Epoch:0094\n",
      "acc_train:0.9033 pre_train:0.8607 recall_train:0.9699 F1_train:0.9120 AUC_train:0.9632\n",
      "acc_val:0.8500 pre_val:0.7966 recall_val:0.9400 F1_val:0.862385 AUC_val:0.9174\n",
      "Epoch:0095\n",
      "acc_train:0.9111 pre_train:0.8752 recall_train:0.9656 F1_train:0.9182 AUC_train:0.9719\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9202\n",
      "Epoch:0096\n",
      "acc_train:0.9256 pre_train:0.8887 recall_train:0.9785 F1_train:0.9314 AUC_train:0.9763\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.9206\n",
      "Epoch:0097\n",
      "acc_train:0.9344 pre_train:0.9012 recall_train:0.9806 F1_train:0.9392 AUC_train:0.9807\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9202\n",
      "Epoch:0098\n",
      "acc_train:0.9344 pre_train:0.9093 recall_train:0.9699 F1_train:0.9386 AUC_train:0.9730\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9222\n",
      "Epoch:0099\n",
      "acc_train:0.9533 pre_train:0.9361 recall_train:0.9763 F1_train:0.9558 AUC_train:0.9830\n",
      "acc_val:0.8500 pre_val:0.8070 recall_val:0.9200 F1_val:0.859813 AUC_val:0.9230\n",
      "Epoch:0100\n",
      "acc_train:0.9378 pre_train:0.9066 recall_train:0.9806 F1_train:0.9421 AUC_train:0.9822\n",
      "acc_val:0.8500 pre_val:0.8070 recall_val:0.9200 F1_val:0.859813 AUC_val:0.9218\n",
      "Epoch:0101\n",
      "acc_train:0.9333 pre_train:0.8947 recall_train:0.9871 F1_train:0.9387 AUC_train:0.9802\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9202\n",
      "Epoch:0102\n",
      "acc_train:0.9178 pre_train:0.8781 recall_train:0.9763 F1_train:0.9246 AUC_train:0.9753\n",
      "acc_val:0.8200 pre_val:0.7667 recall_val:0.9200 F1_val:0.836364 AUC_val:0.9180\n",
      "Epoch:0103\n",
      "acc_train:0.9189 pre_train:0.8798 recall_train:0.9763 F1_train:0.9256 AUC_train:0.9728\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.9160\n",
      "Epoch:0104\n",
      "acc_train:0.9200 pre_train:0.8830 recall_train:0.9742 F1_train:0.9264 AUC_train:0.9751\n",
      "acc_val:0.8200 pre_val:0.7857 recall_val:0.8800 F1_val:0.830189 AUC_val:0.9132\n",
      "Epoch:0105\n",
      "acc_train:0.9422 pre_train:0.9089 recall_train:0.9871 F1_train:0.9464 AUC_train:0.9849\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9164\n",
      "Epoch:0106\n",
      "acc_train:0.9344 pre_train:0.8965 recall_train:0.9871 F1_train:0.9396 AUC_train:0.9787\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9140\n",
      "Epoch:0107\n",
      "acc_train:0.9389 pre_train:0.9116 recall_train:0.9763 F1_train:0.9429 AUC_train:0.9819\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9142\n",
      "Epoch:0108\n",
      "acc_train:0.9467 pre_train:0.9281 recall_train:0.9720 F1_train:0.9496 AUC_train:0.9808\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9138\n",
      "Epoch:0109\n",
      "acc_train:0.9511 pre_train:0.9235 recall_train:0.9871 F1_train:0.9543 AUC_train:0.9870\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9198\n",
      "Epoch:0110\n",
      "acc_train:0.9522 pre_train:0.9271 recall_train:0.9849 F1_train:0.9552 AUC_train:0.9860\n",
      "acc_val:0.8700 pre_val:0.8246 recall_val:0.9400 F1_val:0.878505 AUC_val:0.9250\n",
      "Epoch:0111\n",
      "acc_train:0.9456 pre_train:0.9262 recall_train:0.9720 F1_train:0.9486 AUC_train:0.9907\n",
      "acc_val:0.8700 pre_val:0.8364 recall_val:0.9200 F1_val:0.876190 AUC_val:0.9290\n",
      "Epoch:0112\n",
      "acc_train:0.9456 pre_train:0.9228 recall_train:0.9763 F1_train:0.9488 AUC_train:0.9859\n",
      "acc_val:0.8700 pre_val:0.8364 recall_val:0.9200 F1_val:0.876190 AUC_val:0.9278\n",
      "Epoch:0113\n",
      "acc_train:0.9433 pre_train:0.9157 recall_train:0.9806 F1_train:0.9470 AUC_train:0.9857\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9246\n",
      "Epoch:0114\n",
      "acc_train:0.9333 pre_train:0.8978 recall_train:0.9828 F1_train:0.9384 AUC_train:0.9845\n",
      "acc_val:0.8500 pre_val:0.8070 recall_val:0.9200 F1_val:0.859813 AUC_val:0.9254\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0115\n",
      "acc_train:0.9511 pre_train:0.9340 recall_train:0.9742 F1_train:0.9537 AUC_train:0.9839\n",
      "acc_val:0.8600 pre_val:0.8103 recall_val:0.9400 F1_val:0.870370 AUC_val:0.9246\n",
      "Epoch:0116\n",
      "acc_train:0.9511 pre_train:0.9287 recall_train:0.9806 F1_train:0.9540 AUC_train:0.9877\n",
      "acc_val:0.8600 pre_val:0.8103 recall_val:0.9400 F1_val:0.870370 AUC_val:0.9212\n",
      "Epoch:0117\n",
      "acc_train:0.9589 pre_train:0.9385 recall_train:0.9849 F1_train:0.9612 AUC_train:0.9892\n",
      "acc_val:0.8600 pre_val:0.8103 recall_val:0.9400 F1_val:0.870370 AUC_val:0.9200\n",
      "Epoch:0118\n",
      "acc_train:0.9500 pre_train:0.9286 recall_train:0.9785 F1_train:0.9529 AUC_train:0.9898\n",
      "acc_val:0.8600 pre_val:0.8103 recall_val:0.9400 F1_val:0.870370 AUC_val:0.9180\n",
      "Epoch:0119\n",
      "acc_train:0.9556 pre_train:0.9455 recall_train:0.9699 F1_train:0.9575 AUC_train:0.9897\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9164\n",
      "Epoch:0120\n",
      "acc_train:0.9544 pre_train:0.9473 recall_train:0.9656 F1_train:0.9563 AUC_train:0.9890\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9134\n",
      "Epoch:0121\n",
      "acc_train:0.9611 pre_train:0.9498 recall_train:0.9763 F1_train:0.9629 AUC_train:0.9892\n",
      "acc_val:0.8100 pre_val:0.7719 recall_val:0.8800 F1_val:0.822430 AUC_val:0.9064\n",
      "Epoch:0122\n",
      "acc_train:0.9522 pre_train:0.9433 recall_train:0.9656 F1_train:0.9543 AUC_train:0.9861\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.9020\n",
      "Epoch:0123\n",
      "acc_train:0.9500 pre_train:0.9321 recall_train:0.9742 F1_train:0.9527 AUC_train:0.9898\n",
      "acc_val:0.8100 pre_val:0.7627 recall_val:0.9000 F1_val:0.825688 AUC_val:0.9020\n",
      "Epoch:0124\n",
      "acc_train:0.9556 pre_train:0.9400 recall_train:0.9763 F1_train:0.9578 AUC_train:0.9898\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9098\n",
      "Epoch:0125\n",
      "acc_train:0.9656 pre_train:0.9483 recall_train:0.9871 F1_train:0.9673 AUC_train:0.9908\n",
      "acc_val:0.8400 pre_val:0.8148 recall_val:0.8800 F1_val:0.846154 AUC_val:0.9138\n",
      "Epoch:0126\n",
      "acc_train:0.9567 pre_train:0.9401 recall_train:0.9785 F1_train:0.9589 AUC_train:0.9892\n",
      "acc_val:0.8200 pre_val:0.7857 recall_val:0.8800 F1_val:0.830189 AUC_val:0.9130\n",
      "Epoch:0127\n",
      "acc_train:0.9789 pre_train:0.9646 recall_train:0.9957 F1_train:0.9799 AUC_train:0.9959\n",
      "acc_val:0.8000 pre_val:0.7586 recall_val:0.8800 F1_val:0.814815 AUC_val:0.9116\n",
      "Epoch:0128\n",
      "acc_train:0.9633 pre_train:0.9426 recall_train:0.9892 F1_train:0.9654 AUC_train:0.9896\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9090\n",
      "Epoch:0129\n",
      "acc_train:0.9656 pre_train:0.9521 recall_train:0.9828 F1_train:0.9672 AUC_train:0.9933\n",
      "acc_val:0.8400 pre_val:0.7931 recall_val:0.9200 F1_val:0.851852 AUC_val:0.9108\n",
      "Epoch:0130\n",
      "acc_train:0.9567 pre_train:0.9365 recall_train:0.9828 F1_train:0.9591 AUC_train:0.9873\n",
      "acc_val:0.8400 pre_val:0.7931 recall_val:0.9200 F1_val:0.851852 AUC_val:0.9136\n",
      "Epoch:0131\n",
      "acc_train:0.9678 pre_train:0.9561 recall_train:0.9828 F1_train:0.9692 AUC_train:0.9901\n",
      "acc_val:0.8500 pre_val:0.8070 recall_val:0.9200 F1_val:0.859813 AUC_val:0.9166\n",
      "Epoch:0132\n",
      "acc_train:0.9556 pre_train:0.9455 recall_train:0.9699 F1_train:0.9575 AUC_train:0.9904\n",
      "acc_val:0.8400 pre_val:0.7931 recall_val:0.9200 F1_val:0.851852 AUC_val:0.9178\n",
      "Early Stopping!!! epoch：131\n",
      " Starting the 1-4 Fold:：\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.5478 pre_train:0.6051 recall_train:0.3591 F1_train:0.4507 AUC_train:0.5943\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6232\n",
      "Epoch:0002\n",
      "acc_train:0.6133 pre_train:0.6822 recall_train:0.4710 F1_train:0.5573 AUC_train:0.6518\n",
      "acc_val:0.6500 pre_val:0.7419 recall_val:0.4600 F1_val:0.567901 AUC_val:0.6284\n",
      "Epoch:0003\n",
      "acc_train:0.5867 pre_train:0.6545 recall_train:0.4237 F1_train:0.5144 AUC_train:0.6357\n",
      "acc_val:0.6400 pre_val:0.7333 recall_val:0.4400 F1_val:0.550000 AUC_val:0.6140\n",
      "Epoch:0004\n",
      "acc_train:0.5744 pre_train:0.6314 recall_train:0.4237 F1_train:0.5071 AUC_train:0.6162\n",
      "acc_val:0.6400 pre_val:0.7500 recall_val:0.4200 F1_val:0.538462 AUC_val:0.6168\n",
      "Epoch:0005\n",
      "acc_train:0.6011 pre_train:0.6616 recall_train:0.4667 F1_train:0.5473 AUC_train:0.6392\n",
      "acc_val:0.6400 pre_val:0.7500 recall_val:0.4200 F1_val:0.538462 AUC_val:0.6184\n",
      "Epoch:0006\n",
      "acc_train:0.5744 pre_train:0.6020 recall_train:0.5204 F1_train:0.5582 AUC_train:0.6126\n",
      "acc_val:0.6400 pre_val:0.7333 recall_val:0.4400 F1_val:0.550000 AUC_val:0.6248\n",
      "Epoch:0007\n",
      "acc_train:0.5967 pre_train:0.6275 recall_train:0.5398 F1_train:0.5803 AUC_train:0.6410\n",
      "acc_val:0.6500 pre_val:0.7419 recall_val:0.4600 F1_val:0.567901 AUC_val:0.6512\n",
      "Epoch:0008\n",
      "acc_train:0.6011 pre_train:0.6387 recall_train:0.5247 F1_train:0.5762 AUC_train:0.6570\n",
      "acc_val:0.6800 pre_val:0.7647 recall_val:0.5200 F1_val:0.619048 AUC_val:0.6820\n",
      "Epoch:0009\n",
      "acc_train:0.6322 pre_train:0.6948 recall_train:0.5140 F1_train:0.5909 AUC_train:0.6679\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.6916\n",
      "Epoch:0010\n",
      "acc_train:0.6156 pre_train:0.6441 recall_train:0.5720 F1_train:0.6059 AUC_train:0.6592\n",
      "acc_val:0.6500 pre_val:0.6923 recall_val:0.5400 F1_val:0.606742 AUC_val:0.7040\n",
      "Epoch:0011\n",
      "acc_train:0.6278 pre_train:0.6498 recall_train:0.6065 F1_train:0.6274 AUC_train:0.6911\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6960\n",
      "Epoch:0012\n",
      "acc_train:0.6011 pre_train:0.6152 recall_train:0.6086 F1_train:0.6119 AUC_train:0.6636\n",
      "acc_val:0.6600 pre_val:0.7105 recall_val:0.5400 F1_val:0.613636 AUC_val:0.7068\n",
      "Epoch:0013\n",
      "acc_train:0.6311 pre_train:0.6515 recall_train:0.6151 F1_train:0.6327 AUC_train:0.6756\n",
      "acc_val:0.6600 pre_val:0.7105 recall_val:0.5400 F1_val:0.613636 AUC_val:0.7156\n",
      "Epoch:0014\n",
      "acc_train:0.6011 pre_train:0.6100 recall_train:0.6323 F1_train:0.6209 AUC_train:0.6815\n",
      "acc_val:0.6600 pre_val:0.7105 recall_val:0.5400 F1_val:0.613636 AUC_val:0.7232\n",
      "Epoch:0015\n",
      "acc_train:0.5867 pre_train:0.5987 recall_train:0.6065 F1_train:0.6026 AUC_train:0.6523\n",
      "acc_val:0.6600 pre_val:0.7105 recall_val:0.5400 F1_val:0.613636 AUC_val:0.7272\n",
      "Epoch:0016\n",
      "acc_train:0.6211 pre_train:0.6348 recall_train:0.6280 F1_train:0.6314 AUC_train:0.7009\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.7304\n",
      "Epoch:0017\n",
      "acc_train:0.6489 pre_train:0.6987 recall_train:0.5634 F1_train:0.6238 AUC_train:0.6959\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7349\n",
      "Epoch:0018\n",
      "acc_train:0.6244 pre_train:0.6309 recall_train:0.6581 F1_train:0.6442 AUC_train:0.7154\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7428\n",
      "Epoch:0019\n",
      "acc_train:0.6422 pre_train:0.6792 recall_train:0.5828 F1_train:0.6273 AUC_train:0.6998\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7464\n",
      "Epoch:0020\n",
      "acc_train:0.6656 pre_train:0.7228 recall_train:0.5720 F1_train:0.6387 AUC_train:0.7171\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7492\n",
      "Epoch:0021\n",
      "acc_train:0.6744 pre_train:0.7228 recall_train:0.6000 F1_train:0.6557 AUC_train:0.7375\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7580\n",
      "Epoch:0022\n",
      "acc_train:0.6844 pre_train:0.7685 recall_train:0.5570 F1_train:0.6459 AUC_train:0.7426\n",
      "acc_val:0.6800 pre_val:0.7647 recall_val:0.5200 F1_val:0.619048 AUC_val:0.7636\n",
      "Epoch:0023\n",
      "acc_train:0.6822 pre_train:0.7439 recall_train:0.5871 F1_train:0.6562 AUC_train:0.7519\n",
      "acc_val:0.7000 pre_val:0.8125 recall_val:0.5200 F1_val:0.634146 AUC_val:0.7696\n",
      "Epoch:0024\n",
      "acc_train:0.6989 pre_train:0.7513 recall_train:0.6237 F1_train:0.6816 AUC_train:0.7753\n",
      "acc_val:0.6900 pre_val:0.7879 recall_val:0.5200 F1_val:0.626506 AUC_val:0.7800\n",
      "Epoch:0025\n",
      "acc_train:0.6656 pre_train:0.7135 recall_train:0.5892 F1_train:0.6455 AUC_train:0.7592\n",
      "acc_val:0.6900 pre_val:0.7879 recall_val:0.5200 F1_val:0.626506 AUC_val:0.7892\n",
      "Epoch:0026\n",
      "acc_train:0.7111 pre_train:0.8233 recall_train:0.5613 F1_train:0.6675 AUC_train:0.7851\n",
      "acc_val:0.7000 pre_val:0.8125 recall_val:0.5200 F1_val:0.634146 AUC_val:0.7952\n",
      "Epoch:0027\n",
      "acc_train:0.7078 pre_train:0.7672 recall_train:0.6237 F1_train:0.6880 AUC_train:0.8050\n",
      "acc_val:0.7000 pre_val:0.8125 recall_val:0.5200 F1_val:0.634146 AUC_val:0.7896\n",
      "Epoch:0028\n",
      "acc_train:0.7267 pre_train:0.8349 recall_train:0.5871 F1_train:0.6894 AUC_train:0.7975\n",
      "acc_val:0.7100 pre_val:0.8387 recall_val:0.5200 F1_val:0.641975 AUC_val:0.7940\n",
      "Epoch:0029\n",
      "acc_train:0.7078 pre_train:0.8279 recall_train:0.5484 F1_train:0.6598 AUC_train:0.7828\n",
      "acc_val:0.7100 pre_val:0.8387 recall_val:0.5200 F1_val:0.641975 AUC_val:0.7952\n",
      "Epoch:0030\n",
      "acc_train:0.6967 pre_train:0.7981 recall_train:0.5527 F1_train:0.6531 AUC_train:0.8019\n",
      "acc_val:0.7200 pre_val:0.8667 recall_val:0.5200 F1_val:0.650000 AUC_val:0.7972\n",
      "Epoch:0031\n",
      "acc_train:0.7211 pre_train:0.8302 recall_train:0.5785 F1_train:0.6819 AUC_train:0.8098\n",
      "acc_val:0.7000 pre_val:0.8571 recall_val:0.4800 F1_val:0.615385 AUC_val:0.7836\n",
      "Epoch:0032\n",
      "acc_train:0.7411 pre_train:0.8867 recall_train:0.5720 F1_train:0.6954 AUC_train:0.8478\n",
      "acc_val:0.6900 pre_val:0.8519 recall_val:0.4600 F1_val:0.597403 AUC_val:0.7952\n",
      "Epoch:0033\n",
      "acc_train:0.7367 pre_train:0.8800 recall_train:0.5677 F1_train:0.6902 AUC_train:0.8257\n",
      "acc_val:0.6900 pre_val:0.8519 recall_val:0.4600 F1_val:0.597403 AUC_val:0.7942\n",
      "Epoch:0034\n",
      "acc_train:0.7256 pre_train:0.8733 recall_train:0.5484 F1_train:0.6737 AUC_train:0.8546\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.8036\n",
      "Epoch:0035\n",
      "acc_train:0.7389 pre_train:0.8912 recall_train:0.5634 F1_train:0.6904 AUC_train:0.8502\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.8092\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0036\n",
      "acc_train:0.7467 pre_train:0.9017 recall_train:0.5720 F1_train:0.7000 AUC_train:0.8770\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.8176\n",
      "Epoch:0037\n",
      "acc_train:0.7367 pre_train:0.8851 recall_train:0.5634 F1_train:0.6886 AUC_train:0.8959\n",
      "acc_val:0.6800 pre_val:0.8214 recall_val:0.4600 F1_val:0.589744 AUC_val:0.8324\n",
      "Epoch:0038\n",
      "acc_train:0.7133 pre_train:0.8371 recall_train:0.5527 F1_train:0.6658 AUC_train:0.8694\n",
      "acc_val:0.7000 pre_val:0.8333 recall_val:0.5000 F1_val:0.625000 AUC_val:0.8708\n",
      "Epoch:0039\n",
      "acc_train:0.7489 pre_train:0.8842 recall_train:0.5914 F1_train:0.7088 AUC_train:0.9004\n",
      "acc_val:0.7100 pre_val:0.8387 recall_val:0.5200 F1_val:0.641975 AUC_val:0.8908\n",
      "Epoch:0040\n",
      "acc_train:0.8011 pre_train:0.7782 recall_train:0.8602 F1_train:0.8172 AUC_train:0.9056\n",
      "acc_val:0.7000 pre_val:0.8125 recall_val:0.5200 F1_val:0.634146 AUC_val:0.8928\n",
      "Epoch:0041\n",
      "acc_train:0.8033 pre_train:0.7857 recall_train:0.8516 F1_train:0.8173 AUC_train:0.9091\n",
      "acc_val:0.7000 pre_val:0.8125 recall_val:0.5200 F1_val:0.634146 AUC_val:0.8912\n",
      "Epoch:0042\n",
      "acc_train:0.8411 pre_train:0.8340 recall_train:0.8645 F1_train:0.8490 AUC_train:0.9257\n",
      "acc_val:0.7000 pre_val:0.8125 recall_val:0.5200 F1_val:0.634146 AUC_val:0.8940\n",
      "Epoch:0043\n",
      "acc_train:0.7544 pre_train:0.8961 recall_train:0.5935 F1_train:0.7141 AUC_train:0.9150\n",
      "acc_val:0.7000 pre_val:0.8125 recall_val:0.5200 F1_val:0.634146 AUC_val:0.8966\n",
      "Epoch:0044\n",
      "acc_train:0.8611 pre_train:0.8712 recall_train:0.8581 F1_train:0.8646 AUC_train:0.9220\n",
      "acc_val:0.6900 pre_val:0.7879 recall_val:0.5200 F1_val:0.626506 AUC_val:0.8812\n",
      "Epoch:0045\n",
      "acc_train:0.8322 pre_train:0.8008 recall_train:0.8989 F1_train:0.8470 AUC_train:0.9171\n",
      "acc_val:0.6900 pre_val:0.7879 recall_val:0.5200 F1_val:0.626506 AUC_val:0.8820\n",
      "Epoch:0046\n",
      "acc_train:0.8278 pre_train:0.8370 recall_train:0.8280 F1_train:0.8324 AUC_train:0.9057\n",
      "acc_val:0.6900 pre_val:0.7879 recall_val:0.5200 F1_val:0.626506 AUC_val:0.8934\n",
      "Epoch:0047\n",
      "acc_train:0.8567 pre_train:0.8320 recall_train:0.9054 F1_train:0.8671 AUC_train:0.9290\n",
      "acc_val:0.8600 pre_val:0.8462 recall_val:0.8800 F1_val:0.862745 AUC_val:0.9058\n",
      "Epoch:0048\n",
      "acc_train:0.8522 pre_train:0.8281 recall_train:0.9011 F1_train:0.8630 AUC_train:0.9286\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9074\n",
      "Epoch:0049\n",
      "acc_train:0.8622 pre_train:0.8235 recall_train:0.9333 F1_train:0.8750 AUC_train:0.9463\n",
      "acc_val:0.8200 pre_val:0.7857 recall_val:0.8800 F1_val:0.830189 AUC_val:0.9164\n",
      "Epoch:0050\n",
      "acc_train:0.8422 pre_train:0.7985 recall_train:0.9290 F1_train:0.8588 AUC_train:0.9361\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9152\n",
      "Epoch:0051\n",
      "acc_train:0.8411 pre_train:0.7865 recall_train:0.9505 F1_train:0.8608 AUC_train:0.9509\n",
      "acc_val:0.8400 pre_val:0.7931 recall_val:0.9200 F1_val:0.851852 AUC_val:0.9090\n",
      "Epoch:0052\n",
      "acc_train:0.8567 pre_train:0.8100 recall_train:0.9441 F1_train:0.8719 AUC_train:0.9407\n",
      "acc_val:0.8400 pre_val:0.7833 recall_val:0.9400 F1_val:0.854545 AUC_val:0.9110\n",
      "Epoch:0053\n",
      "acc_train:0.8589 pre_train:0.8084 recall_train:0.9527 F1_train:0.8746 AUC_train:0.9471\n",
      "acc_val:0.8100 pre_val:0.7460 recall_val:0.9400 F1_val:0.831858 AUC_val:0.9000\n",
      "Epoch:0054\n",
      "acc_train:0.8367 pre_train:0.7944 recall_train:0.9226 F1_train:0.8537 AUC_train:0.9438\n",
      "acc_val:0.8200 pre_val:0.7581 recall_val:0.9400 F1_val:0.839286 AUC_val:0.8948\n",
      "Epoch:0055\n",
      "acc_train:0.8456 pre_train:0.7985 recall_train:0.9376 F1_train:0.8625 AUC_train:0.9377\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9180\n",
      "Epoch:0056\n",
      "acc_train:0.8433 pre_train:0.7893 recall_train:0.9505 F1_train:0.8624 AUC_train:0.9425\n",
      "acc_val:0.8300 pre_val:0.7619 recall_val:0.9600 F1_val:0.849558 AUC_val:0.9296\n",
      "Epoch:0057\n",
      "acc_train:0.8489 pre_train:0.7912 recall_train:0.9613 F1_train:0.8680 AUC_train:0.9582\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9308\n",
      "Epoch:0058\n",
      "acc_train:0.8567 pre_train:0.8111 recall_train:0.9419 F1_train:0.8716 AUC_train:0.9457\n",
      "acc_val:0.8500 pre_val:0.7869 recall_val:0.9600 F1_val:0.864865 AUC_val:0.9306\n",
      "Epoch:0059\n",
      "acc_train:0.8811 pre_train:0.8429 recall_train:0.9462 F1_train:0.8916 AUC_train:0.9576\n",
      "acc_val:0.8500 pre_val:0.7869 recall_val:0.9600 F1_val:0.864865 AUC_val:0.9314\n",
      "Epoch:0060\n",
      "acc_train:0.8811 pre_train:0.8340 recall_train:0.9613 F1_train:0.8931 AUC_train:0.9573\n",
      "acc_val:0.8500 pre_val:0.7869 recall_val:0.9600 F1_val:0.864865 AUC_val:0.9310\n",
      "Epoch:0061\n",
      "acc_train:0.8889 pre_train:0.8437 recall_train:0.9634 F1_train:0.8996 AUC_train:0.9626\n",
      "acc_val:0.8700 pre_val:0.8033 recall_val:0.9800 F1_val:0.882883 AUC_val:0.9290\n",
      "Epoch:0062\n",
      "acc_train:0.8756 pre_train:0.8336 recall_train:0.9484 F1_train:0.8873 AUC_train:0.9574\n",
      "acc_val:0.8700 pre_val:0.8136 recall_val:0.9600 F1_val:0.880734 AUC_val:0.9298\n",
      "Epoch:0063\n",
      "acc_train:0.9133 pre_train:0.8862 recall_train:0.9548 F1_train:0.9193 AUC_train:0.9653\n",
      "acc_val:0.8700 pre_val:0.8136 recall_val:0.9600 F1_val:0.880734 AUC_val:0.9314\n",
      "Epoch:0064\n",
      "acc_train:0.8822 pre_train:0.8330 recall_train:0.9656 F1_train:0.8944 AUC_train:0.9630\n",
      "acc_val:0.8700 pre_val:0.8246 recall_val:0.9400 F1_val:0.878505 AUC_val:0.9288\n",
      "Epoch:0065\n",
      "acc_train:0.8889 pre_train:0.8558 recall_train:0.9441 F1_train:0.8978 AUC_train:0.9652\n",
      "acc_val:0.8700 pre_val:0.8246 recall_val:0.9400 F1_val:0.878505 AUC_val:0.9298\n",
      "Epoch:0066\n",
      "acc_train:0.8844 pre_train:0.8478 recall_train:0.9462 F1_train:0.8943 AUC_train:0.9636\n",
      "acc_val:0.8600 pre_val:0.8000 recall_val:0.9600 F1_val:0.872727 AUC_val:0.9324\n",
      "Epoch:0067\n",
      "acc_train:0.8878 pre_train:0.8583 recall_train:0.9376 F1_train:0.8962 AUC_train:0.9593\n",
      "acc_val:0.8600 pre_val:0.7903 recall_val:0.9800 F1_val:0.875000 AUC_val:0.9304\n",
      "Epoch:0068\n",
      "acc_train:0.9044 pre_train:0.8723 recall_train:0.9548 F1_train:0.9117 AUC_train:0.9657\n",
      "acc_val:0.8600 pre_val:0.7903 recall_val:0.9800 F1_val:0.875000 AUC_val:0.9332\n",
      "Epoch:0069\n",
      "acc_train:0.9067 pre_train:0.8757 recall_train:0.9548 F1_train:0.9136 AUC_train:0.9615\n",
      "acc_val:0.8600 pre_val:0.7903 recall_val:0.9800 F1_val:0.875000 AUC_val:0.9356\n",
      "Epoch:0070\n",
      "acc_train:0.8967 pre_train:0.8619 recall_train:0.9527 F1_train:0.9050 AUC_train:0.9656\n",
      "acc_val:0.8700 pre_val:0.8033 recall_val:0.9800 F1_val:0.882883 AUC_val:0.9352\n",
      "Epoch:0071\n",
      "acc_train:0.9033 pre_train:0.8593 recall_train:0.9720 F1_train:0.9122 AUC_train:0.9755\n",
      "acc_val:0.8700 pre_val:0.8033 recall_val:0.9800 F1_val:0.882883 AUC_val:0.9412\n",
      "Epoch:0072\n",
      "acc_train:0.8911 pre_train:0.8469 recall_train:0.9634 F1_train:0.9014 AUC_train:0.9702\n",
      "acc_val:0.8500 pre_val:0.7869 recall_val:0.9600 F1_val:0.864865 AUC_val:0.9398\n",
      "Epoch:0073\n",
      "acc_train:0.9011 pre_train:0.8574 recall_train:0.9699 F1_train:0.9102 AUC_train:0.9676\n",
      "acc_val:0.8600 pre_val:0.7812 recall_val:1.0000 F1_val:0.877193 AUC_val:0.9494\n",
      "Epoch:0074\n",
      "acc_train:0.9033 pre_train:0.8539 recall_train:0.9806 F1_train:0.9129 AUC_train:0.9766\n",
      "acc_val:0.8700 pre_val:0.7937 recall_val:1.0000 F1_val:0.884956 AUC_val:0.9490\n",
      "Epoch:0075\n",
      "acc_train:0.9189 pre_train:0.8843 recall_train:0.9699 F1_train:0.9251 AUC_train:0.9739\n",
      "acc_val:0.8700 pre_val:0.8136 recall_val:0.9600 F1_val:0.880734 AUC_val:0.9490\n",
      "Epoch:0076\n",
      "acc_train:0.9211 pre_train:0.8893 recall_train:0.9677 F1_train:0.9269 AUC_train:0.9747\n",
      "acc_val:0.8700 pre_val:0.8136 recall_val:0.9600 F1_val:0.880734 AUC_val:0.9436\n",
      "Epoch:0077\n",
      "acc_train:0.9289 pre_train:0.9117 recall_train:0.9548 F1_train:0.9328 AUC_train:0.9759\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9468\n",
      "Epoch:0078\n",
      "acc_train:0.9356 pre_train:0.9284 recall_train:0.9484 F1_train:0.9383 AUC_train:0.9745\n",
      "acc_val:0.8800 pre_val:0.8800 recall_val:0.8800 F1_val:0.880000 AUC_val:0.9462\n",
      "Epoch:0079\n",
      "acc_train:0.9211 pre_train:0.9070 recall_train:0.9441 F1_train:0.9252 AUC_train:0.9734\n",
      "acc_val:0.8500 pre_val:0.8723 recall_val:0.8200 F1_val:0.845361 AUC_val:0.9210\n",
      "Epoch:0080\n",
      "acc_train:0.8967 pre_train:0.8765 recall_train:0.9312 F1_train:0.9030 AUC_train:0.9678\n",
      "acc_val:0.8400 pre_val:0.8542 recall_val:0.8200 F1_val:0.836735 AUC_val:0.9064\n",
      "Epoch:0081\n",
      "acc_train:0.9133 pre_train:0.8925 recall_train:0.9462 F1_train:0.9186 AUC_train:0.9680\n",
      "acc_val:0.8500 pre_val:0.8431 recall_val:0.8600 F1_val:0.851485 AUC_val:0.9138\n",
      "Epoch:0082\n",
      "acc_train:0.9122 pre_train:0.8814 recall_train:0.9591 F1_train:0.9186 AUC_train:0.9671\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0083\n",
      "acc_train:0.9211 pre_train:0.8863 recall_train:0.9720 F1_train:0.9272 AUC_train:0.9780\n",
      "acc_val:0.8300 pre_val:0.7705 recall_val:0.9400 F1_val:0.846847 AUC_val:0.9378\n",
      "Epoch:0084\n",
      "acc_train:0.9278 pre_train:0.8953 recall_train:0.9742 F1_train:0.9331 AUC_train:0.9794\n",
      "acc_val:0.8200 pre_val:0.7581 recall_val:0.9400 F1_val:0.839286 AUC_val:0.9470\n",
      "Epoch:0085\n",
      "acc_train:0.9167 pre_train:0.8869 recall_train:0.9613 F1_train:0.9226 AUC_train:0.9783\n",
      "acc_val:0.8300 pre_val:0.7705 recall_val:0.9400 F1_val:0.846847 AUC_val:0.9466\n",
      "Epoch:0086\n",
      "acc_train:0.9244 pre_train:0.8978 recall_train:0.9634 F1_train:0.9295 AUC_train:0.9748\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9562\n",
      "Epoch:0087\n",
      "acc_train:0.9278 pre_train:0.8968 recall_train:0.9720 F1_train:0.9329 AUC_train:0.9785\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9508\n",
      "Epoch:0088\n",
      "acc_train:0.9322 pre_train:0.9089 recall_train:0.9656 F1_train:0.9364 AUC_train:0.9749\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9444\n",
      "Epoch:0089\n",
      "acc_train:0.9256 pre_train:0.8964 recall_train:0.9677 F1_train:0.9307 AUC_train:0.9758\n",
      "acc_val:0.8400 pre_val:0.7833 recall_val:0.9400 F1_val:0.854545 AUC_val:0.9396\n",
      "Epoch:0090\n",
      "acc_train:0.9233 pre_train:0.8976 recall_train:0.9613 F1_train:0.9283 AUC_train:0.9822\n",
      "acc_val:0.8200 pre_val:0.7667 recall_val:0.9200 F1_val:0.836364 AUC_val:0.9384\n",
      "Epoch:0091\n",
      "acc_train:0.9222 pre_train:0.8865 recall_train:0.9742 F1_train:0.9283 AUC_train:0.9803\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9388\n",
      "Epoch:0092\n",
      "acc_train:0.9256 pre_train:0.9289 recall_train:0.9269 F1_train:0.9279 AUC_train:0.9768\n",
      "acc_val:0.8600 pre_val:0.8333 recall_val:0.9000 F1_val:0.865385 AUC_val:0.9298\n",
      "Epoch:0093\n",
      "acc_train:0.9300 pre_train:0.8957 recall_train:0.9785 F1_train:0.9353 AUC_train:0.9850\n",
      "acc_val:0.8600 pre_val:0.8333 recall_val:0.9000 F1_val:0.865385 AUC_val:0.9294\n",
      "Epoch:0094\n",
      "acc_train:0.9411 pre_train:0.9221 recall_train:0.9677 F1_train:0.9444 AUC_train:0.9889\n",
      "acc_val:0.8700 pre_val:0.8627 recall_val:0.8800 F1_val:0.871287 AUC_val:0.9278\n",
      "Epoch:0095\n",
      "acc_train:0.9322 pre_train:0.9106 recall_train:0.9634 F1_train:0.9363 AUC_train:0.9772\n",
      "acc_val:0.8700 pre_val:0.8627 recall_val:0.8800 F1_val:0.871287 AUC_val:0.9288\n",
      "Epoch:0096\n",
      "acc_train:0.9356 pre_train:0.9111 recall_train:0.9699 F1_train:0.9396 AUC_train:0.9840\n",
      "acc_val:0.8600 pre_val:0.8462 recall_val:0.8800 F1_val:0.862745 AUC_val:0.9332\n",
      "Epoch:0097\n",
      "acc_train:0.9389 pre_train:0.9253 recall_train:0.9591 F1_train:0.9419 AUC_train:0.9812\n",
      "acc_val:0.8500 pre_val:0.8431 recall_val:0.8600 F1_val:0.851485 AUC_val:0.9338\n",
      "Epoch:0098\n",
      "acc_train:0.9344 pre_train:0.9301 recall_train:0.9441 F1_train:0.9370 AUC_train:0.9860\n",
      "acc_val:0.8500 pre_val:0.8302 recall_val:0.8800 F1_val:0.854369 AUC_val:0.9332\n",
      "Early Stopping!!! epoch：97\n",
      " Starting the 1-5 Fold:：\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.4956 pre_train:0.5211 recall_train:0.2925 F1_train:0.3747 AUC_train:0.4738\n",
      "acc_val:0.4800 pre_val:0.4898 recall_val:0.9600 F1_val:0.648649 AUC_val:0.4216\n",
      "Epoch:0002\n",
      "acc_train:0.5478 pre_train:0.6306 recall_train:0.3011 F1_train:0.4076 AUC_train:0.5606\n",
      "acc_val:0.4800 pre_val:0.4898 recall_val:0.9600 F1_val:0.648649 AUC_val:0.4520\n",
      "Epoch:0003\n",
      "acc_train:0.5611 pre_train:0.6683 recall_train:0.2989 F1_train:0.4131 AUC_train:0.5619\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.9800 F1_val:0.662162 AUC_val:0.5024\n",
      "Epoch:0004\n",
      "acc_train:0.5533 pre_train:0.6329 recall_train:0.3226 F1_train:0.4274 AUC_train:0.5658\n",
      "acc_val:0.5300 pre_val:0.5181 recall_val:0.8600 F1_val:0.646617 AUC_val:0.5444\n",
      "Epoch:0005\n",
      "acc_train:0.5878 pre_train:0.6679 recall_train:0.4022 F1_train:0.5020 AUC_train:0.6102\n",
      "acc_val:0.5800 pre_val:0.5541 recall_val:0.8200 F1_val:0.661290 AUC_val:0.5828\n",
      "Epoch:0006\n",
      "acc_train:0.5633 pre_train:0.6417 recall_train:0.3505 F1_train:0.4534 AUC_train:0.5590\n",
      "acc_val:0.5100 pre_val:0.5088 recall_val:0.5800 F1_val:0.542056 AUC_val:0.5812\n",
      "Epoch:0007\n",
      "acc_train:0.5733 pre_train:0.6695 recall_train:0.3441 F1_train:0.4545 AUC_train:0.5935\n",
      "acc_val:0.5600 pre_val:0.5625 recall_val:0.5400 F1_val:0.551020 AUC_val:0.5952\n",
      "Epoch:0008\n",
      "acc_train:0.5956 pre_train:0.6797 recall_train:0.4108 F1_train:0.5121 AUC_train:0.6302\n",
      "acc_val:0.5700 pre_val:0.5814 recall_val:0.5000 F1_val:0.537634 AUC_val:0.6000\n",
      "Epoch:0009\n",
      "acc_train:0.5722 pre_train:0.6471 recall_train:0.3785 F1_train:0.4776 AUC_train:0.5989\n",
      "acc_val:0.6100 pre_val:0.6486 recall_val:0.4800 F1_val:0.551724 AUC_val:0.6008\n",
      "Epoch:0010\n",
      "acc_train:0.5900 pre_train:0.6655 recall_train:0.4151 F1_train:0.5113 AUC_train:0.6435\n",
      "acc_val:0.6200 pre_val:0.6667 recall_val:0.4800 F1_val:0.558140 AUC_val:0.6072\n",
      "Epoch:0011\n",
      "acc_train:0.5756 pre_train:0.6466 recall_train:0.3935 F1_train:0.4893 AUC_train:0.5944\n",
      "acc_val:0.6200 pre_val:0.6765 recall_val:0.4600 F1_val:0.547619 AUC_val:0.6184\n",
      "Epoch:0012\n",
      "acc_train:0.6122 pre_train:0.6883 recall_train:0.4559 F1_train:0.5485 AUC_train:0.6234\n",
      "acc_val:0.6100 pre_val:0.6774 recall_val:0.4200 F1_val:0.518519 AUC_val:0.6224\n",
      "Epoch:0013\n",
      "acc_train:0.6144 pre_train:0.6686 recall_train:0.5032 F1_train:0.5742 AUC_train:0.6354\n",
      "acc_val:0.6300 pre_val:0.6970 recall_val:0.4600 F1_val:0.554217 AUC_val:0.6404\n",
      "Epoch:0014\n",
      "acc_train:0.6133 pre_train:0.6918 recall_train:0.4538 F1_train:0.5481 AUC_train:0.6498\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.6612\n",
      "Epoch:0015\n",
      "acc_train:0.6278 pre_train:0.6533 recall_train:0.5957 F1_train:0.6232 AUC_train:0.6379\n",
      "acc_val:0.6200 pre_val:0.6034 recall_val:0.7000 F1_val:0.648148 AUC_val:0.6608\n",
      "Epoch:0016\n",
      "acc_train:0.6222 pre_train:0.7090 recall_train:0.4559 F1_train:0.5550 AUC_train:0.6865\n",
      "acc_val:0.6200 pre_val:0.5938 recall_val:0.7600 F1_val:0.666667 AUC_val:0.6680\n",
      "Epoch:0017\n",
      "acc_train:0.6011 pre_train:0.6352 recall_train:0.5355 F1_train:0.5811 AUC_train:0.6437\n",
      "acc_val:0.6200 pre_val:0.5968 recall_val:0.7400 F1_val:0.660714 AUC_val:0.6684\n",
      "Epoch:0018\n",
      "acc_train:0.5811 pre_train:0.5840 recall_train:0.6581 F1_train:0.6188 AUC_train:0.6030\n",
      "acc_val:0.5500 pre_val:0.5342 recall_val:0.7800 F1_val:0.634146 AUC_val:0.6736\n",
      "Epoch:0019\n",
      "acc_train:0.6189 pre_train:0.6517 recall_train:0.5634 F1_train:0.6044 AUC_train:0.6377\n",
      "acc_val:0.6200 pre_val:0.5968 recall_val:0.7400 F1_val:0.660714 AUC_val:0.6716\n",
      "Epoch:0020\n",
      "acc_train:0.6311 pre_train:0.6792 recall_train:0.5419 F1_train:0.6029 AUC_train:0.6719\n",
      "acc_val:0.6300 pre_val:0.6275 recall_val:0.6400 F1_val:0.633663 AUC_val:0.6784\n",
      "Epoch:0021\n",
      "acc_train:0.6356 pre_train:0.6817 recall_train:0.5527 F1_train:0.6105 AUC_train:0.6769\n",
      "acc_val:0.6200 pre_val:0.6200 recall_val:0.6200 F1_val:0.620000 AUC_val:0.6700\n",
      "Epoch:0022\n",
      "acc_train:0.6256 pre_train:0.7065 recall_train:0.4710 F1_train:0.5652 AUC_train:0.6691\n",
      "acc_val:0.6200 pre_val:0.6304 recall_val:0.5800 F1_val:0.604167 AUC_val:0.6724\n",
      "Epoch:0023\n",
      "acc_train:0.6078 pre_train:0.6918 recall_train:0.4344 F1_train:0.5337 AUC_train:0.6535\n",
      "acc_val:0.6500 pre_val:0.6923 recall_val:0.5400 F1_val:0.606742 AUC_val:0.6720\n",
      "Epoch:0024\n",
      "acc_train:0.6089 pre_train:0.6738 recall_train:0.4710 F1_train:0.5544 AUC_train:0.6495\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6712\n",
      "Epoch:0025\n",
      "acc_train:0.6189 pre_train:0.6614 recall_train:0.5376 F1_train:0.5931 AUC_train:0.6703\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.6732\n",
      "Epoch:0026\n",
      "acc_train:0.6400 pre_train:0.7067 recall_train:0.5183 F1_train:0.5980 AUC_train:0.6767\n",
      "acc_val:0.6500 pre_val:0.7143 recall_val:0.5000 F1_val:0.588235 AUC_val:0.6716\n",
      "Epoch:0027\n",
      "acc_train:0.6500 pre_train:0.7435 recall_train:0.4925 F1_train:0.5925 AUC_train:0.6956\n",
      "acc_val:0.6500 pre_val:0.7273 recall_val:0.4800 F1_val:0.578313 AUC_val:0.6688\n",
      "Epoch:0028\n",
      "acc_train:0.6644 pre_train:0.6783 recall_train:0.6667 F1_train:0.6725 AUC_train:0.7040\n",
      "acc_val:0.6500 pre_val:0.7143 recall_val:0.5000 F1_val:0.588235 AUC_val:0.6700\n",
      "Epoch:0029\n",
      "acc_train:0.6444 pre_train:0.7139 recall_train:0.5204 F1_train:0.6020 AUC_train:0.6921\n",
      "acc_val:0.6500 pre_val:0.7273 recall_val:0.4800 F1_val:0.578313 AUC_val:0.6744\n",
      "Epoch:0030\n",
      "acc_train:0.6167 pre_train:0.6875 recall_train:0.4731 F1_train:0.5605 AUC_train:0.6866\n",
      "acc_val:0.6400 pre_val:0.7188 recall_val:0.4600 F1_val:0.560976 AUC_val:0.6756\n",
      "Epoch:0031\n",
      "acc_train:0.6444 pre_train:0.7217 recall_train:0.5075 F1_train:0.5960 AUC_train:0.6923\n",
      "acc_val:0.6400 pre_val:0.7188 recall_val:0.4600 F1_val:0.560976 AUC_val:0.6740\n",
      "Epoch:0032\n",
      "acc_train:0.6456 pre_train:0.6984 recall_train:0.5527 F1_train:0.6170 AUC_train:0.7010\n",
      "acc_val:0.6400 pre_val:0.7188 recall_val:0.4600 F1_val:0.560976 AUC_val:0.6744\n",
      "Epoch:0033\n",
      "acc_train:0.6411 pre_train:0.7165 recall_train:0.5054 F1_train:0.5927 AUC_train:0.6987\n",
      "acc_val:0.6400 pre_val:0.7188 recall_val:0.4600 F1_val:0.560976 AUC_val:0.6756\n",
      "Epoch:0034\n",
      "acc_train:0.6211 pre_train:0.6925 recall_train:0.4796 F1_train:0.5667 AUC_train:0.6704\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6768\n",
      "Epoch:0035\n",
      "acc_train:0.6311 pre_train:0.7085 recall_train:0.4860 F1_train:0.5765 AUC_train:0.6912\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6804\n",
      "Epoch:0036\n",
      "acc_train:0.6178 pre_train:0.6828 recall_train:0.4860 F1_train:0.5678 AUC_train:0.6494\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6808\n",
      "Epoch:0037\n",
      "acc_train:0.6489 pre_train:0.7427 recall_train:0.4903 F1_train:0.5907 AUC_train:0.7206\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6832\n",
      "Epoch:0038\n",
      "acc_train:0.6467 pre_train:0.7442 recall_train:0.4817 F1_train:0.5849 AUC_train:0.6992\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0039\n",
      "acc_train:0.6400 pre_train:0.7423 recall_train:0.4645 F1_train:0.5714 AUC_train:0.7095\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6940\n",
      "Epoch:0040\n",
      "acc_train:0.6411 pre_train:0.7247 recall_train:0.4925 F1_train:0.5864 AUC_train:0.6998\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6988\n",
      "Epoch:0041\n",
      "acc_train:0.6367 pre_train:0.7226 recall_train:0.4817 F1_train:0.5781 AUC_train:0.6996\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6980\n",
      "Epoch:0042\n",
      "acc_train:0.6556 pre_train:0.7327 recall_train:0.5247 F1_train:0.6115 AUC_train:0.6961\n",
      "acc_val:0.6300 pre_val:0.7097 recall_val:0.4400 F1_val:0.543210 AUC_val:0.7008\n",
      "Epoch:0043\n",
      "acc_train:0.6611 pre_train:0.7817 recall_train:0.4774 F1_train:0.5928 AUC_train:0.7390\n",
      "acc_val:0.6300 pre_val:0.7097 recall_val:0.4400 F1_val:0.543210 AUC_val:0.7012\n",
      "Epoch:0044\n",
      "acc_train:0.6356 pre_train:0.7291 recall_train:0.4688 F1_train:0.5707 AUC_train:0.7097\n",
      "acc_val:0.6300 pre_val:0.7097 recall_val:0.4400 F1_val:0.543210 AUC_val:0.6956\n",
      "Epoch:0045\n",
      "acc_train:0.6778 pre_train:0.7869 recall_train:0.5161 F1_train:0.6234 AUC_train:0.7569\n",
      "acc_val:0.6300 pre_val:0.7097 recall_val:0.4400 F1_val:0.543210 AUC_val:0.6992\n",
      "Epoch:0046\n",
      "acc_train:0.6667 pre_train:0.7687 recall_train:0.5075 F1_train:0.6114 AUC_train:0.7517\n",
      "acc_val:0.6400 pre_val:0.7333 recall_val:0.4400 F1_val:0.550000 AUC_val:0.7012\n",
      "Epoch:0047\n",
      "acc_train:0.6700 pre_train:0.7530 recall_train:0.5376 F1_train:0.6274 AUC_train:0.7551\n",
      "acc_val:0.6300 pre_val:0.7241 recall_val:0.4200 F1_val:0.531646 AUC_val:0.7012\n",
      "Epoch:0048\n",
      "acc_train:0.6644 pre_train:0.7638 recall_train:0.5075 F1_train:0.6098 AUC_train:0.7487\n",
      "acc_val:0.6400 pre_val:0.7500 recall_val:0.4200 F1_val:0.538462 AUC_val:0.7100\n",
      "Epoch:0049\n",
      "acc_train:0.6844 pre_train:0.7768 recall_train:0.5462 F1_train:0.6414 AUC_train:0.7395\n",
      "acc_val:0.6400 pre_val:0.7500 recall_val:0.4200 F1_val:0.538462 AUC_val:0.7176\n",
      "Epoch:0050\n",
      "acc_train:0.6711 pre_train:0.7522 recall_train:0.5419 F1_train:0.6300 AUC_train:0.7456\n",
      "acc_val:0.6200 pre_val:0.7308 recall_val:0.3800 F1_val:0.500000 AUC_val:0.7152\n",
      "Epoch:0051\n",
      "acc_train:0.6733 pre_train:0.7478 recall_train:0.5548 F1_train:0.6370 AUC_train:0.7584\n",
      "acc_val:0.6100 pre_val:0.7200 recall_val:0.3600 F1_val:0.480000 AUC_val:0.7152\n",
      "Epoch:0052\n",
      "acc_train:0.6867 pre_train:0.7453 recall_train:0.5978 F1_train:0.6635 AUC_train:0.7601\n",
      "acc_val:0.6200 pre_val:0.7143 recall_val:0.4000 F1_val:0.512821 AUC_val:0.7032\n",
      "Epoch:0053\n",
      "acc_train:0.6789 pre_train:0.7328 recall_train:0.5957 F1_train:0.6572 AUC_train:0.7412\n",
      "acc_val:0.6100 pre_val:0.6897 recall_val:0.4000 F1_val:0.506329 AUC_val:0.7052\n",
      "Epoch:0054\n",
      "acc_train:0.6989 pre_train:0.7539 recall_train:0.6194 F1_train:0.6800 AUC_train:0.7707\n",
      "acc_val:0.6200 pre_val:0.7143 recall_val:0.4000 F1_val:0.512821 AUC_val:0.7048\n",
      "Epoch:0055\n",
      "acc_train:0.7100 pre_train:0.7727 recall_train:0.6215 F1_train:0.6889 AUC_train:0.7938\n",
      "acc_val:0.6300 pre_val:0.7241 recall_val:0.4200 F1_val:0.531646 AUC_val:0.7064\n",
      "Epoch:0056\n",
      "acc_train:0.6933 pre_train:0.7547 recall_train:0.6022 F1_train:0.6699 AUC_train:0.7682\n",
      "acc_val:0.6400 pre_val:0.7333 recall_val:0.4400 F1_val:0.550000 AUC_val:0.7084\n",
      "Epoch:0057\n",
      "acc_train:0.6911 pre_train:0.7467 recall_train:0.6086 F1_train:0.6706 AUC_train:0.7688\n",
      "acc_val:0.6400 pre_val:0.7333 recall_val:0.4400 F1_val:0.550000 AUC_val:0.7176\n",
      "Epoch:0058\n",
      "acc_train:0.6933 pre_train:0.7534 recall_train:0.6043 F1_train:0.6706 AUC_train:0.7658\n",
      "acc_val:0.6300 pre_val:0.7097 recall_val:0.4400 F1_val:0.543210 AUC_val:0.7276\n",
      "Epoch:0059\n",
      "acc_train:0.7267 pre_train:0.7717 recall_train:0.6688 F1_train:0.7166 AUC_train:0.8071\n",
      "acc_val:0.6600 pre_val:0.7857 recall_val:0.4400 F1_val:0.564103 AUC_val:0.7404\n",
      "Epoch:0060\n",
      "acc_train:0.7378 pre_train:0.8021 recall_train:0.6538 F1_train:0.7204 AUC_train:0.8244\n",
      "acc_val:0.6700 pre_val:0.8148 recall_val:0.4400 F1_val:0.571429 AUC_val:0.7524\n",
      "Epoch:0061\n",
      "acc_train:0.7411 pre_train:0.8102 recall_train:0.6516 F1_train:0.7223 AUC_train:0.8242\n",
      "acc_val:0.6600 pre_val:0.8077 recall_val:0.4200 F1_val:0.552632 AUC_val:0.7620\n",
      "Epoch:0062\n",
      "acc_train:0.7500 pre_train:0.8261 recall_train:0.6538 F1_train:0.7299 AUC_train:0.8300\n",
      "acc_val:0.6800 pre_val:0.8750 recall_val:0.4200 F1_val:0.567568 AUC_val:0.7832\n",
      "Epoch:0063\n",
      "acc_train:0.7478 pre_train:0.8324 recall_train:0.6409 F1_train:0.7242 AUC_train:0.8468\n",
      "acc_val:0.6900 pre_val:0.8800 recall_val:0.4400 F1_val:0.586667 AUC_val:0.7984\n",
      "Epoch:0064\n",
      "acc_train:0.7544 pre_train:0.8370 recall_train:0.6516 F1_train:0.7328 AUC_train:0.8519\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.8072\n",
      "Epoch:0065\n",
      "acc_train:0.7500 pre_train:0.8279 recall_train:0.6516 F1_train:0.7292 AUC_train:0.8503\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.8124\n",
      "Epoch:0066\n",
      "acc_train:0.7889 pre_train:0.9056 recall_train:0.6602 F1_train:0.7637 AUC_train:0.8883\n",
      "acc_val:0.7000 pre_val:0.9167 recall_val:0.4400 F1_val:0.594595 AUC_val:0.8168\n",
      "Epoch:0067\n",
      "acc_train:0.7756 pre_train:0.8683 recall_train:0.6667 F1_train:0.7543 AUC_train:0.8736\n",
      "acc_val:0.7000 pre_val:0.9167 recall_val:0.4400 F1_val:0.594595 AUC_val:0.8232\n",
      "Epoch:0068\n",
      "acc_train:0.7833 pre_train:0.8971 recall_train:0.6559 F1_train:0.7578 AUC_train:0.8924\n",
      "acc_val:0.7100 pre_val:0.8889 recall_val:0.4800 F1_val:0.623377 AUC_val:0.8292\n",
      "Epoch:0069\n",
      "acc_train:0.8000 pre_train:0.9179 recall_train:0.6731 F1_train:0.7767 AUC_train:0.8962\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.8348\n",
      "Epoch:0070\n",
      "acc_train:0.8044 pre_train:0.9048 recall_train:0.6946 F1_train:0.7859 AUC_train:0.8947\n",
      "acc_val:0.6900 pre_val:0.8519 recall_val:0.4600 F1_val:0.597403 AUC_val:0.8336\n",
      "Epoch:0071\n",
      "acc_train:0.8144 pre_train:0.8744 recall_train:0.7484 F1_train:0.8065 AUC_train:0.9092\n",
      "acc_val:0.6800 pre_val:0.8462 recall_val:0.4400 F1_val:0.578947 AUC_val:0.8248\n",
      "Epoch:0072\n",
      "acc_train:0.8167 pre_train:0.9121 recall_train:0.7140 F1_train:0.8010 AUC_train:0.8982\n",
      "acc_val:0.6600 pre_val:0.8333 recall_val:0.4000 F1_val:0.540541 AUC_val:0.8188\n",
      "Epoch:0073\n",
      "acc_train:0.8189 pre_train:0.9081 recall_train:0.7226 F1_train:0.8048 AUC_train:0.9029\n",
      "acc_val:0.6800 pre_val:0.8462 recall_val:0.4400 F1_val:0.578947 AUC_val:0.8272\n",
      "Epoch:0074\n",
      "acc_train:0.8456 pre_train:0.9267 recall_train:0.7613 F1_train:0.8359 AUC_train:0.9264\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.8384\n",
      "Epoch:0075\n",
      "acc_train:0.8411 pre_train:0.9259 recall_train:0.7527 F1_train:0.8304 AUC_train:0.9220\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.8564\n",
      "Epoch:0076\n",
      "acc_train:0.8300 pre_train:0.9171 recall_train:0.7376 F1_train:0.8176 AUC_train:0.9248\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.8716\n",
      "Epoch:0077\n",
      "acc_train:0.8456 pre_train:0.9528 recall_train:0.7376 F1_train:0.8315 AUC_train:0.9401\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.8772\n",
      "Epoch:0078\n",
      "acc_train:0.8467 pre_train:0.9554 recall_train:0.7376 F1_train:0.8325 AUC_train:0.9469\n",
      "acc_val:0.7100 pre_val:0.8889 recall_val:0.4800 F1_val:0.623377 AUC_val:0.8744\n",
      "Epoch:0079\n",
      "acc_train:0.8522 pre_train:0.9611 recall_train:0.7441 F1_train:0.8388 AUC_train:0.9501\n",
      "acc_val:0.7200 pre_val:0.8929 recall_val:0.5000 F1_val:0.641026 AUC_val:0.8708\n",
      "Epoch:0080\n",
      "acc_train:0.8611 pre_train:0.9645 recall_train:0.7591 F1_train:0.8496 AUC_train:0.9442\n",
      "acc_val:0.7300 pre_val:0.8966 recall_val:0.5200 F1_val:0.658228 AUC_val:0.8696\n",
      "Epoch:0081\n",
      "acc_train:0.8556 pre_train:0.9640 recall_train:0.7484 F1_train:0.8426 AUC_train:0.9517\n",
      "acc_val:0.7300 pre_val:0.8966 recall_val:0.5200 F1_val:0.658228 AUC_val:0.8652\n",
      "Epoch:0082\n",
      "acc_train:0.8800 pre_train:0.9710 recall_train:0.7914 F1_train:0.8720 AUC_train:0.9661\n",
      "acc_val:0.7100 pre_val:0.8621 recall_val:0.5000 F1_val:0.632911 AUC_val:0.8604\n",
      "Epoch:0083\n",
      "acc_train:0.8878 pre_train:0.9596 recall_train:0.8172 F1_train:0.8827 AUC_train:0.9624\n",
      "acc_val:0.7400 pre_val:0.8750 recall_val:0.5600 F1_val:0.682927 AUC_val:0.8552\n",
      "Epoch:0084\n",
      "acc_train:0.8800 pre_train:0.9519 recall_train:0.8086 F1_train:0.8744 AUC_train:0.9544\n",
      "acc_val:0.7300 pre_val:0.8286 recall_val:0.5800 F1_val:0.682353 AUC_val:0.8552\n",
      "Epoch:0085\n",
      "acc_train:0.8822 pre_train:0.9521 recall_train:0.8129 F1_train:0.8770 AUC_train:0.9630\n",
      "acc_val:0.7400 pre_val:0.8529 recall_val:0.5800 F1_val:0.690476 AUC_val:0.8628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0086\n",
      "acc_train:0.8856 pre_train:0.9548 recall_train:0.8172 F1_train:0.8806 AUC_train:0.9579\n",
      "acc_val:0.7400 pre_val:0.8750 recall_val:0.5600 F1_val:0.682927 AUC_val:0.8732\n",
      "Epoch:0087\n",
      "acc_train:0.9056 pre_train:0.9680 recall_train:0.8452 F1_train:0.9024 AUC_train:0.9722\n",
      "acc_val:0.7300 pre_val:0.8485 recall_val:0.5600 F1_val:0.674699 AUC_val:0.8792\n",
      "Epoch:0088\n",
      "acc_train:0.8889 pre_train:0.9740 recall_train:0.8065 F1_train:0.8824 AUC_train:0.9730\n",
      "acc_val:0.7300 pre_val:0.8485 recall_val:0.5600 F1_val:0.674699 AUC_val:0.8808\n",
      "Epoch:0089\n",
      "acc_train:0.8944 pre_train:0.9744 recall_train:0.8172 F1_train:0.8889 AUC_train:0.9744\n",
      "acc_val:0.7300 pre_val:0.8485 recall_val:0.5600 F1_val:0.674699 AUC_val:0.8808\n",
      "Epoch:0090\n",
      "acc_train:0.9067 pre_train:0.9727 recall_train:0.8430 F1_train:0.9032 AUC_train:0.9720\n",
      "acc_val:0.7400 pre_val:0.8529 recall_val:0.5800 F1_val:0.690476 AUC_val:0.8828\n",
      "Epoch:0091\n",
      "acc_train:0.8833 pre_train:0.9712 recall_train:0.7978 F1_train:0.8760 AUC_train:0.9785\n",
      "acc_val:0.7500 pre_val:0.8571 recall_val:0.6000 F1_val:0.705882 AUC_val:0.8880\n",
      "Epoch:0092\n",
      "acc_train:0.9056 pre_train:0.9774 recall_train:0.8366 F1_train:0.9015 AUC_train:0.9784\n",
      "acc_val:0.7600 pre_val:0.8611 recall_val:0.6200 F1_val:0.720930 AUC_val:0.8956\n",
      "Epoch:0093\n",
      "acc_train:0.9222 pre_train:0.9829 recall_train:0.8645 F1_train:0.9199 AUC_train:0.9796\n",
      "acc_val:0.7600 pre_val:0.8611 recall_val:0.6200 F1_val:0.720930 AUC_val:0.8964\n",
      "Epoch:0094\n",
      "acc_train:0.9244 pre_train:0.9830 recall_train:0.8688 F1_train:0.9224 AUC_train:0.9817\n",
      "acc_val:0.7700 pre_val:0.8649 recall_val:0.6400 F1_val:0.735632 AUC_val:0.8924\n",
      "Epoch:0095\n",
      "acc_train:0.9089 pre_train:0.9705 recall_train:0.8495 F1_train:0.9060 AUC_train:0.9790\n",
      "acc_val:0.7700 pre_val:0.8649 recall_val:0.6400 F1_val:0.735632 AUC_val:0.8932\n",
      "Epoch:0096\n",
      "acc_train:0.9189 pre_train:0.9712 recall_train:0.8688 F1_train:0.9171 AUC_train:0.9804\n",
      "acc_val:0.7700 pre_val:0.8649 recall_val:0.6400 F1_val:0.735632 AUC_val:0.8968\n",
      "Epoch:0097\n",
      "acc_train:0.9267 pre_train:0.9831 recall_train:0.8731 F1_train:0.9248 AUC_train:0.9840\n",
      "acc_val:0.7500 pre_val:0.8571 recall_val:0.6000 F1_val:0.705882 AUC_val:0.8992\n",
      "Epoch:0098\n",
      "acc_train:0.9167 pre_train:0.9688 recall_train:0.8667 F1_train:0.9149 AUC_train:0.9819\n",
      "acc_val:0.7500 pre_val:0.8571 recall_val:0.6000 F1_val:0.705882 AUC_val:0.9012\n",
      "Epoch:0099\n",
      "acc_train:0.9233 pre_train:0.9783 recall_train:0.8710 F1_train:0.9215 AUC_train:0.9868\n",
      "acc_val:0.7600 pre_val:0.8611 recall_val:0.6200 F1_val:0.720930 AUC_val:0.9024\n",
      "Epoch:0100\n",
      "acc_train:0.9156 pre_train:0.9732 recall_train:0.8602 F1_train:0.9132 AUC_train:0.9823\n",
      "acc_val:0.7600 pre_val:0.8611 recall_val:0.6200 F1_val:0.720930 AUC_val:0.9028\n",
      "Epoch:0101\n",
      "acc_train:0.9256 pre_train:0.9761 recall_train:0.8774 F1_train:0.9241 AUC_train:0.9855\n",
      "acc_val:0.7500 pre_val:0.8571 recall_val:0.6000 F1_val:0.705882 AUC_val:0.9036\n",
      "Epoch:0102\n",
      "acc_train:0.9156 pre_train:0.9779 recall_train:0.8559 F1_train:0.9128 AUC_train:0.9830\n",
      "acc_val:0.7500 pre_val:0.8571 recall_val:0.6000 F1_val:0.705882 AUC_val:0.9048\n",
      "Epoch:0103\n",
      "acc_train:0.9322 pre_train:0.9856 recall_train:0.8817 F1_train:0.9308 AUC_train:0.9872\n",
      "acc_val:0.7500 pre_val:0.8571 recall_val:0.6000 F1_val:0.705882 AUC_val:0.9064\n",
      "Epoch:0104\n",
      "acc_train:0.9311 pre_train:0.9741 recall_train:0.8903 F1_train:0.9303 AUC_train:0.9870\n",
      "acc_val:0.7400 pre_val:0.8750 recall_val:0.5600 F1_val:0.682927 AUC_val:0.9048\n",
      "Epoch:0105\n",
      "acc_train:0.9344 pre_train:0.9810 recall_train:0.8903 F1_train:0.9335 AUC_train:0.9884\n",
      "acc_val:0.7400 pre_val:0.8750 recall_val:0.5600 F1_val:0.682927 AUC_val:0.9028\n",
      "Epoch:0106\n",
      "acc_train:0.9322 pre_train:0.9879 recall_train:0.8796 F1_train:0.9306 AUC_train:0.9890\n",
      "acc_val:0.7400 pre_val:0.8750 recall_val:0.5600 F1_val:0.682927 AUC_val:0.9028\n",
      "Epoch:0107\n",
      "acc_train:0.9444 pre_train:0.9859 recall_train:0.9054 F1_train:0.9439 AUC_train:0.9903\n",
      "acc_val:0.7400 pre_val:0.8750 recall_val:0.5600 F1_val:0.682927 AUC_val:0.9032\n",
      "Epoch:0108\n",
      "acc_train:0.9389 pre_train:0.9881 recall_train:0.8925 F1_train:0.9379 AUC_train:0.9871\n",
      "acc_val:0.7600 pre_val:0.8824 recall_val:0.6000 F1_val:0.714286 AUC_val:0.9028\n",
      "Epoch:0109\n",
      "acc_train:0.9500 pre_train:0.9907 recall_train:0.9118 F1_train:0.9496 AUC_train:0.9900\n",
      "acc_val:0.7600 pre_val:0.8611 recall_val:0.6200 F1_val:0.720930 AUC_val:0.8972\n",
      "Epoch:0110\n",
      "acc_train:0.9344 pre_train:0.9833 recall_train:0.8882 F1_train:0.9333 AUC_train:0.9899\n",
      "acc_val:0.7900 pre_val:0.8718 recall_val:0.6800 F1_val:0.764045 AUC_val:0.8948\n",
      "Epoch:0111\n",
      "acc_train:0.9433 pre_train:0.9792 recall_train:0.9097 F1_train:0.9431 AUC_train:0.9864\n",
      "acc_val:0.7900 pre_val:0.8718 recall_val:0.6800 F1_val:0.764045 AUC_val:0.8984\n",
      "Epoch:0112\n",
      "acc_train:0.9444 pre_train:0.9792 recall_train:0.9118 F1_train:0.9443 AUC_train:0.9912\n",
      "acc_val:0.8000 pre_val:0.8750 recall_val:0.7000 F1_val:0.777778 AUC_val:0.8964\n",
      "Epoch:0113\n",
      "acc_train:0.9389 pre_train:0.9576 recall_train:0.9226 F1_train:0.9398 AUC_train:0.9894\n",
      "acc_val:0.8000 pre_val:0.8750 recall_val:0.7000 F1_val:0.777778 AUC_val:0.8956\n",
      "Epoch:0114\n",
      "acc_train:0.9444 pre_train:0.9684 recall_train:0.9226 F1_train:0.9449 AUC_train:0.9898\n",
      "acc_val:0.7800 pre_val:0.8500 recall_val:0.6800 F1_val:0.755556 AUC_val:0.8960\n",
      "Epoch:0115\n",
      "acc_train:0.9433 pre_train:0.9683 recall_train:0.9204 F1_train:0.9438 AUC_train:0.9890\n",
      "acc_val:0.7800 pre_val:0.8684 recall_val:0.6600 F1_val:0.750000 AUC_val:0.8960\n",
      "Epoch:0116\n",
      "acc_train:0.9478 pre_train:0.9729 recall_train:0.9247 F1_train:0.9482 AUC_train:0.9886\n",
      "acc_val:0.7700 pre_val:0.8649 recall_val:0.6400 F1_val:0.735632 AUC_val:0.8988\n",
      "Epoch:0117\n",
      "acc_train:0.9467 pre_train:0.9771 recall_train:0.9183 F1_train:0.9468 AUC_train:0.9892\n",
      "acc_val:0.7400 pre_val:0.8529 recall_val:0.5800 F1_val:0.690476 AUC_val:0.9000\n",
      "Epoch:0118\n",
      "acc_train:0.9422 pre_train:0.9791 recall_train:0.9075 F1_train:0.9420 AUC_train:0.9911\n",
      "acc_val:0.7400 pre_val:0.8529 recall_val:0.5800 F1_val:0.690476 AUC_val:0.9064\n",
      "Epoch:0119\n",
      "acc_train:0.9522 pre_train:0.9774 recall_train:0.9290 F1_train:0.9526 AUC_train:0.9903\n",
      "acc_val:0.7400 pre_val:0.8750 recall_val:0.5600 F1_val:0.682927 AUC_val:0.9060\n",
      "Epoch:0120\n",
      "acc_train:0.9533 pre_train:0.9840 recall_train:0.9247 F1_train:0.9534 AUC_train:0.9947\n",
      "acc_val:0.7300 pre_val:0.8710 recall_val:0.5400 F1_val:0.666667 AUC_val:0.9064\n",
      "Epoch:0121\n",
      "acc_train:0.9578 pre_train:0.9819 recall_train:0.9355 F1_train:0.9581 AUC_train:0.9942\n",
      "acc_val:0.7200 pre_val:0.8667 recall_val:0.5200 F1_val:0.650000 AUC_val:0.9060\n",
      "Epoch:0122\n",
      "acc_train:0.9367 pre_train:0.9744 recall_train:0.9011 F1_train:0.9363 AUC_train:0.9879\n",
      "acc_val:0.7200 pre_val:0.8667 recall_val:0.5200 F1_val:0.650000 AUC_val:0.9048\n",
      "Epoch:0123\n",
      "acc_train:0.9567 pre_train:0.9841 recall_train:0.9312 F1_train:0.9569 AUC_train:0.9940\n",
      "acc_val:0.7200 pre_val:0.8667 recall_val:0.5200 F1_val:0.650000 AUC_val:0.9060\n",
      "Epoch:0124\n",
      "acc_train:0.9622 pre_train:0.9909 recall_train:0.9355 F1_train:0.9624 AUC_train:0.9936\n",
      "acc_val:0.7200 pre_val:0.8438 recall_val:0.5400 F1_val:0.658537 AUC_val:0.9060\n",
      "Epoch:0125\n",
      "acc_train:0.9444 pre_train:0.9814 recall_train:0.9097 F1_train:0.9442 AUC_train:0.9893\n",
      "acc_val:0.7100 pre_val:0.8387 recall_val:0.5200 F1_val:0.641975 AUC_val:0.9084\n",
      "Epoch:0126\n",
      "acc_train:0.9533 pre_train:0.9862 recall_train:0.9226 F1_train:0.9533 AUC_train:0.9935\n",
      "acc_val:0.7200 pre_val:0.8438 recall_val:0.5400 F1_val:0.658537 AUC_val:0.9020\n",
      "Epoch:0127\n",
      "acc_train:0.9633 pre_train:0.9909 recall_train:0.9376 F1_train:0.9635 AUC_train:0.9956\n",
      "acc_val:0.7300 pre_val:0.8485 recall_val:0.5600 F1_val:0.674699 AUC_val:0.8980\n",
      "Epoch:0128\n",
      "acc_train:0.9567 pre_train:0.9819 recall_train:0.9333 F1_train:0.9570 AUC_train:0.9925\n",
      "acc_val:0.7400 pre_val:0.8750 recall_val:0.5600 F1_val:0.682927 AUC_val:0.8932\n",
      "Epoch:0129\n",
      "acc_train:0.9600 pre_train:0.9820 recall_train:0.9398 F1_train:0.9604 AUC_train:0.9939\n",
      "acc_val:0.7500 pre_val:0.8788 recall_val:0.5800 F1_val:0.698795 AUC_val:0.8932\n",
      "Epoch:0130\n",
      "acc_train:0.9678 pre_train:0.9844 recall_train:0.9527 F1_train:0.9683 AUC_train:0.9957\n",
      "acc_val:0.7500 pre_val:0.8788 recall_val:0.5800 F1_val:0.698795 AUC_val:0.8932\n",
      "Epoch:0131\n",
      "acc_train:0.9633 pre_train:0.9887 recall_train:0.9398 F1_train:0.9636 AUC_train:0.9938\n",
      "acc_val:0.7500 pre_val:0.8788 recall_val:0.5800 F1_val:0.698795 AUC_val:0.8936\n",
      "Epoch:0132\n",
      "acc_train:0.9511 pre_train:0.9817 recall_train:0.9226 F1_train:0.9512 AUC_train:0.9932\n",
      "acc_val:0.7400 pre_val:0.8529 recall_val:0.5800 F1_val:0.690476 AUC_val:0.8968\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0133\n",
      "acc_train:0.9567 pre_train:0.9733 recall_train:0.9419 F1_train:0.9574 AUC_train:0.9912\n",
      "acc_val:0.7200 pre_val:0.8438 recall_val:0.5400 F1_val:0.658537 AUC_val:0.9016\n",
      "Epoch:0134\n",
      "acc_train:0.9522 pre_train:0.9710 recall_train:0.9355 F1_train:0.9529 AUC_train:0.9927\n",
      "acc_val:0.7200 pre_val:0.8438 recall_val:0.5400 F1_val:0.658537 AUC_val:0.9036\n",
      "Early Stopping!!! epoch：133\n",
      " Starting the 1-6 Fold:：\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.5111 pre_train:0.5523 recall_train:0.2839 F1_train:0.3750 AUC_train:0.5089\n",
      "acc_val:0.4800 pre_val:0.4889 recall_val:0.8800 F1_val:0.628571 AUC_val:0.4424\n",
      "Epoch:0002\n",
      "acc_train:0.5200 pre_train:0.5623 recall_train:0.3204 F1_train:0.4082 AUC_train:0.5383\n",
      "acc_val:0.4800 pre_val:0.4889 recall_val:0.8800 F1_val:0.628571 AUC_val:0.5044\n",
      "Epoch:0003\n",
      "acc_train:0.5222 pre_train:0.5655 recall_train:0.3247 F1_train:0.4126 AUC_train:0.5336\n",
      "acc_val:0.4700 pre_val:0.4824 recall_val:0.8200 F1_val:0.607407 AUC_val:0.4752\n",
      "Epoch:0004\n",
      "acc_train:0.5378 pre_train:0.5992 recall_train:0.3183 F1_train:0.4157 AUC_train:0.5682\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.8200 F1_val:0.621212 AUC_val:0.4860\n",
      "Epoch:0005\n",
      "acc_train:0.5556 pre_train:0.6109 recall_train:0.3849 F1_train:0.4723 AUC_train:0.5924\n",
      "acc_val:0.5200 pre_val:0.5128 recall_val:0.8000 F1_val:0.625000 AUC_val:0.5100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0006\n",
      "acc_train:0.5589 pre_train:0.6000 recall_train:0.4387 F1_train:0.5068 AUC_train:0.6003\n",
      "acc_val:0.5300 pre_val:0.5205 recall_val:0.7600 F1_val:0.617886 AUC_val:0.5300\n",
      "Epoch:0007\n",
      "acc_train:0.6078 pre_train:0.7029 recall_train:0.4172 F1_train:0.5236 AUC_train:0.6267\n",
      "acc_val:0.5100 pre_val:0.5070 recall_val:0.7200 F1_val:0.595041 AUC_val:0.5408\n",
      "Epoch:0008\n",
      "acc_train:0.5856 pre_train:0.6494 recall_train:0.4301 F1_train:0.5175 AUC_train:0.6053\n",
      "acc_val:0.5100 pre_val:0.5079 recall_val:0.6400 F1_val:0.566372 AUC_val:0.5392\n",
      "Epoch:0009\n",
      "acc_train:0.5856 pre_train:0.6565 recall_train:0.4151 F1_train:0.5086 AUC_train:0.6125\n",
      "acc_val:0.5500 pre_val:0.5472 recall_val:0.5800 F1_val:0.563107 AUC_val:0.5828\n",
      "Epoch:0010\n",
      "acc_train:0.5989 pre_train:0.6677 recall_train:0.4452 F1_train:0.5342 AUC_train:0.6186\n",
      "acc_val:0.5900 pre_val:0.5918 recall_val:0.5800 F1_val:0.585859 AUC_val:0.6120\n",
      "Epoch:0011\n",
      "acc_train:0.6033 pre_train:0.6687 recall_train:0.4602 F1_train:0.5452 AUC_train:0.6120\n",
      "acc_val:0.5800 pre_val:0.5800 recall_val:0.5800 F1_val:0.580000 AUC_val:0.6164\n",
      "Epoch:0012\n",
      "acc_train:0.6022 pre_train:0.6677 recall_train:0.4581 F1_train:0.5434 AUC_train:0.6247\n",
      "acc_val:0.6300 pre_val:0.6585 recall_val:0.5400 F1_val:0.593407 AUC_val:0.6256\n",
      "Epoch:0013\n",
      "acc_train:0.6167 pre_train:0.6685 recall_train:0.5118 F1_train:0.5798 AUC_train:0.6398\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.6372\n",
      "Epoch:0014\n",
      "acc_train:0.6156 pre_train:0.6889 recall_train:0.4667 F1_train:0.5564 AUC_train:0.6360\n",
      "acc_val:0.6500 pre_val:0.7143 recall_val:0.5000 F1_val:0.588235 AUC_val:0.6404\n",
      "Epoch:0015\n",
      "acc_train:0.6167 pre_train:0.6714 recall_train:0.5054 F1_train:0.5767 AUC_train:0.6569\n",
      "acc_val:0.6400 pre_val:0.7059 recall_val:0.4800 F1_val:0.571429 AUC_val:0.6468\n",
      "Epoch:0016\n",
      "acc_train:0.5767 pre_train:0.6160 recall_train:0.4796 F1_train:0.5393 AUC_train:0.6106\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6500\n",
      "Epoch:0017\n",
      "acc_train:0.6322 pre_train:0.7055 recall_train:0.4946 F1_train:0.5815 AUC_train:0.6737\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6480\n",
      "Epoch:0018\n",
      "acc_train:0.6067 pre_train:0.6740 recall_train:0.4624 F1_train:0.5485 AUC_train:0.6420\n",
      "acc_val:0.6400 pre_val:0.6944 recall_val:0.5000 F1_val:0.581395 AUC_val:0.6448\n",
      "Epoch:0019\n",
      "acc_train:0.6189 pre_train:0.6572 recall_train:0.5484 F1_train:0.5979 AUC_train:0.6724\n",
      "acc_val:0.6400 pre_val:0.6944 recall_val:0.5000 F1_val:0.581395 AUC_val:0.6624\n",
      "Epoch:0020\n",
      "acc_train:0.6167 pre_train:0.6345 recall_train:0.6086 F1_train:0.6213 AUC_train:0.6500\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6032\n",
      "Epoch:0021\n",
      "acc_train:0.6311 pre_train:0.6667 recall_train:0.5720 F1_train:0.6157 AUC_train:0.6694\n",
      "acc_val:0.6400 pre_val:0.6944 recall_val:0.5000 F1_val:0.581395 AUC_val:0.6036\n",
      "Epoch:0022\n",
      "acc_train:0.6133 pre_train:0.6481 recall_train:0.5505 F1_train:0.5953 AUC_train:0.6447\n",
      "acc_val:0.6400 pre_val:0.6944 recall_val:0.5000 F1_val:0.581395 AUC_val:0.6140\n",
      "Epoch:0023\n",
      "acc_train:0.6267 pre_train:0.6684 recall_train:0.5505 F1_train:0.6038 AUC_train:0.6532\n",
      "acc_val:0.6400 pre_val:0.6842 recall_val:0.5200 F1_val:0.590909 AUC_val:0.6828\n",
      "Epoch:0024\n",
      "acc_train:0.6289 pre_train:0.6642 recall_train:0.5699 F1_train:0.6134 AUC_train:0.6641\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.6868\n",
      "Epoch:0025\n",
      "acc_train:0.6344 pre_train:0.6899 recall_train:0.5312 F1_train:0.6002 AUC_train:0.6702\n",
      "acc_val:0.6300 pre_val:0.6857 recall_val:0.4800 F1_val:0.564706 AUC_val:0.6888\n",
      "Epoch:0026\n",
      "acc_train:0.6356 pre_train:0.6898 recall_train:0.5355 F1_train:0.6029 AUC_train:0.6742\n",
      "acc_val:0.6400 pre_val:0.7059 recall_val:0.4800 F1_val:0.571429 AUC_val:0.6912\n",
      "Epoch:0027\n",
      "acc_train:0.6311 pre_train:0.6916 recall_train:0.5161 F1_train:0.5911 AUC_train:0.6673\n",
      "acc_val:0.6500 pre_val:0.7273 recall_val:0.4800 F1_val:0.578313 AUC_val:0.6920\n",
      "Epoch:0028\n",
      "acc_train:0.6511 pre_train:0.6902 recall_train:0.5892 F1_train:0.6357 AUC_train:0.6932\n",
      "acc_val:0.6500 pre_val:0.7273 recall_val:0.4800 F1_val:0.578313 AUC_val:0.6956\n",
      "Epoch:0029\n",
      "acc_train:0.6244 pre_train:0.6907 recall_train:0.4946 F1_train:0.5764 AUC_train:0.6812\n",
      "acc_val:0.6500 pre_val:0.7586 recall_val:0.4400 F1_val:0.556962 AUC_val:0.6972\n",
      "Epoch:0030\n",
      "acc_train:0.6511 pre_train:0.7428 recall_train:0.4968 F1_train:0.5954 AUC_train:0.6897\n",
      "acc_val:0.6600 pre_val:0.7857 recall_val:0.4400 F1_val:0.564103 AUC_val:0.6976\n",
      "Epoch:0031\n",
      "acc_train:0.6656 pre_train:0.7595 recall_train:0.5161 F1_train:0.6146 AUC_train:0.7042\n",
      "acc_val:0.6600 pre_val:0.7857 recall_val:0.4400 F1_val:0.564103 AUC_val:0.6980\n",
      "Epoch:0032\n",
      "acc_train:0.6511 pre_train:0.7227 recall_train:0.5269 F1_train:0.6095 AUC_train:0.6985\n",
      "acc_val:0.6600 pre_val:0.7857 recall_val:0.4400 F1_val:0.564103 AUC_val:0.7004\n",
      "Epoch:0033\n",
      "acc_train:0.6700 pre_train:0.7500 recall_train:0.5419 F1_train:0.6292 AUC_train:0.7181\n",
      "acc_val:0.6600 pre_val:0.7857 recall_val:0.4400 F1_val:0.564103 AUC_val:0.7048\n",
      "Epoch:0034\n",
      "acc_train:0.6600 pre_train:0.7202 recall_train:0.5591 F1_train:0.6295 AUC_train:0.7107\n",
      "acc_val:0.6800 pre_val:0.8214 recall_val:0.4600 F1_val:0.589744 AUC_val:0.7076\n",
      "Epoch:0035\n",
      "acc_train:0.6700 pre_train:0.7561 recall_train:0.5333 F1_train:0.6255 AUC_train:0.7216\n",
      "acc_val:0.6900 pre_val:0.8519 recall_val:0.4600 F1_val:0.597403 AUC_val:0.7060\n",
      "Epoch:0036\n",
      "acc_train:0.6756 pre_train:0.7836 recall_train:0.5140 F1_train:0.6208 AUC_train:0.7143\n",
      "acc_val:0.6900 pre_val:0.8519 recall_val:0.4600 F1_val:0.597403 AUC_val:0.7112\n",
      "Epoch:0037\n",
      "acc_train:0.6544 pre_train:0.7348 recall_train:0.5183 F1_train:0.6078 AUC_train:0.6948\n",
      "acc_val:0.6900 pre_val:0.8519 recall_val:0.4600 F1_val:0.597403 AUC_val:0.7136\n",
      "Epoch:0038\n",
      "acc_train:0.6889 pre_train:0.7864 recall_train:0.5462 F1_train:0.6447 AUC_train:0.7507\n",
      "acc_val:0.6900 pre_val:0.8519 recall_val:0.4600 F1_val:0.597403 AUC_val:0.7212\n",
      "Epoch:0039\n",
      "acc_train:0.6922 pre_train:0.8113 recall_train:0.5269 F1_train:0.6389 AUC_train:0.7323\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.7216\n",
      "Epoch:0040\n",
      "acc_train:0.6800 pre_train:0.8062 recall_train:0.5011 F1_train:0.6180 AUC_train:0.7262\n",
      "acc_val:0.7000 pre_val:0.8846 recall_val:0.4600 F1_val:0.605263 AUC_val:0.7272\n",
      "Epoch:0041\n",
      "acc_train:0.6989 pre_train:0.8170 recall_train:0.5376 F1_train:0.6485 AUC_train:0.7202\n",
      "acc_val:0.6900 pre_val:0.8519 recall_val:0.4600 F1_val:0.597403 AUC_val:0.7404\n",
      "Epoch:0042\n",
      "acc_train:0.7056 pre_train:0.8521 recall_train:0.5204 F1_train:0.6462 AUC_train:0.7281\n",
      "acc_val:0.6900 pre_val:0.8519 recall_val:0.4600 F1_val:0.597403 AUC_val:0.7380\n",
      "Epoch:0043\n",
      "acc_train:0.6956 pre_train:0.8152 recall_train:0.5312 F1_train:0.6432 AUC_train:0.7337\n",
      "acc_val:0.6900 pre_val:0.8519 recall_val:0.4600 F1_val:0.597403 AUC_val:0.7400\n",
      "Epoch:0044\n",
      "acc_train:0.7189 pre_train:0.8732 recall_train:0.5333 F1_train:0.6622 AUC_train:0.7411\n",
      "acc_val:0.6900 pre_val:0.8519 recall_val:0.4600 F1_val:0.597403 AUC_val:0.7376\n",
      "Epoch:0045\n",
      "acc_train:0.7156 pre_train:0.8719 recall_train:0.5269 F1_train:0.6568 AUC_train:0.7377\n",
      "acc_val:0.7000 pre_val:0.8571 recall_val:0.4800 F1_val:0.615385 AUC_val:0.7384\n",
      "Epoch:0046\n",
      "acc_train:0.7100 pre_train:0.8669 recall_train:0.5183 F1_train:0.6487 AUC_train:0.7341\n",
      "acc_val:0.7000 pre_val:0.8571 recall_val:0.4800 F1_val:0.615385 AUC_val:0.7364\n",
      "Epoch:0047\n",
      "acc_train:0.6889 pre_train:0.7900 recall_train:0.5419 F1_train:0.6429 AUC_train:0.7519\n",
      "acc_val:0.7100 pre_val:0.8621 recall_val:0.5000 F1_val:0.632911 AUC_val:0.7356\n",
      "Epoch:0048\n",
      "acc_train:0.7156 pre_train:0.8495 recall_train:0.5462 F1_train:0.6649 AUC_train:0.7539\n",
      "acc_val:0.7100 pre_val:0.8621 recall_val:0.5000 F1_val:0.632911 AUC_val:0.7416\n",
      "Epoch:0049\n",
      "acc_train:0.7100 pre_train:0.8208 recall_train:0.5613 F1_train:0.6667 AUC_train:0.7598\n",
      "acc_val:0.7100 pre_val:0.8621 recall_val:0.5000 F1_val:0.632911 AUC_val:0.7400\n",
      "Epoch:0050\n",
      "acc_train:0.7178 pre_train:0.8651 recall_train:0.5376 F1_train:0.6631 AUC_train:0.7741\n",
      "acc_val:0.7100 pre_val:0.8621 recall_val:0.5000 F1_val:0.632911 AUC_val:0.7384\n",
      "Epoch:0051\n",
      "acc_train:0.7089 pre_train:0.8285 recall_train:0.5505 F1_train:0.6615 AUC_train:0.7504\n",
      "acc_val:0.7200 pre_val:0.8929 recall_val:0.5000 F1_val:0.641026 AUC_val:0.7360\n",
      "Epoch:0052\n",
      "acc_train:0.7167 pre_train:0.8409 recall_train:0.5570 F1_train:0.6701 AUC_train:0.7569\n",
      "acc_val:0.7300 pre_val:0.8966 recall_val:0.5200 F1_val:0.658228 AUC_val:0.7336\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0053\n",
      "acc_train:0.7033 pre_train:0.8214 recall_train:0.5441 F1_train:0.6546 AUC_train:0.7714\n",
      "acc_val:0.7300 pre_val:0.8966 recall_val:0.5200 F1_val:0.658228 AUC_val:0.7332\n",
      "Epoch:0054\n",
      "acc_train:0.7011 pre_train:0.7832 recall_train:0.5828 F1_train:0.6683 AUC_train:0.7686\n",
      "acc_val:0.7300 pre_val:0.8966 recall_val:0.5200 F1_val:0.658228 AUC_val:0.7468\n",
      "Epoch:0055\n",
      "acc_train:0.7300 pre_train:0.8627 recall_train:0.5677 F1_train:0.6848 AUC_train:0.7717\n",
      "acc_val:0.7300 pre_val:0.8966 recall_val:0.5200 F1_val:0.658228 AUC_val:0.7420\n",
      "Epoch:0056\n",
      "acc_train:0.7467 pre_train:0.8738 recall_train:0.5957 F1_train:0.7084 AUC_train:0.7886\n",
      "acc_val:0.7300 pre_val:0.8966 recall_val:0.5200 F1_val:0.658228 AUC_val:0.7392\n",
      "Epoch:0057\n",
      "acc_train:0.7111 pre_train:0.7621 recall_train:0.6409 F1_train:0.6963 AUC_train:0.7792\n",
      "acc_val:0.7200 pre_val:0.8667 recall_val:0.5200 F1_val:0.650000 AUC_val:0.7500\n",
      "Epoch:0058\n",
      "acc_train:0.7378 pre_train:0.8545 recall_train:0.5935 F1_train:0.7005 AUC_train:0.8054\n",
      "acc_val:0.7200 pre_val:0.8667 recall_val:0.5200 F1_val:0.650000 AUC_val:0.7592\n",
      "Epoch:0059\n",
      "acc_train:0.7211 pre_train:0.8408 recall_train:0.5677 F1_train:0.6778 AUC_train:0.7957\n",
      "acc_val:0.7300 pre_val:0.8966 recall_val:0.5200 F1_val:0.658228 AUC_val:0.7600\n",
      "Epoch:0060\n",
      "acc_train:0.7478 pre_train:0.8584 recall_train:0.6129 F1_train:0.7152 AUC_train:0.8051\n",
      "acc_val:0.7300 pre_val:0.8966 recall_val:0.5200 F1_val:0.658228 AUC_val:0.7624\n",
      "Epoch:0061\n",
      "acc_train:0.7400 pre_train:0.8738 recall_train:0.5806 F1_train:0.6977 AUC_train:0.8102\n",
      "acc_val:0.7300 pre_val:0.8966 recall_val:0.5200 F1_val:0.658228 AUC_val:0.7600\n",
      "Epoch:0062\n",
      "acc_train:0.7378 pre_train:0.8523 recall_train:0.5957 F1_train:0.7013 AUC_train:0.8121\n",
      "acc_val:0.7300 pre_val:0.8966 recall_val:0.5200 F1_val:0.658228 AUC_val:0.7512\n",
      "Epoch:0063\n",
      "acc_train:0.7444 pre_train:0.8707 recall_train:0.5935 F1_train:0.7059 AUC_train:0.8139\n",
      "acc_val:0.7100 pre_val:0.8387 recall_val:0.5200 F1_val:0.641975 AUC_val:0.7516\n",
      "Epoch:0064\n",
      "acc_train:0.7589 pre_train:0.8851 recall_train:0.6129 F1_train:0.7243 AUC_train:0.8214\n",
      "acc_val:0.7000 pre_val:0.8333 recall_val:0.5000 F1_val:0.625000 AUC_val:0.7492\n",
      "Epoch:0065\n",
      "acc_train:0.7489 pre_train:0.8770 recall_train:0.5978 F1_train:0.7110 AUC_train:0.8180\n",
      "acc_val:0.7000 pre_val:0.8333 recall_val:0.5000 F1_val:0.625000 AUC_val:0.7492\n",
      "Epoch:0066\n",
      "acc_train:0.7611 pre_train:0.8882 recall_train:0.6151 F1_train:0.7268 AUC_train:0.8307\n",
      "acc_val:0.7000 pre_val:0.8333 recall_val:0.5000 F1_val:0.625000 AUC_val:0.7468\n",
      "Epoch:0067\n",
      "acc_train:0.7567 pre_train:0.8917 recall_train:0.6022 F1_train:0.7189 AUC_train:0.8399\n",
      "acc_val:0.7000 pre_val:0.8333 recall_val:0.5000 F1_val:0.625000 AUC_val:0.7464\n",
      "Epoch:0068\n",
      "acc_train:0.7511 pre_train:0.8777 recall_train:0.6022 F1_train:0.7143 AUC_train:0.8308\n",
      "acc_val:0.7200 pre_val:0.8667 recall_val:0.5200 F1_val:0.650000 AUC_val:0.7512\n",
      "Epoch:0069\n",
      "acc_train:0.7722 pre_train:0.9088 recall_train:0.6215 F1_train:0.7382 AUC_train:0.8522\n",
      "acc_val:0.7200 pre_val:0.8667 recall_val:0.5200 F1_val:0.650000 AUC_val:0.7572\n",
      "Epoch:0070\n",
      "acc_train:0.7678 pre_train:0.9025 recall_train:0.6172 F1_train:0.7331 AUC_train:0.8500\n",
      "acc_val:0.7200 pre_val:0.8667 recall_val:0.5200 F1_val:0.650000 AUC_val:0.7668\n",
      "Epoch:0071\n",
      "acc_train:0.7733 pre_train:0.8991 recall_train:0.6323 F1_train:0.7424 AUC_train:0.8743\n",
      "acc_val:0.7200 pre_val:0.8667 recall_val:0.5200 F1_val:0.650000 AUC_val:0.7696\n",
      "Epoch:0072\n",
      "acc_train:0.7811 pre_train:0.8895 recall_train:0.6581 F1_train:0.7565 AUC_train:0.8570\n",
      "acc_val:0.7200 pre_val:0.8667 recall_val:0.5200 F1_val:0.650000 AUC_val:0.7768\n",
      "Epoch:0073\n",
      "acc_train:0.7867 pre_train:0.8889 recall_train:0.6710 F1_train:0.7647 AUC_train:0.8759\n",
      "acc_val:0.7100 pre_val:0.8387 recall_val:0.5200 F1_val:0.641975 AUC_val:0.7804\n",
      "Epoch:0074\n",
      "acc_train:0.7867 pre_train:0.8957 recall_train:0.6645 F1_train:0.7630 AUC_train:0.8818\n",
      "acc_val:0.7200 pre_val:0.8438 recall_val:0.5400 F1_val:0.658537 AUC_val:0.7920\n",
      "Epoch:0075\n",
      "acc_train:0.7756 pre_train:0.8507 recall_train:0.6860 F1_train:0.7595 AUC_train:0.8707\n",
      "acc_val:0.7100 pre_val:0.8000 recall_val:0.5600 F1_val:0.658824 AUC_val:0.7976\n",
      "Epoch:0076\n",
      "acc_train:0.7911 pre_train:0.9110 recall_train:0.6602 F1_train:0.7656 AUC_train:0.8878\n",
      "acc_val:0.7100 pre_val:0.8000 recall_val:0.5600 F1_val:0.658824 AUC_val:0.7996\n",
      "Epoch:0077\n",
      "acc_train:0.7811 pre_train:0.8895 recall_train:0.6581 F1_train:0.7565 AUC_train:0.8925\n",
      "acc_val:0.7100 pre_val:0.8000 recall_val:0.5600 F1_val:0.658824 AUC_val:0.8072\n",
      "Epoch:0078\n",
      "acc_train:0.7956 pre_train:0.9026 recall_train:0.6774 F1_train:0.7740 AUC_train:0.8920\n",
      "acc_val:0.7100 pre_val:0.8000 recall_val:0.5600 F1_val:0.658824 AUC_val:0.8116\n",
      "Epoch:0079\n",
      "acc_train:0.7856 pre_train:0.8977 recall_train:0.6602 F1_train:0.7608 AUC_train:0.8807\n",
      "acc_val:0.7100 pre_val:0.8000 recall_val:0.5600 F1_val:0.658824 AUC_val:0.8120\n",
      "Epoch:0080\n",
      "acc_train:0.8056 pre_train:0.9315 recall_train:0.6731 F1_train:0.7815 AUC_train:0.9069\n",
      "acc_val:0.7100 pre_val:0.8182 recall_val:0.5400 F1_val:0.650602 AUC_val:0.8228\n",
      "Epoch:0081\n",
      "acc_train:0.7989 pre_train:0.9057 recall_train:0.6817 F1_train:0.7779 AUC_train:0.9078\n",
      "acc_val:0.6900 pre_val:0.7879 recall_val:0.5200 F1_val:0.626506 AUC_val:0.8368\n",
      "Epoch:0082\n",
      "acc_train:0.8056 pre_train:0.9315 recall_train:0.6731 F1_train:0.7815 AUC_train:0.9208\n",
      "acc_val:0.7100 pre_val:0.8182 recall_val:0.5400 F1_val:0.650602 AUC_val:0.8280\n",
      "Epoch:0083\n",
      "acc_train:0.8089 pre_train:0.9322 recall_train:0.6796 F1_train:0.7861 AUC_train:0.9153\n",
      "acc_val:0.7200 pre_val:0.8056 recall_val:0.5800 F1_val:0.674419 AUC_val:0.8160\n",
      "Epoch:0084\n",
      "acc_train:0.8089 pre_train:0.9246 recall_train:0.6860 F1_train:0.7877 AUC_train:0.9213\n",
      "acc_val:0.7200 pre_val:0.8056 recall_val:0.5800 F1_val:0.674419 AUC_val:0.7988\n",
      "Epoch:0085\n",
      "acc_train:0.8267 pre_train:0.9233 recall_train:0.7247 F1_train:0.8120 AUC_train:0.9238\n",
      "acc_val:0.7100 pre_val:0.7561 recall_val:0.6200 F1_val:0.681319 AUC_val:0.7876\n",
      "Epoch:0086\n",
      "acc_train:0.8222 pre_train:0.9088 recall_train:0.7290 F1_train:0.8091 AUC_train:0.9231\n",
      "acc_val:0.7100 pre_val:0.7692 recall_val:0.6000 F1_val:0.674157 AUC_val:0.7892\n",
      "Epoch:0087\n",
      "acc_train:0.8422 pre_train:0.9152 recall_train:0.7656 F1_train:0.8337 AUC_train:0.9391\n",
      "acc_val:0.7400 pre_val:0.8000 recall_val:0.6400 F1_val:0.711111 AUC_val:0.7868\n",
      "Epoch:0088\n",
      "acc_train:0.8256 pre_train:0.9053 recall_train:0.7398 F1_train:0.8142 AUC_train:0.9167\n",
      "acc_val:0.7400 pre_val:0.8000 recall_val:0.6400 F1_val:0.711111 AUC_val:0.8052\n",
      "Epoch:0089\n",
      "acc_train:0.8389 pre_train:0.9444 recall_train:0.7312 F1_train:0.8242 AUC_train:0.9403\n",
      "acc_val:0.7400 pre_val:0.8000 recall_val:0.6400 F1_val:0.711111 AUC_val:0.8144\n",
      "Epoch:0090\n",
      "acc_train:0.8233 pre_train:0.8984 recall_train:0.7419 F1_train:0.8127 AUC_train:0.9444\n",
      "acc_val:0.7400 pre_val:0.8000 recall_val:0.6400 F1_val:0.711111 AUC_val:0.8208\n",
      "Epoch:0091\n",
      "acc_train:0.8322 pre_train:0.9153 recall_train:0.7441 F1_train:0.8209 AUC_train:0.9392\n",
      "acc_val:0.7200 pre_val:0.7750 recall_val:0.6200 F1_val:0.688889 AUC_val:0.8140\n",
      "Epoch:0092\n",
      "acc_train:0.8422 pre_train:0.9284 recall_train:0.7527 F1_train:0.8314 AUC_train:0.9482\n",
      "acc_val:0.7100 pre_val:0.7692 recall_val:0.6000 F1_val:0.674157 AUC_val:0.8164\n",
      "Epoch:0093\n",
      "acc_train:0.8422 pre_train:0.9152 recall_train:0.7656 F1_train:0.8337 AUC_train:0.9457\n",
      "acc_val:0.6900 pre_val:0.7111 recall_val:0.6400 F1_val:0.673684 AUC_val:0.7788\n",
      "Epoch:0094\n",
      "acc_train:0.8600 pre_train:0.8792 recall_train:0.8452 F1_train:0.8618 AUC_train:0.9463\n",
      "acc_val:0.6900 pre_val:0.7021 recall_val:0.6600 F1_val:0.680412 AUC_val:0.7488\n",
      "Epoch:0095\n",
      "acc_train:0.8478 pre_train:0.9205 recall_train:0.7720 F1_train:0.8398 AUC_train:0.9319\n",
      "acc_val:0.7000 pre_val:0.7174 recall_val:0.6600 F1_val:0.687500 AUC_val:0.7920\n",
      "Epoch:0096\n",
      "acc_train:0.8511 pre_train:0.9211 recall_train:0.7785 F1_train:0.8438 AUC_train:0.9355\n",
      "acc_val:0.7000 pre_val:0.7174 recall_val:0.6600 F1_val:0.687500 AUC_val:0.8092\n",
      "Epoch:0097\n",
      "acc_train:0.8489 pre_train:0.9273 recall_train:0.7677 F1_train:0.8400 AUC_train:0.9467\n",
      "acc_val:0.7200 pre_val:0.7619 recall_val:0.6400 F1_val:0.695652 AUC_val:0.8196\n",
      "Epoch:0098\n",
      "acc_train:0.8544 pre_train:0.9349 recall_train:0.7720 F1_train:0.8457 AUC_train:0.9470\n",
      "acc_val:0.7300 pre_val:0.7805 recall_val:0.6400 F1_val:0.703297 AUC_val:0.8260\n",
      "Epoch:0099\n",
      "acc_train:0.8578 pre_train:0.9266 recall_train:0.7871 F1_train:0.8512 AUC_train:0.9542\n",
      "acc_val:0.7400 pre_val:0.8000 recall_val:0.6400 F1_val:0.711111 AUC_val:0.8276\n",
      "Early Stopping!!! epoch：98\n",
      " Starting the 1-7 Fold:：\n",
      "Fitting estimator with 19900 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.4633 pre_train:0.4879 recall_train:0.7806 F1_train:0.6005 AUC_train:0.4611\n",
      "acc_val:0.4300 pre_val:0.4588 recall_val:0.7800 F1_val:0.577778 AUC_val:0.5096\n",
      "Epoch:0002\n",
      "acc_train:0.4344 pre_train:0.4695 recall_train:0.7290 F1_train:0.5712 AUC_train:0.4539\n",
      "acc_val:0.4300 pre_val:0.4588 recall_val:0.7800 F1_val:0.577778 AUC_val:0.5476\n",
      "Epoch:0003\n",
      "acc_train:0.4544 pre_train:0.4825 recall_train:0.7699 F1_train:0.5932 AUC_train:0.4627\n",
      "acc_val:0.4300 pre_val:0.4588 recall_val:0.7800 F1_val:0.577778 AUC_val:0.5376\n",
      "Epoch:0004\n",
      "acc_train:0.4689 pre_train:0.4906 recall_train:0.7333 F1_train:0.5879 AUC_train:0.4899\n",
      "acc_val:0.4300 pre_val:0.4588 recall_val:0.7800 F1_val:0.577778 AUC_val:0.5376\n",
      "Epoch:0005\n",
      "acc_train:0.5111 pre_train:0.5234 recall_train:0.6022 F1_train:0.5600 AUC_train:0.5156\n",
      "acc_val:0.4400 pre_val:0.4659 recall_val:0.8200 F1_val:0.594203 AUC_val:0.5332\n",
      "Epoch:0006\n",
      "acc_train:0.5778 pre_train:0.7033 recall_train:0.3161 F1_train:0.4362 AUC_train:0.6021\n",
      "acc_val:0.5100 pre_val:0.5056 recall_val:0.9000 F1_val:0.647482 AUC_val:0.6032\n",
      "Epoch:0007\n",
      "acc_train:0.5644 pre_train:0.6637 recall_train:0.3183 F1_train:0.4302 AUC_train:0.5619\n",
      "acc_val:0.5200 pre_val:0.5152 recall_val:0.6800 F1_val:0.586207 AUC_val:0.6456\n",
      "Epoch:0008\n",
      "acc_train:0.5678 pre_train:0.6583 recall_train:0.3398 F1_train:0.4482 AUC_train:0.5626\n",
      "acc_val:0.5700 pre_val:0.5556 recall_val:0.7000 F1_val:0.619469 AUC_val:0.6324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0009\n",
      "acc_train:0.5767 pre_train:0.6694 recall_train:0.3570 F1_train:0.4656 AUC_train:0.5536\n",
      "acc_val:0.5200 pre_val:0.5147 recall_val:0.7000 F1_val:0.593220 AUC_val:0.6224\n",
      "Epoch:0010\n",
      "acc_train:0.5778 pre_train:0.7094 recall_train:0.3097 F1_train:0.4311 AUC_train:0.6022\n",
      "acc_val:0.5800 pre_val:0.5870 recall_val:0.5400 F1_val:0.562500 AUC_val:0.6140\n",
      "Epoch:0011\n",
      "acc_train:0.5789 pre_train:0.7108 recall_train:0.3118 F1_train:0.4335 AUC_train:0.5741\n",
      "acc_val:0.5900 pre_val:0.6047 recall_val:0.5200 F1_val:0.559140 AUC_val:0.6068\n",
      "Epoch:0012\n",
      "acc_train:0.5800 pre_train:0.7062 recall_train:0.3204 F1_train:0.4408 AUC_train:0.6020\n",
      "acc_val:0.6000 pre_val:0.6471 recall_val:0.4400 F1_val:0.523810 AUC_val:0.6020\n",
      "Epoch:0013\n",
      "acc_train:0.5822 pre_train:0.6978 recall_train:0.3376 F1_train:0.4551 AUC_train:0.5931\n",
      "acc_val:0.6100 pre_val:0.6774 recall_val:0.4200 F1_val:0.518519 AUC_val:0.5964\n",
      "Epoch:0014\n",
      "acc_train:0.5844 pre_train:0.7156 recall_train:0.3247 F1_train:0.4467 AUC_train:0.6164\n",
      "acc_val:0.6000 pre_val:0.6562 recall_val:0.4200 F1_val:0.512195 AUC_val:0.5948\n",
      "Epoch:0015\n",
      "acc_train:0.5878 pre_train:0.7009 recall_train:0.3527 F1_train:0.4692 AUC_train:0.6090\n",
      "acc_val:0.6100 pre_val:0.6774 recall_val:0.4200 F1_val:0.518519 AUC_val:0.5948\n",
      "Epoch:0016\n",
      "acc_train:0.5922 pre_train:0.6870 recall_train:0.3871 F1_train:0.4952 AUC_train:0.6133\n",
      "acc_val:0.6000 pre_val:0.6562 recall_val:0.4200 F1_val:0.512195 AUC_val:0.5900\n",
      "Epoch:0017\n",
      "acc_train:0.6000 pre_train:0.7126 recall_train:0.3785 F1_train:0.4944 AUC_train:0.6130\n",
      "acc_val:0.6100 pre_val:0.6897 recall_val:0.4000 F1_val:0.506329 AUC_val:0.5904\n",
      "Epoch:0018\n",
      "acc_train:0.5800 pre_train:0.6900 recall_train:0.3398 F1_train:0.4553 AUC_train:0.5980\n",
      "acc_val:0.6100 pre_val:0.6897 recall_val:0.4000 F1_val:0.506329 AUC_val:0.6084\n",
      "Epoch:0019\n",
      "acc_train:0.5878 pre_train:0.6895 recall_train:0.3677 F1_train:0.4797 AUC_train:0.6136\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6080\n",
      "Epoch:0020\n",
      "acc_train:0.6011 pre_train:0.7172 recall_train:0.3763 F1_train:0.4937 AUC_train:0.6356\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6064\n",
      "Epoch:0021\n",
      "acc_train:0.5900 pre_train:0.7143 recall_train:0.3441 F1_train:0.4644 AUC_train:0.6282\n",
      "acc_val:0.6000 pre_val:0.6923 recall_val:0.3600 F1_val:0.473684 AUC_val:0.6076\n",
      "Epoch:0022\n",
      "acc_train:0.6078 pre_train:0.6728 recall_train:0.4688 F1_train:0.5526 AUC_train:0.6318\n",
      "acc_val:0.6100 pre_val:0.6897 recall_val:0.4000 F1_val:0.506329 AUC_val:0.6104\n",
      "Epoch:0023\n",
      "acc_train:0.5922 pre_train:0.7042 recall_train:0.3634 F1_train:0.4794 AUC_train:0.6278\n",
      "acc_val:0.6000 pre_val:0.6786 recall_val:0.3800 F1_val:0.487179 AUC_val:0.6104\n",
      "Epoch:0024\n",
      "acc_train:0.6156 pre_train:0.7245 recall_train:0.4129 F1_train:0.5260 AUC_train:0.6281\n",
      "acc_val:0.6100 pre_val:0.6897 recall_val:0.4000 F1_val:0.506329 AUC_val:0.6116\n",
      "Epoch:0025\n",
      "acc_train:0.5889 pre_train:0.6939 recall_train:0.3656 F1_train:0.4789 AUC_train:0.6275\n",
      "acc_val:0.6100 pre_val:0.6897 recall_val:0.4000 F1_val:0.506329 AUC_val:0.6124\n",
      "Epoch:0026\n",
      "acc_train:0.6033 pre_train:0.6731 recall_train:0.4516 F1_train:0.5405 AUC_train:0.6283\n",
      "acc_val:0.6100 pre_val:0.6897 recall_val:0.4000 F1_val:0.506329 AUC_val:0.6168\n",
      "Epoch:0027\n",
      "acc_train:0.6289 pre_train:0.7282 recall_train:0.4495 F1_train:0.5559 AUC_train:0.6552\n",
      "acc_val:0.6000 pre_val:0.6562 recall_val:0.4200 F1_val:0.512195 AUC_val:0.6199\n",
      "Epoch:0028\n",
      "acc_train:0.6133 pre_train:0.7331 recall_train:0.3957 F1_train:0.5140 AUC_train:0.6558\n",
      "acc_val:0.6100 pre_val:0.6774 recall_val:0.4200 F1_val:0.518519 AUC_val:0.6204\n",
      "Epoch:0029\n",
      "acc_train:0.6033 pre_train:0.6862 recall_train:0.4280 F1_train:0.5272 AUC_train:0.6375\n",
      "acc_val:0.6100 pre_val:0.6774 recall_val:0.4200 F1_val:0.518519 AUC_val:0.6204\n",
      "Epoch:0030\n",
      "acc_train:0.6011 pre_train:0.7087 recall_train:0.3871 F1_train:0.5007 AUC_train:0.6425\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6208\n",
      "Epoch:0031\n",
      "acc_train:0.6022 pre_train:0.6989 recall_train:0.4043 F1_train:0.5123 AUC_train:0.6339\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6196\n",
      "Epoch:0032\n",
      "acc_train:0.4778 pre_train:0.4970 recall_train:0.8796 F1_train:0.6351 AUC_train:0.5294\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6212\n",
      "Epoch:0033\n",
      "acc_train:0.6078 pre_train:0.6761 recall_train:0.4624 F1_train:0.5492 AUC_train:0.6369\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6188\n",
      "Epoch:0034\n",
      "acc_train:0.6356 pre_train:0.7161 recall_train:0.4882 F1_train:0.5806 AUC_train:0.6530\n",
      "acc_val:0.5500 pre_val:0.5287 recall_val:0.9200 F1_val:0.671533 AUC_val:0.6224\n",
      "Epoch:0035\n",
      "acc_train:0.5856 pre_train:0.5855 recall_train:0.6774 F1_train:0.6281 AUC_train:0.6231\n",
      "acc_val:0.5600 pre_val:0.5469 recall_val:0.7000 F1_val:0.614035 AUC_val:0.6304\n",
      "Epoch:0036\n",
      "acc_train:0.4956 pre_train:0.5069 recall_train:0.8710 F1_train:0.6408 AUC_train:0.5675\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6192\n",
      "Epoch:0037\n",
      "acc_train:0.6122 pre_train:0.6737 recall_train:0.4839 F1_train:0.5632 AUC_train:0.6306\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6272\n",
      "Epoch:0038\n",
      "acc_train:0.6278 pre_train:0.6958 recall_train:0.4968 F1_train:0.5797 AUC_train:0.6527\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6208\n",
      "Epoch:0039\n",
      "acc_train:0.6022 pre_train:0.7238 recall_train:0.3720 F1_train:0.4915 AUC_train:0.6375\n",
      "acc_val:0.5700 pre_val:0.5507 recall_val:0.7600 F1_val:0.638655 AUC_val:0.6180\n",
      "Epoch:0040\n",
      "acc_train:0.5889 pre_train:0.6892 recall_train:0.3720 F1_train:0.4832 AUC_train:0.6249\n",
      "acc_val:0.6200 pre_val:0.6667 recall_val:0.4800 F1_val:0.558140 AUC_val:0.6184\n",
      "Epoch:0041\n",
      "acc_train:0.6100 pre_train:0.7280 recall_train:0.3914 F1_train:0.5091 AUC_train:0.6277\n",
      "acc_val:0.6200 pre_val:0.7308 recall_val:0.3800 F1_val:0.500000 AUC_val:0.6260\n",
      "Epoch:0042\n",
      "acc_train:0.5911 pre_train:0.7215 recall_train:0.3398 F1_train:0.4620 AUC_train:0.6348\n",
      "acc_val:0.6200 pre_val:0.7308 recall_val:0.3800 F1_val:0.500000 AUC_val:0.6280\n",
      "Epoch:0043\n",
      "acc_train:0.4989 pre_train:0.5095 recall_train:0.8065 F1_train:0.6245 AUC_train:0.5558\n",
      "acc_val:0.5200 pre_val:0.5104 recall_val:0.9800 F1_val:0.671233 AUC_val:0.6356\n",
      "Epoch:0044\n",
      "acc_train:0.5944 pre_train:0.7033 recall_train:0.3720 F1_train:0.4866 AUC_train:0.6373\n",
      "acc_val:0.6300 pre_val:0.6327 recall_val:0.6200 F1_val:0.626263 AUC_val:0.6336\n",
      "Epoch:0045\n",
      "acc_train:0.5856 pre_train:0.6285 recall_train:0.4839 F1_train:0.5468 AUC_train:0.5720\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.8200 F1_val:0.621212 AUC_val:0.6332\n",
      "Epoch:0046\n",
      "acc_train:0.6000 pre_train:0.7333 recall_train:0.3548 F1_train:0.4783 AUC_train:0.6496\n",
      "acc_val:0.6400 pre_val:0.6944 recall_val:0.5000 F1_val:0.581395 AUC_val:0.6292\n",
      "Epoch:0047\n",
      "acc_train:0.6044 pre_train:0.7280 recall_train:0.3742 F1_train:0.4943 AUC_train:0.6215\n",
      "acc_val:0.6200 pre_val:0.7143 recall_val:0.4000 F1_val:0.512821 AUC_val:0.6320\n",
      "Epoch:0048\n",
      "acc_train:0.5856 pre_train:0.7233 recall_train:0.3204 F1_train:0.4441 AUC_train:0.6304\n",
      "acc_val:0.6300 pre_val:0.7407 recall_val:0.4000 F1_val:0.519481 AUC_val:0.6332\n",
      "Epoch:0049\n",
      "acc_train:0.5956 pre_train:0.7186 recall_train:0.3570 F1_train:0.4770 AUC_train:0.6345\n",
      "acc_val:0.6200 pre_val:0.7500 recall_val:0.3600 F1_val:0.486486 AUC_val:0.6236\n",
      "Epoch:0050\n",
      "acc_train:0.6000 pre_train:0.7273 recall_train:0.3613 F1_train:0.4828 AUC_train:0.6494\n",
      "acc_val:0.6300 pre_val:0.7600 recall_val:0.3800 F1_val:0.506667 AUC_val:0.6288\n",
      "Epoch:0051\n",
      "acc_train:0.5978 pre_train:0.7102 recall_train:0.3742 F1_train:0.4901 AUC_train:0.6303\n",
      "acc_val:0.6200 pre_val:0.7308 recall_val:0.3800 F1_val:0.500000 AUC_val:0.6304\n",
      "Epoch:0052\n",
      "acc_train:0.5922 pre_train:0.7008 recall_train:0.3677 F1_train:0.4824 AUC_train:0.6294\n",
      "acc_val:0.6200 pre_val:0.7308 recall_val:0.3800 F1_val:0.500000 AUC_val:0.6312\n",
      "Epoch:0053\n",
      "acc_train:0.5944 pre_train:0.6969 recall_train:0.3806 F1_train:0.4924 AUC_train:0.6164\n",
      "acc_val:0.6300 pre_val:0.7407 recall_val:0.4000 F1_val:0.519481 AUC_val:0.6336\n",
      "Epoch:0054\n",
      "acc_train:0.5856 pre_train:0.7000 recall_train:0.3462 F1_train:0.4633 AUC_train:0.6232\n",
      "acc_val:0.6200 pre_val:0.7308 recall_val:0.3800 F1_val:0.500000 AUC_val:0.6340\n",
      "Epoch:0055\n",
      "acc_train:0.5878 pre_train:0.7374 recall_train:0.3140 F1_train:0.4404 AUC_train:0.6283\n",
      "acc_val:0.6200 pre_val:0.7308 recall_val:0.3800 F1_val:0.500000 AUC_val:0.6332\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0056\n",
      "acc_train:0.5989 pre_train:0.6985 recall_train:0.3935 F1_train:0.5034 AUC_train:0.6334\n",
      "acc_val:0.6300 pre_val:0.7407 recall_val:0.4000 F1_val:0.519481 AUC_val:0.6360\n",
      "Epoch:0057\n",
      "acc_train:0.6044 pre_train:0.7280 recall_train:0.3742 F1_train:0.4943 AUC_train:0.6355\n",
      "acc_val:0.6300 pre_val:0.7241 recall_val:0.4200 F1_val:0.531646 AUC_val:0.6364\n",
      "Epoch:0058\n",
      "acc_train:0.6200 pre_train:0.7431 recall_train:0.4043 F1_train:0.5237 AUC_train:0.6482\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6336\n",
      "Epoch:0059\n",
      "acc_train:0.6222 pre_train:0.7119 recall_train:0.4516 F1_train:0.5526 AUC_train:0.6351\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6324\n",
      "Epoch:0060\n",
      "acc_train:0.6122 pre_train:0.7057 recall_train:0.4280 F1_train:0.5328 AUC_train:0.6282\n",
      "acc_val:0.6200 pre_val:0.7000 recall_val:0.4200 F1_val:0.525000 AUC_val:0.6336\n",
      "Epoch:0061\n",
      "acc_train:0.6200 pre_train:0.7490 recall_train:0.3978 F1_train:0.5197 AUC_train:0.6552\n",
      "acc_val:0.6300 pre_val:0.7097 recall_val:0.4400 F1_val:0.543210 AUC_val:0.6384\n",
      "Epoch:0062\n",
      "acc_train:0.6178 pre_train:0.7153 recall_train:0.4323 F1_train:0.5389 AUC_train:0.6254\n",
      "acc_val:0.6400 pre_val:0.7188 recall_val:0.4600 F1_val:0.560976 AUC_val:0.6424\n",
      "Epoch:0063\n",
      "acc_train:0.5956 pre_train:0.7167 recall_train:0.3591 F1_train:0.4785 AUC_train:0.6474\n",
      "acc_val:0.6400 pre_val:0.7188 recall_val:0.4600 F1_val:0.560976 AUC_val:0.6440\n",
      "Epoch:0064\n",
      "acc_train:0.6122 pre_train:0.7231 recall_train:0.4043 F1_train:0.5186 AUC_train:0.6603\n",
      "acc_val:0.6400 pre_val:0.7059 recall_val:0.4800 F1_val:0.571429 AUC_val:0.6428\n",
      "Epoch:0065\n",
      "acc_train:0.6178 pre_train:0.7138 recall_train:0.4344 F1_train:0.5401 AUC_train:0.6325\n",
      "acc_val:0.6300 pre_val:0.6970 recall_val:0.4600 F1_val:0.554217 AUC_val:0.6268\n",
      "Epoch:0066\n",
      "acc_train:0.6211 pre_train:0.7053 recall_train:0.4581 F1_train:0.5554 AUC_train:0.6377\n",
      "acc_val:0.5600 pre_val:0.5417 recall_val:0.7800 F1_val:0.639344 AUC_val:0.6256\n",
      "Epoch:0067\n",
      "acc_train:0.6011 pre_train:0.7038 recall_train:0.3935 F1_train:0.5048 AUC_train:0.6484\n",
      "acc_val:0.5800 pre_val:0.5741 recall_val:0.6200 F1_val:0.596154 AUC_val:0.6236\n",
      "Epoch:0068\n",
      "acc_train:0.6256 pre_train:0.7238 recall_train:0.4452 F1_train:0.5513 AUC_train:0.6310\n",
      "acc_val:0.5100 pre_val:0.5057 recall_val:0.8800 F1_val:0.642336 AUC_val:0.6220\n",
      "Epoch:0069\n",
      "acc_train:0.6189 pre_train:0.7328 recall_train:0.4129 F1_train:0.5282 AUC_train:0.6474\n",
      "acc_val:0.6000 pre_val:0.6087 recall_val:0.5600 F1_val:0.583333 AUC_val:0.6176\n",
      "Epoch:0070\n",
      "acc_train:0.6022 pre_train:0.7202 recall_train:0.3763 F1_train:0.4944 AUC_train:0.6248\n",
      "acc_val:0.6100 pre_val:0.6410 recall_val:0.5000 F1_val:0.561798 AUC_val:0.6172\n",
      "Epoch:0071\n",
      "acc_train:0.6233 pre_train:0.7019 recall_train:0.4710 F1_train:0.5637 AUC_train:0.6454\n",
      "acc_val:0.6100 pre_val:0.6410 recall_val:0.5000 F1_val:0.561798 AUC_val:0.6192\n",
      "Epoch:0072\n",
      "acc_train:0.6233 pre_train:0.7561 recall_train:0.4000 F1_train:0.5232 AUC_train:0.6582\n",
      "acc_val:0.6300 pre_val:0.6970 recall_val:0.4600 F1_val:0.554217 AUC_val:0.6160\n",
      "Epoch:0073\n",
      "acc_train:0.6078 pre_train:0.7295 recall_train:0.3828 F1_train:0.5021 AUC_train:0.6480\n",
      "acc_val:0.6100 pre_val:0.6897 recall_val:0.4000 F1_val:0.506329 AUC_val:0.6116\n",
      "Epoch:0074\n",
      "acc_train:0.6111 pre_train:0.7273 recall_train:0.3957 F1_train:0.5125 AUC_train:0.6583\n",
      "acc_val:0.6200 pre_val:0.7308 recall_val:0.3800 F1_val:0.500000 AUC_val:0.6140\n",
      "Epoch:0075\n",
      "acc_train:0.6133 pre_train:0.7427 recall_train:0.3849 F1_train:0.5071 AUC_train:0.6714\n",
      "acc_val:0.6200 pre_val:0.7308 recall_val:0.3800 F1_val:0.500000 AUC_val:0.6153\n",
      "Early Stopping!!! epoch：74\n",
      " Starting the 1-8 Fold:：\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.5233 pre_train:0.5287 recall_train:0.7140 F1_train:0.6075 AUC_train:0.5232\n",
      "acc_val:0.5100 pre_val:1.0000 recall_val:0.0200 F1_val:0.039216 AUC_val:0.4864\n",
      "Epoch:0002\n",
      "acc_train:0.5111 pre_train:0.5204 recall_train:0.6860 F1_train:0.5918 AUC_train:0.5022\n",
      "acc_val:0.4900 pre_val:0.4444 recall_val:0.0800 F1_val:0.135593 AUC_val:0.4976\n",
      "Epoch:0003\n",
      "acc_train:0.5322 pre_train:0.5370 recall_train:0.6860 F1_train:0.6025 AUC_train:0.5324\n",
      "acc_val:0.5500 pre_val:0.6667 recall_val:0.2000 F1_val:0.307692 AUC_val:0.5132\n",
      "Epoch:0004\n",
      "acc_train:0.4978 pre_train:0.5108 recall_train:0.6602 F1_train:0.5760 AUC_train:0.5077\n",
      "acc_val:0.5500 pre_val:0.6667 recall_val:0.2000 F1_val:0.307692 AUC_val:0.5900\n",
      "Epoch:0005\n",
      "acc_train:0.5289 pre_train:0.5379 recall_train:0.6258 F1_train:0.5785 AUC_train:0.5483\n",
      "acc_val:0.5700 pre_val:0.6400 recall_val:0.3200 F1_val:0.426667 AUC_val:0.6112\n",
      "Epoch:0006\n",
      "acc_train:0.5411 pre_train:0.5469 recall_train:0.6516 F1_train:0.5947 AUC_train:0.5742\n",
      "acc_val:0.5900 pre_val:0.6452 recall_val:0.4000 F1_val:0.493827 AUC_val:0.6332\n",
      "Epoch:0007\n",
      "acc_train:0.5767 pre_train:0.5753 recall_train:0.6903 F1_train:0.6276 AUC_train:0.6119\n",
      "acc_val:0.5100 pre_val:0.5143 recall_val:0.3600 F1_val:0.423529 AUC_val:0.6020\n",
      "Epoch:0008\n",
      "acc_train:0.5567 pre_train:0.5637 recall_train:0.6280 F1_train:0.5941 AUC_train:0.5776\n",
      "acc_val:0.5400 pre_val:0.5417 recall_val:0.5200 F1_val:0.530612 AUC_val:0.5776\n",
      "Epoch:0009\n",
      "acc_train:0.5756 pre_train:0.5900 recall_train:0.5849 F1_train:0.5875 AUC_train:0.6048\n",
      "acc_val:0.5900 pre_val:0.5849 recall_val:0.6200 F1_val:0.601942 AUC_val:0.5928\n",
      "Epoch:0010\n",
      "acc_train:0.5578 pre_train:0.5674 recall_train:0.6065 F1_train:0.5863 AUC_train:0.5629\n",
      "acc_val:0.5600 pre_val:0.5577 recall_val:0.5800 F1_val:0.568627 AUC_val:0.5896\n",
      "Epoch:0011\n",
      "acc_train:0.5867 pre_train:0.5876 recall_train:0.6710 F1_train:0.6265 AUC_train:0.6076\n",
      "acc_val:0.6000 pre_val:0.5893 recall_val:0.6600 F1_val:0.622642 AUC_val:0.5964\n",
      "Epoch:0012\n",
      "acc_train:0.6133 pre_train:0.6191 recall_train:0.6538 F1_train:0.6360 AUC_train:0.6404\n",
      "acc_val:0.6200 pre_val:0.6000 recall_val:0.7200 F1_val:0.654545 AUC_val:0.6228\n",
      "Epoch:0013\n",
      "acc_train:0.5933 pre_train:0.6000 recall_train:0.6387 F1_train:0.6188 AUC_train:0.6230\n",
      "acc_val:0.6200 pre_val:0.6000 recall_val:0.7200 F1_val:0.654545 AUC_val:0.6348\n",
      "Epoch:0014\n",
      "acc_train:0.6144 pre_train:0.6143 recall_train:0.6817 F1_train:0.6463 AUC_train:0.6771\n",
      "acc_val:0.6200 pre_val:0.6000 recall_val:0.7200 F1_val:0.654545 AUC_val:0.6424\n",
      "Epoch:0015\n",
      "acc_train:0.6322 pre_train:0.6329 recall_train:0.6860 F1_train:0.6584 AUC_train:0.6830\n",
      "acc_val:0.6200 pre_val:0.6000 recall_val:0.7200 F1_val:0.654545 AUC_val:0.6456\n",
      "Epoch:0016\n",
      "acc_train:0.5967 pre_train:0.5941 recall_train:0.6925 F1_train:0.6395 AUC_train:0.6327\n",
      "acc_val:0.6200 pre_val:0.5938 recall_val:0.7600 F1_val:0.666667 AUC_val:0.6464\n",
      "Epoch:0017\n",
      "acc_train:0.6011 pre_train:0.6056 recall_train:0.6538 F1_train:0.6287 AUC_train:0.6260\n",
      "acc_val:0.6100 pre_val:0.5902 recall_val:0.7200 F1_val:0.648649 AUC_val:0.6272\n",
      "Epoch:0018\n",
      "acc_train:0.6367 pre_train:0.6342 recall_train:0.7011 F1_train:0.6660 AUC_train:0.6974\n",
      "acc_val:0.6000 pre_val:0.5833 recall_val:0.7000 F1_val:0.636364 AUC_val:0.6348\n",
      "Epoch:0019\n",
      "acc_train:0.6389 pre_train:0.6311 recall_train:0.7247 F1_train:0.6747 AUC_train:0.6910\n",
      "acc_val:0.6100 pre_val:0.5932 recall_val:0.7000 F1_val:0.642202 AUC_val:0.6384\n",
      "Epoch:0020\n",
      "acc_train:0.6167 pre_train:0.6115 recall_train:0.7075 F1_train:0.6560 AUC_train:0.6749\n",
      "acc_val:0.6000 pre_val:0.5862 recall_val:0.6800 F1_val:0.629630 AUC_val:0.6420\n",
      "Epoch:0021\n",
      "acc_train:0.6400 pre_train:0.6313 recall_train:0.7290 F1_train:0.6766 AUC_train:0.7009\n",
      "acc_val:0.6300 pre_val:0.6140 recall_val:0.7000 F1_val:0.654206 AUC_val:0.6508\n",
      "Epoch:0022\n",
      "acc_train:0.6522 pre_train:0.6467 recall_train:0.7204 F1_train:0.6816 AUC_train:0.6970\n",
      "acc_val:0.6300 pre_val:0.6140 recall_val:0.7000 F1_val:0.654206 AUC_val:0.6620\n",
      "Epoch:0023\n",
      "acc_train:0.6944 pre_train:0.6721 recall_train:0.7978 F1_train:0.7296 AUC_train:0.7561\n",
      "acc_val:0.6200 pre_val:0.6034 recall_val:0.7000 F1_val:0.648148 AUC_val:0.6844\n",
      "Epoch:0024\n",
      "acc_train:0.6722 pre_train:0.6622 recall_train:0.7462 F1_train:0.7017 AUC_train:0.7382\n",
      "acc_val:0.6400 pre_val:0.6207 recall_val:0.7200 F1_val:0.666667 AUC_val:0.6956\n",
      "Epoch:0025\n",
      "acc_train:0.7089 pre_train:0.6816 recall_train:0.8194 F1_train:0.7441 AUC_train:0.7721\n",
      "acc_val:0.6500 pre_val:0.6190 recall_val:0.7800 F1_val:0.690265 AUC_val:0.7128\n",
      "Epoch:0026\n",
      "acc_train:0.6956 pre_train:0.6644 recall_train:0.8301 F1_train:0.7380 AUC_train:0.7426\n",
      "acc_val:0.6800 pre_val:0.6406 recall_val:0.8200 F1_val:0.719298 AUC_val:0.7288\n",
      "Epoch:0027\n",
      "acc_train:0.7167 pre_train:0.6792 recall_train:0.8559 F1_train:0.7574 AUC_train:0.7800\n",
      "acc_val:0.6600 pre_val:0.6176 recall_val:0.8400 F1_val:0.711864 AUC_val:0.7404\n",
      "Epoch:0028\n",
      "acc_train:0.6989 pre_train:0.6554 recall_train:0.8796 F1_train:0.7511 AUC_train:0.7575\n",
      "acc_val:0.6600 pre_val:0.6111 recall_val:0.8800 F1_val:0.721311 AUC_val:0.7480\n",
      "Epoch:0029\n",
      "acc_train:0.7544 pre_train:0.7068 recall_train:0.8968 F1_train:0.7905 AUC_train:0.8093\n",
      "acc_val:0.6900 pre_val:0.6377 recall_val:0.8800 F1_val:0.739496 AUC_val:0.7552\n",
      "Epoch:0030\n",
      "acc_train:0.7656 pre_train:0.7205 recall_train:0.8925 F1_train:0.7973 AUC_train:0.8182\n",
      "acc_val:0.6800 pre_val:0.6406 recall_val:0.8200 F1_val:0.719298 AUC_val:0.7464\n",
      "Epoch:0031\n",
      "acc_train:0.7511 pre_train:0.7067 recall_train:0.8860 F1_train:0.7863 AUC_train:0.8166\n",
      "acc_val:0.6900 pre_val:0.6508 recall_val:0.8200 F1_val:0.725664 AUC_val:0.7540\n",
      "Epoch:0032\n",
      "acc_train:0.7678 pre_train:0.7238 recall_train:0.8903 F1_train:0.7985 AUC_train:0.8244\n",
      "acc_val:0.7100 pre_val:0.6780 recall_val:0.8000 F1_val:0.733945 AUC_val:0.7584\n",
      "Epoch:0033\n",
      "acc_train:0.7622 pre_train:0.7175 recall_train:0.8903 F1_train:0.7946 AUC_train:0.8046\n",
      "acc_val:0.6800 pre_val:0.6607 recall_val:0.7400 F1_val:0.698113 AUC_val:0.7456\n",
      "Epoch:0034\n",
      "acc_train:0.7467 pre_train:0.7054 recall_train:0.8753 F1_train:0.7812 AUC_train:0.8162\n",
      "acc_val:0.6800 pre_val:0.6731 recall_val:0.7000 F1_val:0.686275 AUC_val:0.7428\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0035\n",
      "acc_train:0.7533 pre_train:0.7135 recall_train:0.8731 F1_train:0.7853 AUC_train:0.8267\n",
      "acc_val:0.7100 pre_val:0.6981 recall_val:0.7400 F1_val:0.718447 AUC_val:0.7680\n",
      "Epoch:0036\n",
      "acc_train:0.7778 pre_train:0.7353 recall_train:0.8903 F1_train:0.8054 AUC_train:0.8286\n",
      "acc_val:0.7300 pre_val:0.7255 recall_val:0.7400 F1_val:0.732673 AUC_val:0.7820\n",
      "Epoch:0037\n",
      "acc_train:0.7622 pre_train:0.7221 recall_train:0.8774 F1_train:0.7922 AUC_train:0.8198\n",
      "acc_val:0.7600 pre_val:0.7500 recall_val:0.7800 F1_val:0.764706 AUC_val:0.7920\n",
      "Epoch:0038\n",
      "acc_train:0.7911 pre_train:0.7505 recall_train:0.8925 F1_train:0.8153 AUC_train:0.8418\n",
      "acc_val:0.7600 pre_val:0.7321 recall_val:0.8200 F1_val:0.773585 AUC_val:0.7944\n",
      "Epoch:0039\n",
      "acc_train:0.8044 pre_train:0.7651 recall_train:0.8968 F1_train:0.8257 AUC_train:0.8556\n",
      "acc_val:0.7700 pre_val:0.7288 recall_val:0.8600 F1_val:0.788991 AUC_val:0.7936\n",
      "Epoch:0040\n",
      "acc_train:0.8189 pre_train:0.7807 recall_train:0.9032 F1_train:0.8375 AUC_train:0.8556\n",
      "acc_val:0.7300 pre_val:0.6825 recall_val:0.8600 F1_val:0.761062 AUC_val:0.8012\n",
      "Epoch:0041\n",
      "acc_train:0.8056 pre_train:0.7636 recall_train:0.9032 F1_train:0.8276 AUC_train:0.8549\n",
      "acc_val:0.7200 pre_val:0.6719 recall_val:0.8600 F1_val:0.754386 AUC_val:0.8124\n",
      "Epoch:0042\n",
      "acc_train:0.8211 pre_train:0.7764 recall_train:0.9183 F1_train:0.8414 AUC_train:0.8735\n",
      "acc_val:0.7300 pre_val:0.6825 recall_val:0.8600 F1_val:0.761062 AUC_val:0.8080\n",
      "Epoch:0043\n",
      "acc_train:0.8522 pre_train:0.8063 recall_train:0.9398 F1_train:0.8679 AUC_train:0.8921\n",
      "acc_val:0.7400 pre_val:0.6875 recall_val:0.8800 F1_val:0.771930 AUC_val:0.8244\n",
      "Epoch:0044\n",
      "acc_train:0.8489 pre_train:0.8086 recall_train:0.9269 F1_train:0.8637 AUC_train:0.8927\n",
      "acc_val:0.7900 pre_val:0.7302 recall_val:0.9200 F1_val:0.814159 AUC_val:0.8452\n",
      "Epoch:0045\n",
      "acc_train:0.8389 pre_train:0.7857 recall_train:0.9462 F1_train:0.8585 AUC_train:0.9012\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.8472\n",
      "Epoch:0046\n",
      "acc_train:0.8456 pre_train:0.7932 recall_train:0.9484 F1_train:0.8639 AUC_train:0.9154\n",
      "acc_val:0.7700 pre_val:0.7143 recall_val:0.9000 F1_val:0.796460 AUC_val:0.8524\n",
      "Epoch:0047\n",
      "acc_train:0.8489 pre_train:0.7912 recall_train:0.9613 F1_train:0.8680 AUC_train:0.9141\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.8568\n",
      "Epoch:0048\n",
      "acc_train:0.8667 pre_train:0.8119 recall_train:0.9656 F1_train:0.8821 AUC_train:0.9238\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.8652\n",
      "Epoch:0049\n",
      "acc_train:0.8578 pre_train:0.7972 recall_train:0.9720 F1_train:0.8760 AUC_train:0.9189\n",
      "acc_val:0.7500 pre_val:0.6812 recall_val:0.9400 F1_val:0.789916 AUC_val:0.8688\n",
      "Epoch:0050\n",
      "acc_train:0.8733 pre_train:0.8197 recall_train:0.9677 F1_train:0.8876 AUC_train:0.9185\n",
      "acc_val:0.7600 pre_val:0.6857 recall_val:0.9600 F1_val:0.800000 AUC_val:0.8736\n",
      "Epoch:0051\n",
      "acc_train:0.8733 pre_train:0.8220 recall_train:0.9634 F1_train:0.8871 AUC_train:0.9354\n",
      "acc_val:0.7700 pre_val:0.6901 recall_val:0.9800 F1_val:0.809917 AUC_val:0.8792\n",
      "Epoch:0052\n",
      "acc_train:0.8678 pre_train:0.8168 recall_train:0.9591 F1_train:0.8823 AUC_train:0.9335\n",
      "acc_val:0.7700 pre_val:0.6901 recall_val:0.9800 F1_val:0.809917 AUC_val:0.8796\n",
      "Epoch:0053\n",
      "acc_train:0.8800 pre_train:0.8275 recall_train:0.9699 F1_train:0.8931 AUC_train:0.9434\n",
      "acc_val:0.7800 pre_val:0.7000 recall_val:0.9800 F1_val:0.816667 AUC_val:0.8852\n",
      "Epoch:0054\n",
      "acc_train:0.8978 pre_train:0.8422 recall_train:0.9871 F1_train:0.9089 AUC_train:0.9557\n",
      "acc_val:0.7800 pre_val:0.7000 recall_val:0.9800 F1_val:0.816667 AUC_val:0.8844\n",
      "Epoch:0055\n",
      "acc_train:0.8922 pre_train:0.8420 recall_train:0.9742 F1_train:0.9033 AUC_train:0.9461\n",
      "acc_val:0.7800 pre_val:0.7000 recall_val:0.9800 F1_val:0.816667 AUC_val:0.8856\n",
      "Epoch:0056\n",
      "acc_train:0.8833 pre_train:0.8358 recall_train:0.9634 F1_train:0.8951 AUC_train:0.9466\n",
      "acc_val:0.7900 pre_val:0.7101 recall_val:0.9800 F1_val:0.823529 AUC_val:0.8848\n",
      "Epoch:0057\n",
      "acc_train:0.8978 pre_train:0.8499 recall_train:0.9742 F1_train:0.9078 AUC_train:0.9570\n",
      "acc_val:0.7900 pre_val:0.7101 recall_val:0.9800 F1_val:0.823529 AUC_val:0.8784\n",
      "Epoch:0058\n",
      "acc_train:0.8856 pre_train:0.8315 recall_train:0.9763 F1_train:0.8981 AUC_train:0.9486\n",
      "acc_val:0.7900 pre_val:0.7101 recall_val:0.9800 F1_val:0.823529 AUC_val:0.8732\n",
      "Epoch:0059\n",
      "acc_train:0.8833 pre_train:0.8321 recall_train:0.9699 F1_train:0.8957 AUC_train:0.9397\n",
      "acc_val:0.7900 pre_val:0.7101 recall_val:0.9800 F1_val:0.823529 AUC_val:0.8760\n",
      "Epoch:0060\n",
      "acc_train:0.9078 pre_train:0.8604 recall_train:0.9806 F1_train:0.9166 AUC_train:0.9624\n",
      "acc_val:0.7900 pre_val:0.7101 recall_val:0.9800 F1_val:0.823529 AUC_val:0.8840\n",
      "Epoch:0061\n",
      "acc_train:0.9089 pre_train:0.8634 recall_train:0.9785 F1_train:0.9173 AUC_train:0.9495\n",
      "acc_val:0.8000 pre_val:0.7206 recall_val:0.9800 F1_val:0.830508 AUC_val:0.8900\n",
      "Epoch:0062\n",
      "acc_train:0.8933 pre_train:0.8501 recall_train:0.9634 F1_train:0.9032 AUC_train:0.9363\n",
      "acc_val:0.7900 pre_val:0.7101 recall_val:0.9800 F1_val:0.823529 AUC_val:0.8948\n",
      "Epoch:0063\n",
      "acc_train:0.9067 pre_train:0.8728 recall_train:0.9591 F1_train:0.9139 AUC_train:0.9542\n",
      "acc_val:0.7600 pre_val:0.6806 recall_val:0.9800 F1_val:0.803279 AUC_val:0.9016\n",
      "Epoch:0064\n",
      "acc_train:0.9167 pre_train:0.8764 recall_train:0.9763 F1_train:0.9237 AUC_train:0.9667\n",
      "acc_val:0.7500 pre_val:0.6712 recall_val:0.9800 F1_val:0.796748 AUC_val:0.9104\n",
      "Epoch:0065\n",
      "acc_train:0.9311 pre_train:0.8959 recall_train:0.9806 F1_train:0.9363 AUC_train:0.9638\n",
      "acc_val:0.7400 pre_val:0.6622 recall_val:0.9800 F1_val:0.790323 AUC_val:0.9124\n",
      "Epoch:0066\n",
      "acc_train:0.9200 pre_train:0.8743 recall_train:0.9871 F1_train:0.9273 AUC_train:0.9588\n",
      "acc_val:0.7400 pre_val:0.6622 recall_val:0.9800 F1_val:0.790323 AUC_val:0.9064\n",
      "Epoch:0067\n",
      "acc_train:0.9200 pre_train:0.8757 recall_train:0.9849 F1_train:0.9271 AUC_train:0.9691\n",
      "acc_val:0.7400 pre_val:0.6667 recall_val:0.9600 F1_val:0.786885 AUC_val:0.9048\n",
      "Epoch:0068\n",
      "acc_train:0.9267 pre_train:0.8844 recall_train:0.9871 F1_train:0.9329 AUC_train:0.9635\n",
      "acc_val:0.7400 pre_val:0.6667 recall_val:0.9600 F1_val:0.786885 AUC_val:0.9112\n",
      "Epoch:0069\n",
      "acc_train:0.9256 pre_train:0.8798 recall_train:0.9914 F1_train:0.9323 AUC_train:0.9723\n",
      "acc_val:0.7400 pre_val:0.6667 recall_val:0.9600 F1_val:0.786885 AUC_val:0.9264\n",
      "Epoch:0070\n",
      "acc_train:0.9311 pre_train:0.8928 recall_train:0.9849 F1_train:0.9366 AUC_train:0.9732\n",
      "acc_val:0.7300 pre_val:0.6575 recall_val:0.9600 F1_val:0.780488 AUC_val:0.9264\n",
      "Epoch:0071\n",
      "acc_train:0.9433 pre_train:0.9107 recall_train:0.9871 F1_train:0.9474 AUC_train:0.9883\n",
      "acc_val:0.7500 pre_val:0.6761 recall_val:0.9600 F1_val:0.793388 AUC_val:0.9244\n",
      "Epoch:0072\n",
      "acc_train:0.9344 pre_train:0.9012 recall_train:0.9806 F1_train:0.9392 AUC_train:0.9694\n",
      "acc_val:0.7500 pre_val:0.6812 recall_val:0.9400 F1_val:0.789916 AUC_val:0.9192\n",
      "Epoch:0073\n",
      "acc_train:0.9367 pre_train:0.8984 recall_train:0.9892 F1_train:0.9417 AUC_train:0.9721\n",
      "acc_val:0.7500 pre_val:0.6812 recall_val:0.9400 F1_val:0.789916 AUC_val:0.9140\n",
      "Epoch:0074\n",
      "acc_train:0.9267 pre_train:0.8859 recall_train:0.9849 F1_train:0.9328 AUC_train:0.9699\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.9132\n",
      "Epoch:0075\n",
      "acc_train:0.9433 pre_train:0.9107 recall_train:0.9871 F1_train:0.9474 AUC_train:0.9771\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9092\n",
      "Epoch:0076\n",
      "acc_train:0.9344 pre_train:0.8996 recall_train:0.9828 F1_train:0.9394 AUC_train:0.9666\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9108\n",
      "Epoch:0077\n",
      "acc_train:0.9489 pre_train:0.9149 recall_train:0.9935 F1_train:0.9526 AUC_train:0.9739\n",
      "acc_val:0.7600 pre_val:0.6912 recall_val:0.9400 F1_val:0.796610 AUC_val:0.9180\n",
      "Epoch:0078\n",
      "acc_train:0.9322 pre_train:0.9056 recall_train:0.9699 F1_train:0.9367 AUC_train:0.9769\n",
      "acc_val:0.7600 pre_val:0.6912 recall_val:0.9400 F1_val:0.796610 AUC_val:0.9236\n",
      "Epoch:0079\n",
      "acc_train:0.9422 pre_train:0.9057 recall_train:0.9914 F1_train:0.9466 AUC_train:0.9778\n",
      "acc_val:0.7300 pre_val:0.6575 recall_val:0.9600 F1_val:0.780488 AUC_val:0.9304\n",
      "Epoch:0080\n",
      "acc_train:0.9278 pre_train:0.8891 recall_train:0.9828 F1_train:0.9336 AUC_train:0.9799\n",
      "acc_val:0.7200 pre_val:0.6486 recall_val:0.9600 F1_val:0.774194 AUC_val:0.9296\n",
      "Epoch:0081\n",
      "acc_train:0.9544 pre_train:0.9344 recall_train:0.9806 F1_train:0.9570 AUC_train:0.9832\n",
      "acc_val:0.7100 pre_val:0.6400 recall_val:0.9600 F1_val:0.768000 AUC_val:0.9292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0082\n",
      "acc_train:0.9567 pre_train:0.9260 recall_train:0.9957 F1_train:0.9596 AUC_train:0.9857\n",
      "acc_val:0.7200 pre_val:0.6486 recall_val:0.9600 F1_val:0.774194 AUC_val:0.9232\n",
      "Epoch:0083\n",
      "acc_train:0.9511 pre_train:0.9253 recall_train:0.9849 F1_train:0.9542 AUC_train:0.9853\n",
      "acc_val:0.7400 pre_val:0.6667 recall_val:0.9600 F1_val:0.786885 AUC_val:0.9148\n",
      "Epoch:0084\n",
      "acc_train:0.9378 pre_train:0.9034 recall_train:0.9849 F1_train:0.9424 AUC_train:0.9797\n",
      "acc_val:0.7300 pre_val:0.6620 recall_val:0.9400 F1_val:0.776860 AUC_val:0.9136\n",
      "Epoch:0085\n",
      "acc_train:0.9456 pre_train:0.9194 recall_train:0.9806 F1_train:0.9490 AUC_train:0.9794\n",
      "acc_val:0.7000 pre_val:0.6351 recall_val:0.9400 F1_val:0.758065 AUC_val:0.9144\n",
      "Epoch:0086\n",
      "acc_train:0.9544 pre_train:0.9240 recall_train:0.9935 F1_train:0.9575 AUC_train:0.9883\n",
      "acc_val:0.7100 pre_val:0.6438 recall_val:0.9400 F1_val:0.764228 AUC_val:0.9176\n",
      "Epoch:0087\n",
      "acc_train:0.9522 pre_train:0.9271 recall_train:0.9849 F1_train:0.9552 AUC_train:0.9791\n",
      "acc_val:0.7100 pre_val:0.6438 recall_val:0.9400 F1_val:0.764228 AUC_val:0.9112\n",
      "Epoch:0088\n",
      "acc_train:0.9533 pre_train:0.9238 recall_train:0.9914 F1_train:0.9564 AUC_train:0.9859\n",
      "acc_val:0.7300 pre_val:0.6620 recall_val:0.9400 F1_val:0.776860 AUC_val:0.9082\n",
      "Epoch:0089\n",
      "acc_train:0.9567 pre_train:0.9329 recall_train:0.9871 F1_train:0.9592 AUC_train:0.9883\n",
      "acc_val:0.7300 pre_val:0.6620 recall_val:0.9400 F1_val:0.776860 AUC_val:0.9078\n",
      "Epoch:0090\n",
      "acc_train:0.9667 pre_train:0.9503 recall_train:0.9871 F1_train:0.9684 AUC_train:0.9917\n",
      "acc_val:0.7300 pre_val:0.6620 recall_val:0.9400 F1_val:0.776860 AUC_val:0.9062\n",
      "Epoch:0091\n",
      "acc_train:0.9611 pre_train:0.9442 recall_train:0.9828 F1_train:0.9631 AUC_train:0.9892\n",
      "acc_val:0.7400 pre_val:0.6714 recall_val:0.9400 F1_val:0.783333 AUC_val:0.9044\n",
      "Epoch:0092\n",
      "acc_train:0.9633 pre_train:0.9426 recall_train:0.9892 F1_train:0.9654 AUC_train:0.9898\n",
      "acc_val:0.7300 pre_val:0.6620 recall_val:0.9400 F1_val:0.776860 AUC_val:0.9076\n",
      "Epoch:0093\n",
      "acc_train:0.9633 pre_train:0.9355 recall_train:0.9978 F1_train:0.9657 AUC_train:0.9874\n",
      "acc_val:0.7300 pre_val:0.6620 recall_val:0.9400 F1_val:0.776860 AUC_val:0.9044\n",
      "Epoch:0094\n",
      "acc_train:0.9622 pre_train:0.9354 recall_train:0.9957 F1_train:0.9646 AUC_train:0.9900\n",
      "acc_val:0.7200 pre_val:0.6528 recall_val:0.9400 F1_val:0.770492 AUC_val:0.9056\n",
      "Epoch:0095\n",
      "acc_train:0.9600 pre_train:0.9441 recall_train:0.9806 F1_train:0.9620 AUC_train:0.9887\n",
      "acc_val:0.6900 pre_val:0.6267 recall_val:0.9400 F1_val:0.752000 AUC_val:0.9110\n",
      "Epoch:0096\n",
      "acc_train:0.9611 pre_train:0.9370 recall_train:0.9914 F1_train:0.9634 AUC_train:0.9891\n",
      "acc_val:0.6900 pre_val:0.6267 recall_val:0.9400 F1_val:0.752000 AUC_val:0.9142\n",
      "Early Stopping!!! epoch：95\n",
      " Starting the 1-9 Fold:：\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.5167 pre_train:0.5371 recall_train:0.4667 F1_train:0.4994 AUC_train:0.5222\n",
      "acc_val:0.5700 pre_val:0.7059 recall_val:0.2400 F1_val:0.358209 AUC_val:0.4860\n",
      "Epoch:0002\n",
      "acc_train:0.5622 pre_train:0.6066 recall_train:0.4344 F1_train:0.5063 AUC_train:0.5522\n",
      "acc_val:0.5800 pre_val:0.7000 recall_val:0.2800 F1_val:0.400000 AUC_val:0.5620\n",
      "Epoch:0003\n",
      "acc_train:0.5189 pre_train:0.5272 recall_train:0.6667 F1_train:0.5888 AUC_train:0.5504\n",
      "acc_val:0.5300 pre_val:0.5283 recall_val:0.5600 F1_val:0.543689 AUC_val:0.5060\n",
      "Epoch:0004\n",
      "acc_train:0.5122 pre_train:0.5241 recall_train:0.6086 F1_train:0.5632 AUC_train:0.5389\n",
      "acc_val:0.5600 pre_val:0.5405 recall_val:0.8000 F1_val:0.645161 AUC_val:0.6312\n",
      "Epoch:0005\n",
      "acc_train:0.4944 pre_train:0.5094 recall_train:0.5828 F1_train:0.5436 AUC_train:0.5144\n",
      "acc_val:0.5700 pre_val:0.5443 recall_val:0.8600 F1_val:0.666667 AUC_val:0.6660\n",
      "Epoch:0006\n",
      "acc_train:0.4689 pre_train:0.4898 recall_train:0.6710 F1_train:0.5662 AUC_train:0.4852\n",
      "acc_val:0.5500 pre_val:0.5294 recall_val:0.9000 F1_val:0.666667 AUC_val:0.6996\n",
      "Epoch:0007\n",
      "acc_train:0.5478 pre_train:0.5792 recall_train:0.4559 F1_train:0.5102 AUC_train:0.5522\n",
      "acc_val:0.5600 pre_val:0.5349 recall_val:0.9200 F1_val:0.676471 AUC_val:0.7224\n",
      "Epoch:0008\n",
      "acc_train:0.5111 pre_train:0.5177 recall_train:0.7871 F1_train:0.6246 AUC_train:0.5311\n",
      "acc_val:0.5600 pre_val:0.5326 recall_val:0.9800 F1_val:0.690141 AUC_val:0.7072\n",
      "Epoch:0009\n",
      "acc_train:0.5322 pre_train:0.5349 recall_train:0.7247 F1_train:0.6155 AUC_train:0.5696\n",
      "acc_val:0.5600 pre_val:0.5341 recall_val:0.9400 F1_val:0.681159 AUC_val:0.7044\n",
      "Epoch:0010\n",
      "acc_train:0.5322 pre_train:0.5791 recall_train:0.3462 F1_train:0.4334 AUC_train:0.5486\n",
      "acc_val:0.5500 pre_val:0.5281 recall_val:0.9400 F1_val:0.676259 AUC_val:0.6780\n",
      "Epoch:0011\n",
      "acc_train:0.5789 pre_train:0.6617 recall_train:0.3785 F1_train:0.4815 AUC_train:0.6009\n",
      "acc_val:0.5700 pre_val:0.5412 recall_val:0.9200 F1_val:0.681481 AUC_val:0.6772\n",
      "Epoch:0012\n",
      "acc_train:0.5633 pre_train:0.5957 recall_train:0.4817 F1_train:0.5327 AUC_train:0.5689\n",
      "acc_val:0.5600 pre_val:0.5357 recall_val:0.9000 F1_val:0.671642 AUC_val:0.6784\n",
      "Epoch:0013\n",
      "acc_train:0.5811 pre_train:0.6438 recall_train:0.4237 F1_train:0.5110 AUC_train:0.6008\n",
      "acc_val:0.5600 pre_val:0.5375 recall_val:0.8600 F1_val:0.661538 AUC_val:0.6904\n",
      "Epoch:0014\n",
      "acc_train:0.5733 pre_train:0.6302 recall_train:0.4215 F1_train:0.5052 AUC_train:0.5813\n",
      "acc_val:0.5400 pre_val:0.5238 recall_val:0.8800 F1_val:0.656716 AUC_val:0.6748\n",
      "Epoch:0015\n",
      "acc_train:0.5411 pre_train:0.5506 recall_train:0.6086 F1_train:0.5781 AUC_train:0.5875\n",
      "acc_val:0.5400 pre_val:0.5244 recall_val:0.8600 F1_val:0.651515 AUC_val:0.6680\n",
      "Epoch:0016\n",
      "acc_train:0.5511 pre_train:0.5560 recall_train:0.6516 F1_train:0.6000 AUC_train:0.5919\n",
      "acc_val:0.5300 pre_val:0.5181 recall_val:0.8600 F1_val:0.646617 AUC_val:0.6612\n",
      "Epoch:0017\n",
      "acc_train:0.5733 pre_train:0.5879 recall_train:0.5828 F1_train:0.5853 AUC_train:0.6055\n",
      "acc_val:0.5400 pre_val:0.5244 recall_val:0.8600 F1_val:0.651515 AUC_val:0.6572\n",
      "Epoch:0018\n",
      "acc_train:0.5289 pre_train:0.5278 recall_train:0.8366 F1_train:0.6473 AUC_train:0.5925\n",
      "acc_val:0.5300 pre_val:0.5190 recall_val:0.8200 F1_val:0.635659 AUC_val:0.6660\n",
      "Epoch:0019\n",
      "acc_train:0.5967 pre_train:0.6232 recall_train:0.5548 F1_train:0.5870 AUC_train:0.6144\n",
      "acc_val:0.5500 pre_val:0.5352 recall_val:0.7600 F1_val:0.628099 AUC_val:0.6688\n",
      "Epoch:0020\n",
      "acc_train:0.5911 pre_train:0.5791 recall_train:0.7634 F1_train:0.6586 AUC_train:0.6385\n",
      "acc_val:0.5900 pre_val:0.5672 recall_val:0.7600 F1_val:0.649573 AUC_val:0.6728\n",
      "Epoch:0021\n",
      "acc_train:0.5933 pre_train:0.6176 recall_train:0.5591 F1_train:0.5869 AUC_train:0.6210\n",
      "acc_val:0.6300 pre_val:0.6032 recall_val:0.7600 F1_val:0.672566 AUC_val:0.6704\n",
      "Epoch:0022\n",
      "acc_train:0.5889 pre_train:0.5878 recall_train:0.6839 F1_train:0.6322 AUC_train:0.6224\n",
      "acc_val:0.6400 pre_val:0.6591 recall_val:0.5800 F1_val:0.617021 AUC_val:0.6712\n",
      "Epoch:0023\n",
      "acc_train:0.6144 pre_train:0.7049 recall_train:0.4366 F1_train:0.5392 AUC_train:0.6312\n",
      "acc_val:0.6300 pre_val:0.6667 recall_val:0.5200 F1_val:0.584270 AUC_val:0.6704\n",
      "Epoch:0024\n",
      "acc_train:0.5922 pre_train:0.6667 recall_train:0.4215 F1_train:0.5165 AUC_train:0.5890\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6736\n",
      "Epoch:0025\n",
      "acc_train:0.5700 pre_train:0.5915 recall_train:0.5419 F1_train:0.5657 AUC_train:0.6081\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6768\n",
      "Epoch:0026\n",
      "acc_train:0.5567 pre_train:0.5465 recall_train:0.8344 F1_train:0.6604 AUC_train:0.6073\n",
      "acc_val:0.6100 pre_val:0.5965 recall_val:0.6800 F1_val:0.635514 AUC_val:0.6756\n",
      "Epoch:0027\n",
      "acc_train:0.5300 pre_train:0.5326 recall_train:0.7376 F1_train:0.6186 AUC_train:0.5835\n",
      "acc_val:0.6100 pre_val:0.5965 recall_val:0.6800 F1_val:0.635514 AUC_val:0.6788\n",
      "Epoch:0028\n",
      "acc_train:0.5911 pre_train:0.6131 recall_train:0.5656 F1_train:0.5884 AUC_train:0.6132\n",
      "acc_val:0.6400 pre_val:0.6458 recall_val:0.6200 F1_val:0.632653 AUC_val:0.6836\n",
      "Epoch:0029\n",
      "acc_train:0.4844 pre_train:0.5007 recall_train:0.8043 F1_train:0.6172 AUC_train:0.5212\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.9600 F1_val:0.657534 AUC_val:0.6284\n",
      "Epoch:0030\n",
      "acc_train:0.5589 pre_train:0.5601 recall_train:0.6817 F1_train:0.6149 AUC_train:0.6171\n",
      "acc_val:0.5200 pre_val:0.5125 recall_val:0.8200 F1_val:0.630769 AUC_val:0.6256\n",
      "Epoch:0031\n",
      "acc_train:0.5489 pre_train:0.5501 recall_train:0.6968 F1_train:0.6148 AUC_train:0.5884\n",
      "acc_val:0.6200 pre_val:0.6071 recall_val:0.6800 F1_val:0.641509 AUC_val:0.6232\n",
      "Epoch:0032\n",
      "acc_train:0.6056 pre_train:0.6554 recall_train:0.4989 F1_train:0.5665 AUC_train:0.6351\n",
      "acc_val:0.6100 pre_val:0.6410 recall_val:0.5000 F1_val:0.561798 AUC_val:0.6208\n",
      "Epoch:0033\n",
      "acc_train:0.5956 pre_train:0.6004 recall_train:0.6495 F1_train:0.6240 AUC_train:0.6193\n",
      "acc_val:0.6300 pre_val:0.6857 recall_val:0.4800 F1_val:0.564706 AUC_val:0.6120\n",
      "Epoch:0034\n",
      "acc_train:0.5656 pre_train:0.6445 recall_train:0.3548 F1_train:0.4577 AUC_train:0.5932\n",
      "acc_val:0.6400 pre_val:0.7059 recall_val:0.4800 F1_val:0.571429 AUC_val:0.6004\n",
      "Epoch:0035\n",
      "acc_train:0.6178 pre_train:0.7153 recall_train:0.4323 F1_train:0.5389 AUC_train:0.6315\n",
      "acc_val:0.6500 pre_val:0.7419 recall_val:0.4600 F1_val:0.567901 AUC_val:0.5992\n",
      "Epoch:0036\n",
      "acc_train:0.5944 pre_train:0.6309 recall_train:0.5183 F1_train:0.5691 AUC_train:0.6264\n",
      "acc_val:0.6400 pre_val:0.7500 recall_val:0.4200 F1_val:0.538462 AUC_val:0.5968\n",
      "Epoch:0037\n",
      "acc_train:0.6244 pre_train:0.7042 recall_train:0.4710 F1_train:0.5644 AUC_train:0.6532\n",
      "acc_val:0.6300 pre_val:0.7407 recall_val:0.4000 F1_val:0.519481 AUC_val:0.5972\n",
      "Epoch:0038\n",
      "acc_train:0.6133 pre_train:0.6767 recall_train:0.4817 F1_train:0.5628 AUC_train:0.6275\n",
      "acc_val:0.6300 pre_val:0.7407 recall_val:0.4000 F1_val:0.519481 AUC_val:0.5976\n",
      "Epoch:0039\n",
      "acc_train:0.5644 pre_train:0.6229 recall_train:0.3978 F1_train:0.4856 AUC_train:0.5689\n",
      "acc_val:0.6400 pre_val:0.7500 recall_val:0.4200 F1_val:0.538462 AUC_val:0.5840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0040\n",
      "acc_train:0.5833 pre_train:0.6957 recall_train:0.3441 F1_train:0.4604 AUC_train:0.5962\n",
      "acc_val:0.6400 pre_val:0.7500 recall_val:0.4200 F1_val:0.538462 AUC_val:0.5912\n",
      "Epoch:0041\n",
      "acc_train:0.6067 pre_train:0.7110 recall_train:0.4022 F1_train:0.5137 AUC_train:0.6378\n",
      "acc_val:0.6400 pre_val:0.7500 recall_val:0.4200 F1_val:0.538462 AUC_val:0.5908\n",
      "Epoch:0042\n",
      "acc_train:0.6256 pre_train:0.6963 recall_train:0.4882 F1_train:0.5740 AUC_train:0.6384\n",
      "acc_val:0.6400 pre_val:0.7500 recall_val:0.4200 F1_val:0.538462 AUC_val:0.5912\n",
      "Epoch:0043\n",
      "acc_train:0.6256 pre_train:0.6882 recall_train:0.5032 F1_train:0.5814 AUC_train:0.6399\n",
      "acc_val:0.6500 pre_val:0.7586 recall_val:0.4400 F1_val:0.556962 AUC_val:0.5980\n",
      "Epoch:0044\n",
      "acc_train:0.6122 pre_train:0.6349 recall_train:0.5871 F1_train:0.6101 AUC_train:0.6329\n",
      "acc_val:0.6600 pre_val:0.7667 recall_val:0.4600 F1_val:0.575000 AUC_val:0.6076\n",
      "Epoch:0045\n",
      "acc_train:0.6256 pre_train:0.6951 recall_train:0.4903 F1_train:0.5750 AUC_train:0.6442\n",
      "acc_val:0.6300 pre_val:0.6857 recall_val:0.4800 F1_val:0.564706 AUC_val:0.6276\n",
      "Epoch:0046\n",
      "acc_train:0.4900 pre_train:0.5042 recall_train:0.7677 F1_train:0.6087 AUC_train:0.5425\n",
      "acc_val:0.6200 pre_val:0.6250 recall_val:0.6000 F1_val:0.612245 AUC_val:0.6452\n",
      "Epoch:0047\n",
      "acc_train:0.5489 pre_train:0.5499 recall_train:0.6989 F1_train:0.6155 AUC_train:0.6185\n",
      "acc_val:0.6200 pre_val:0.6500 recall_val:0.5200 F1_val:0.577778 AUC_val:0.6468\n",
      "Epoch:0048\n",
      "acc_train:0.5678 pre_train:0.5592 recall_train:0.7720 F1_train:0.6486 AUC_train:0.6484\n",
      "acc_val:0.6100 pre_val:0.6222 recall_val:0.5600 F1_val:0.589474 AUC_val:0.6632\n",
      "Epoch:0049\n",
      "acc_train:0.6133 pre_train:0.6657 recall_train:0.5054 F1_train:0.5746 AUC_train:0.6325\n",
      "acc_val:0.6100 pre_val:0.6279 recall_val:0.5400 F1_val:0.580645 AUC_val:0.6616\n",
      "Epoch:0050\n",
      "acc_train:0.6022 pre_train:0.6789 recall_train:0.4366 F1_train:0.5314 AUC_train:0.6286\n",
      "acc_val:0.6300 pre_val:0.6667 recall_val:0.5200 F1_val:0.584270 AUC_val:0.6728\n",
      "Epoch:0051\n",
      "acc_train:0.6400 pre_train:0.7296 recall_train:0.4817 F1_train:0.5803 AUC_train:0.6663\n",
      "acc_val:0.6400 pre_val:0.6944 recall_val:0.5000 F1_val:0.581395 AUC_val:0.6704\n",
      "Epoch:0052\n",
      "acc_train:0.6167 pre_train:0.6429 recall_train:0.5806 F1_train:0.6102 AUC_train:0.6628\n",
      "acc_val:0.6400 pre_val:0.6944 recall_val:0.5000 F1_val:0.581395 AUC_val:0.6684\n",
      "Epoch:0053\n",
      "acc_train:0.6222 pre_train:0.7289 recall_train:0.4280 F1_train:0.5393 AUC_train:0.6634\n",
      "acc_val:0.6400 pre_val:0.6944 recall_val:0.5000 F1_val:0.581395 AUC_val:0.6780\n",
      "Epoch:0054\n",
      "acc_train:0.6333 pre_train:0.7336 recall_train:0.4559 F1_train:0.5623 AUC_train:0.6682\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.6820\n",
      "Epoch:0055\n",
      "acc_train:0.6189 pre_train:0.6572 recall_train:0.5484 F1_train:0.5979 AUC_train:0.6591\n",
      "acc_val:0.6400 pre_val:0.6842 recall_val:0.5200 F1_val:0.590909 AUC_val:0.6852\n",
      "Epoch:0056\n",
      "acc_train:0.6222 pre_train:0.6551 recall_train:0.5677 F1_train:0.6083 AUC_train:0.6536\n",
      "acc_val:0.6400 pre_val:0.6842 recall_val:0.5200 F1_val:0.590909 AUC_val:0.6868\n",
      "Epoch:0057\n",
      "acc_train:0.6022 pre_train:0.5993 recall_train:0.6946 F1_train:0.6434 AUC_train:0.6750\n",
      "acc_val:0.6300 pre_val:0.6667 recall_val:0.5200 F1_val:0.584270 AUC_val:0.6864\n",
      "Epoch:0058\n",
      "acc_train:0.6333 pre_train:0.6762 recall_train:0.5570 F1_train:0.6108 AUC_train:0.6785\n",
      "acc_val:0.6400 pre_val:0.6591 recall_val:0.5800 F1_val:0.617021 AUC_val:0.6856\n",
      "Epoch:0059\n",
      "acc_train:0.6344 pre_train:0.7024 recall_train:0.5075 F1_train:0.5893 AUC_train:0.6698\n",
      "acc_val:0.6200 pre_val:0.6154 recall_val:0.6400 F1_val:0.627451 AUC_val:0.6940\n",
      "Epoch:0060\n",
      "acc_train:0.5822 pre_train:0.5771 recall_train:0.7161 F1_train:0.6392 AUC_train:0.6478\n",
      "acc_val:0.6300 pre_val:0.6140 recall_val:0.7000 F1_val:0.654206 AUC_val:0.6948\n",
      "Epoch:0061\n",
      "acc_train:0.6267 pre_train:0.6684 recall_train:0.5505 F1_train:0.6038 AUC_train:0.6692\n",
      "acc_val:0.6300 pre_val:0.6140 recall_val:0.7000 F1_val:0.654206 AUC_val:0.6980\n",
      "Epoch:0062\n",
      "acc_train:0.5889 pre_train:0.5841 recall_train:0.7097 F1_train:0.6408 AUC_train:0.6435\n",
      "acc_val:0.6200 pre_val:0.6000 recall_val:0.7200 F1_val:0.654545 AUC_val:0.7012\n",
      "Epoch:0063\n",
      "acc_train:0.6033 pre_train:0.6076 recall_train:0.6559 F1_train:0.6308 AUC_train:0.6648\n",
      "acc_val:0.6300 pre_val:0.6140 recall_val:0.7000 F1_val:0.654206 AUC_val:0.7024\n",
      "Epoch:0064\n",
      "acc_train:0.6022 pre_train:0.5978 recall_train:0.7032 F1_train:0.6462 AUC_train:0.6716\n",
      "acc_val:0.6300 pre_val:0.6140 recall_val:0.7000 F1_val:0.654206 AUC_val:0.7068\n",
      "Epoch:0065\n",
      "acc_train:0.6233 pre_train:0.6207 recall_train:0.6968 F1_train:0.6565 AUC_train:0.6781\n",
      "acc_val:0.6300 pre_val:0.6140 recall_val:0.7000 F1_val:0.654206 AUC_val:0.7096\n",
      "Epoch:0066\n",
      "acc_train:0.5856 pre_train:0.5824 recall_train:0.6989 F1_train:0.6354 AUC_train:0.6405\n",
      "acc_val:0.6300 pre_val:0.6140 recall_val:0.7000 F1_val:0.654206 AUC_val:0.7152\n",
      "Epoch:0067\n",
      "acc_train:0.6067 pre_train:0.5982 recall_train:0.7269 F1_train:0.6563 AUC_train:0.6958\n",
      "acc_val:0.6400 pre_val:0.6250 recall_val:0.7000 F1_val:0.660377 AUC_val:0.7164\n",
      "Epoch:0068\n",
      "acc_train:0.6100 pre_train:0.6439 recall_train:0.5484 F1_train:0.5923 AUC_train:0.6535\n",
      "acc_val:0.6500 pre_val:0.6364 recall_val:0.7000 F1_val:0.666667 AUC_val:0.7196\n",
      "Epoch:0069\n",
      "acc_train:0.6278 pre_train:0.6265 recall_train:0.6925 F1_train:0.6578 AUC_train:0.6912\n",
      "acc_val:0.6300 pre_val:0.6226 recall_val:0.6600 F1_val:0.640777 AUC_val:0.7228\n",
      "Epoch:0070\n",
      "acc_train:0.6211 pre_train:0.6297 recall_train:0.6473 F1_train:0.6384 AUC_train:0.6812\n",
      "acc_val:0.6500 pre_val:0.6744 recall_val:0.5800 F1_val:0.623656 AUC_val:0.7256\n",
      "Epoch:0071\n",
      "acc_train:0.6256 pre_train:0.6190 recall_train:0.7161 F1_train:0.6640 AUC_train:0.6985\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.7320\n",
      "Epoch:0072\n",
      "acc_train:0.6333 pre_train:0.6216 recall_train:0.7419 F1_train:0.6765 AUC_train:0.7126\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.7392\n",
      "Epoch:0073\n",
      "acc_train:0.6322 pre_train:0.6356 recall_train:0.6753 F1_train:0.6548 AUC_train:0.7123\n",
      "acc_val:0.6600 pre_val:0.6905 recall_val:0.5800 F1_val:0.630435 AUC_val:0.7460\n",
      "Epoch:0074\n",
      "acc_train:0.6400 pre_train:0.6407 recall_train:0.6903 F1_train:0.6646 AUC_train:0.7160\n",
      "acc_val:0.6800 pre_val:0.7143 recall_val:0.6000 F1_val:0.652174 AUC_val:0.7576\n",
      "Epoch:0075\n",
      "acc_train:0.6578 pre_train:0.6938 recall_train:0.6043 F1_train:0.6460 AUC_train:0.7285\n",
      "acc_val:0.6900 pre_val:0.7209 recall_val:0.6200 F1_val:0.666667 AUC_val:0.7664\n",
      "Epoch:0076\n",
      "acc_train:0.6733 pre_train:0.6748 recall_train:0.7097 F1_train:0.6918 AUC_train:0.7415\n",
      "acc_val:0.6700 pre_val:0.6977 recall_val:0.6000 F1_val:0.645161 AUC_val:0.7772\n",
      "Epoch:0077\n",
      "acc_train:0.6778 pre_train:0.6660 recall_train:0.7548 F1_train:0.7077 AUC_train:0.7456\n",
      "acc_val:0.6900 pre_val:0.7209 recall_val:0.6200 F1_val:0.666667 AUC_val:0.7884\n",
      "Epoch:0078\n",
      "acc_train:0.6800 pre_train:0.6660 recall_train:0.7634 F1_train:0.7114 AUC_train:0.7433\n",
      "acc_val:0.7000 pre_val:0.7381 recall_val:0.6200 F1_val:0.673913 AUC_val:0.7972\n",
      "Epoch:0079\n",
      "acc_train:0.6978 pre_train:0.6838 recall_train:0.7720 F1_train:0.7253 AUC_train:0.7652\n",
      "acc_val:0.6900 pre_val:0.7317 recall_val:0.6000 F1_val:0.659341 AUC_val:0.8100\n",
      "Epoch:0080\n",
      "acc_train:0.7044 pre_train:0.6940 recall_train:0.7656 F1_train:0.7280 AUC_train:0.7822\n",
      "acc_val:0.6900 pre_val:0.7317 recall_val:0.6000 F1_val:0.659341 AUC_val:0.8196\n",
      "Epoch:0081\n",
      "acc_train:0.7133 pre_train:0.6964 recall_train:0.7892 F1_train:0.7399 AUC_train:0.7778\n",
      "acc_val:0.7100 pre_val:0.7442 recall_val:0.6400 F1_val:0.688172 AUC_val:0.8260\n",
      "Epoch:0082\n",
      "acc_train:0.7467 pre_train:0.7257 recall_train:0.8194 F1_train:0.7697 AUC_train:0.8054\n",
      "acc_val:0.7300 pre_val:0.7556 recall_val:0.6800 F1_val:0.715789 AUC_val:0.8284\n",
      "Epoch:0083\n",
      "acc_train:0.7589 pre_train:0.7616 recall_train:0.7763 F1_train:0.7689 AUC_train:0.8186\n",
      "acc_val:0.7400 pre_val:0.7609 recall_val:0.7000 F1_val:0.729167 AUC_val:0.8300\n",
      "Epoch:0084\n",
      "acc_train:0.7289 pre_train:0.7269 recall_train:0.7613 F1_train:0.7437 AUC_train:0.8068\n",
      "acc_val:0.7400 pre_val:0.7609 recall_val:0.7000 F1_val:0.729167 AUC_val:0.8320\n",
      "Epoch:0085\n",
      "acc_train:0.7389 pre_train:0.7195 recall_train:0.8108 F1_train:0.7624 AUC_train:0.7995\n",
      "acc_val:0.7800 pre_val:0.7800 recall_val:0.7800 F1_val:0.780000 AUC_val:0.8356\n",
      "Epoch:0086\n",
      "acc_train:0.7556 pre_train:0.7307 recall_train:0.8344 F1_train:0.7791 AUC_train:0.8193\n",
      "acc_val:0.7700 pre_val:0.7547 recall_val:0.8000 F1_val:0.776699 AUC_val:0.8364\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0087\n",
      "acc_train:0.7578 pre_train:0.7291 recall_train:0.8452 F1_train:0.7829 AUC_train:0.8126\n",
      "acc_val:0.7700 pre_val:0.7547 recall_val:0.8000 F1_val:0.776699 AUC_val:0.8400\n",
      "Epoch:0088\n",
      "acc_train:0.7622 pre_train:0.7346 recall_train:0.8452 F1_train:0.7860 AUC_train:0.8232\n",
      "acc_val:0.7900 pre_val:0.7636 recall_val:0.8400 F1_val:0.800000 AUC_val:0.8488\n",
      "Epoch:0089\n",
      "acc_train:0.7800 pre_train:0.7543 recall_train:0.8516 F1_train:0.8000 AUC_train:0.8371\n",
      "acc_val:0.8100 pre_val:0.7818 recall_val:0.8600 F1_val:0.819048 AUC_val:0.8572\n",
      "Epoch:0090\n",
      "acc_train:0.7967 pre_train:0.7465 recall_train:0.9183 F1_train:0.8235 AUC_train:0.8518\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.8620\n",
      "Epoch:0091\n",
      "acc_train:0.8200 pre_train:0.7663 recall_train:0.9376 F1_train:0.8433 AUC_train:0.8793\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.8680\n",
      "Epoch:0092\n",
      "acc_train:0.7878 pre_train:0.7387 recall_train:0.9118 F1_train:0.8162 AUC_train:0.8661\n",
      "acc_val:0.8100 pre_val:0.7719 recall_val:0.8800 F1_val:0.822430 AUC_val:0.8740\n",
      "Epoch:0093\n",
      "acc_train:0.8400 pre_train:0.7913 recall_train:0.9376 F1_train:0.8583 AUC_train:0.9008\n",
      "acc_val:0.8100 pre_val:0.7719 recall_val:0.8800 F1_val:0.822430 AUC_val:0.8748\n",
      "Epoch:0094\n",
      "acc_train:0.8300 pre_train:0.7806 recall_train:0.9333 F1_train:0.8501 AUC_train:0.8964\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.8788\n",
      "Epoch:0095\n",
      "acc_train:0.8489 pre_train:0.8018 recall_train:0.9398 F1_train:0.8653 AUC_train:0.9067\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.8852\n",
      "Epoch:0096\n",
      "acc_train:0.8478 pre_train:0.7950 recall_train:0.9505 F1_train:0.8658 AUC_train:0.9143\n",
      "acc_val:0.8200 pre_val:0.7581 recall_val:0.9400 F1_val:0.839286 AUC_val:0.8908\n",
      "Epoch:0097\n",
      "acc_train:0.8611 pre_train:0.8025 recall_train:0.9699 F1_train:0.8783 AUC_train:0.9226\n",
      "acc_val:0.8000 pre_val:0.7273 recall_val:0.9600 F1_val:0.827586 AUC_val:0.8984\n",
      "Epoch:0098\n",
      "acc_train:0.8633 pre_train:0.8098 recall_train:0.9613 F1_train:0.8791 AUC_train:0.9248\n",
      "acc_val:0.8000 pre_val:0.7273 recall_val:0.9600 F1_val:0.827586 AUC_val:0.9000\n",
      "Epoch:0099\n",
      "acc_train:0.8722 pre_train:0.8136 recall_train:0.9763 F1_train:0.8876 AUC_train:0.9394\n",
      "acc_val:0.7900 pre_val:0.7164 recall_val:0.9600 F1_val:0.820513 AUC_val:0.8996\n",
      "Epoch:0100\n",
      "acc_train:0.8678 pre_train:0.8168 recall_train:0.9591 F1_train:0.8823 AUC_train:0.9236\n",
      "acc_val:0.8000 pre_val:0.7273 recall_val:0.9600 F1_val:0.827586 AUC_val:0.8996\n",
      "Epoch:0101\n",
      "acc_train:0.8744 pre_train:0.8200 recall_train:0.9699 F1_train:0.8887 AUC_train:0.9442\n",
      "acc_val:0.8300 pre_val:0.7619 recall_val:0.9600 F1_val:0.849558 AUC_val:0.9044\n",
      "Epoch:0102\n",
      "acc_train:0.8767 pre_train:0.8230 recall_train:0.9699 F1_train:0.8904 AUC_train:0.9497\n",
      "acc_val:0.8200 pre_val:0.7581 recall_val:0.9400 F1_val:0.839286 AUC_val:0.8984\n",
      "Epoch:0103\n",
      "acc_train:0.8778 pre_train:0.8221 recall_train:0.9742 F1_train:0.8917 AUC_train:0.9549\n",
      "acc_val:0.8200 pre_val:0.7581 recall_val:0.9400 F1_val:0.839286 AUC_val:0.8992\n",
      "Epoch:0104\n",
      "acc_train:0.8911 pre_train:0.8392 recall_train:0.9763 F1_train:0.9026 AUC_train:0.9495\n",
      "acc_val:0.8200 pre_val:0.7581 recall_val:0.9400 F1_val:0.839286 AUC_val:0.8988\n",
      "Epoch:0105\n",
      "acc_train:0.8789 pre_train:0.8284 recall_train:0.9656 F1_train:0.8918 AUC_train:0.9391\n",
      "acc_val:0.8100 pre_val:0.7460 recall_val:0.9400 F1_val:0.831858 AUC_val:0.9052\n",
      "Epoch:0106\n",
      "acc_train:0.8889 pre_train:0.8361 recall_train:0.9763 F1_train:0.9008 AUC_train:0.9438\n",
      "acc_val:0.8300 pre_val:0.7705 recall_val:0.9400 F1_val:0.846847 AUC_val:0.9108\n",
      "Epoch:0107\n",
      "acc_train:0.8967 pre_train:0.8419 recall_train:0.9849 F1_train:0.9078 AUC_train:0.9682\n",
      "acc_val:0.8300 pre_val:0.7705 recall_val:0.9400 F1_val:0.846847 AUC_val:0.9108\n",
      "Epoch:0108\n",
      "acc_train:0.8811 pre_train:0.8278 recall_train:0.9720 F1_train:0.8942 AUC_train:0.9575\n",
      "acc_val:0.8300 pre_val:0.7705 recall_val:0.9400 F1_val:0.846847 AUC_val:0.9088\n",
      "Epoch:0109\n",
      "acc_train:0.9022 pre_train:0.8550 recall_train:0.9763 F1_train:0.9116 AUC_train:0.9582\n",
      "acc_val:0.8400 pre_val:0.7931 recall_val:0.9200 F1_val:0.851852 AUC_val:0.9068\n",
      "Epoch:0110\n",
      "acc_train:0.9122 pre_train:0.8683 recall_train:0.9785 F1_train:0.9201 AUC_train:0.9646\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9064\n",
      "Epoch:0111\n",
      "acc_train:0.9044 pre_train:0.8623 recall_train:0.9699 F1_train:0.9130 AUC_train:0.9715\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9040\n",
      "Epoch:0112\n",
      "acc_train:0.9033 pre_train:0.8580 recall_train:0.9742 F1_train:0.9124 AUC_train:0.9592\n",
      "acc_val:0.8500 pre_val:0.8302 recall_val:0.8800 F1_val:0.854369 AUC_val:0.9012\n",
      "Epoch:0113\n",
      "acc_train:0.9167 pre_train:0.8721 recall_train:0.9828 F1_train:0.9242 AUC_train:0.9669\n",
      "acc_val:0.8500 pre_val:0.8302 recall_val:0.8800 F1_val:0.854369 AUC_val:0.8992\n",
      "Epoch:0114\n",
      "acc_train:0.9178 pre_train:0.8796 recall_train:0.9742 F1_train:0.9245 AUC_train:0.9698\n",
      "acc_val:0.8400 pre_val:0.8148 recall_val:0.8800 F1_val:0.846154 AUC_val:0.9020\n",
      "Epoch:0115\n",
      "acc_train:0.9311 pre_train:0.8928 recall_train:0.9849 F1_train:0.9366 AUC_train:0.9730\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9064\n",
      "Epoch:0116\n",
      "acc_train:0.9289 pre_train:0.8878 recall_train:0.9871 F1_train:0.9348 AUC_train:0.9760\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9064\n",
      "Epoch:0117\n",
      "acc_train:0.9322 pre_train:0.9024 recall_train:0.9742 F1_train:0.9369 AUC_train:0.9677\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9094\n",
      "Epoch:0118\n",
      "acc_train:0.9389 pre_train:0.9067 recall_train:0.9828 F1_train:0.9432 AUC_train:0.9772\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.9072\n",
      "Epoch:0119\n",
      "acc_train:0.9311 pre_train:0.8990 recall_train:0.9763 F1_train:0.9361 AUC_train:0.9725\n",
      "acc_val:0.8100 pre_val:0.7627 recall_val:0.9000 F1_val:0.825688 AUC_val:0.9076\n",
      "Epoch:0120\n",
      "acc_train:0.9322 pre_train:0.9008 recall_train:0.9763 F1_train:0.9370 AUC_train:0.9743\n",
      "acc_val:0.8000 pre_val:0.7500 recall_val:0.9000 F1_val:0.818182 AUC_val:0.9096\n",
      "Epoch:0121\n",
      "acc_train:0.9422 pre_train:0.9073 recall_train:0.9892 F1_train:0.9465 AUC_train:0.9763\n",
      "acc_val:0.7900 pre_val:0.7377 recall_val:0.9000 F1_val:0.810811 AUC_val:0.9048\n",
      "Epoch:0122\n",
      "acc_train:0.9378 pre_train:0.9034 recall_train:0.9849 F1_train:0.9424 AUC_train:0.9765\n",
      "acc_val:0.7500 pre_val:0.6923 recall_val:0.9000 F1_val:0.782609 AUC_val:0.9000\n",
      "Epoch:0123\n",
      "acc_train:0.9400 pre_train:0.9006 recall_train:0.9935 F1_train:0.9448 AUC_train:0.9864\n",
      "acc_val:0.7400 pre_val:0.6765 recall_val:0.9200 F1_val:0.779661 AUC_val:0.9016\n",
      "Epoch:0124\n",
      "acc_train:0.9444 pre_train:0.9142 recall_train:0.9849 F1_train:0.9482 AUC_train:0.9770\n",
      "acc_val:0.7300 pre_val:0.6667 recall_val:0.9200 F1_val:0.773109 AUC_val:0.9068\n",
      "Epoch:0125\n",
      "acc_train:0.9433 pre_train:0.9124 recall_train:0.9849 F1_train:0.9473 AUC_train:0.9819\n",
      "acc_val:0.7400 pre_val:0.6714 recall_val:0.9400 F1_val:0.783333 AUC_val:0.9102\n",
      "Epoch:0126\n",
      "acc_train:0.9311 pre_train:0.9006 recall_train:0.9742 F1_train:0.9360 AUC_train:0.9829\n",
      "acc_val:0.7400 pre_val:0.6714 recall_val:0.9400 F1_val:0.783333 AUC_val:0.9058\n",
      "Epoch:0127\n",
      "acc_train:0.9522 pre_train:0.9237 recall_train:0.9892 F1_train:0.9553 AUC_train:0.9860\n",
      "acc_val:0.7600 pre_val:0.6912 recall_val:0.9400 F1_val:0.796610 AUC_val:0.8976\n",
      "Epoch:0128\n",
      "acc_train:0.9578 pre_train:0.9279 recall_train:0.9957 F1_train:0.9606 AUC_train:0.9873\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.8966\n",
      "Epoch:0129\n",
      "acc_train:0.9500 pre_train:0.9200 recall_train:0.9892 F1_train:0.9534 AUC_train:0.9905\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.9032\n",
      "Epoch:0130\n",
      "acc_train:0.9533 pre_train:0.9343 recall_train:0.9785 F1_train:0.9559 AUC_train:0.9848\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.9148\n",
      "Epoch:0131\n",
      "acc_train:0.9567 pre_train:0.9365 recall_train:0.9828 F1_train:0.9591 AUC_train:0.9866\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.9180\n",
      "Epoch:0132\n",
      "acc_train:0.9589 pre_train:0.9315 recall_train:0.9935 F1_train:0.9615 AUC_train:0.9874\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.9116\n",
      "Epoch:0133\n",
      "acc_train:0.9500 pre_train:0.9268 recall_train:0.9806 F1_train:0.9530 AUC_train:0.9918\n",
      "acc_val:0.7500 pre_val:0.6923 recall_val:0.9000 F1_val:0.782609 AUC_val:0.9024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0134\n",
      "acc_train:0.9544 pre_train:0.9327 recall_train:0.9828 F1_train:0.9571 AUC_train:0.9866\n",
      "acc_val:0.7500 pre_val:0.6923 recall_val:0.9000 F1_val:0.782609 AUC_val:0.9012\n",
      "Early Stopping!!! epoch：133\n",
      " Starting the 1-10 Fold:：\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.5322 pre_train:0.5487 recall_train:0.5333 F1_train:0.5409 AUC_train:0.5442\n",
      "acc_val:0.5300 pre_val:0.5283 recall_val:0.5600 F1_val:0.543689 AUC_val:0.5704\n",
      "Epoch:0002\n",
      "acc_train:0.5467 pre_train:0.5624 recall_train:0.5527 F1_train:0.5575 AUC_train:0.5359\n",
      "acc_val:0.5400 pre_val:0.5357 recall_val:0.6000 F1_val:0.566038 AUC_val:0.6072\n",
      "Epoch:0003\n",
      "acc_train:0.5378 pre_train:0.5558 recall_train:0.5247 F1_train:0.5398 AUC_train:0.5691\n",
      "acc_val:0.5700 pre_val:0.5593 recall_val:0.6600 F1_val:0.605505 AUC_val:0.5940\n",
      "Epoch:0004\n",
      "acc_train:0.5656 pre_train:0.5911 recall_train:0.5161 F1_train:0.5511 AUC_train:0.5911\n",
      "acc_val:0.5800 pre_val:0.5625 recall_val:0.7200 F1_val:0.631579 AUC_val:0.6156\n",
      "Epoch:0005\n",
      "acc_train:0.5744 pre_train:0.5876 recall_train:0.5914 F1_train:0.5895 AUC_train:0.5912\n",
      "acc_val:0.5800 pre_val:0.5606 recall_val:0.7400 F1_val:0.637931 AUC_val:0.6164\n",
      "Epoch:0006\n",
      "acc_train:0.5789 pre_train:0.5977 recall_train:0.5656 F1_train:0.5812 AUC_train:0.6140\n",
      "acc_val:0.5800 pre_val:0.5588 recall_val:0.7600 F1_val:0.644068 AUC_val:0.6328\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0007\n",
      "acc_train:0.5644 pre_train:0.5785 recall_train:0.5785 F1_train:0.5785 AUC_train:0.5832\n",
      "acc_val:0.5700 pre_val:0.5538 recall_val:0.7200 F1_val:0.626087 AUC_val:0.6448\n",
      "Epoch:0008\n",
      "acc_train:0.5989 pre_train:0.6070 recall_train:0.6344 F1_train:0.6204 AUC_train:0.6367\n",
      "acc_val:0.6200 pre_val:0.6111 recall_val:0.6600 F1_val:0.634615 AUC_val:0.6716\n",
      "Epoch:0009\n",
      "acc_train:0.5978 pre_train:0.6066 recall_train:0.6301 F1_train:0.6181 AUC_train:0.6420\n",
      "acc_val:0.6400 pre_val:0.6346 recall_val:0.6600 F1_val:0.647059 AUC_val:0.6872\n",
      "Epoch:0010\n",
      "acc_train:0.5911 pre_train:0.6071 recall_train:0.5914 F1_train:0.5991 AUC_train:0.6229\n",
      "acc_val:0.6500 pre_val:0.6415 recall_val:0.6800 F1_val:0.660194 AUC_val:0.6960\n",
      "Epoch:0011\n",
      "acc_train:0.6189 pre_train:0.6287 recall_train:0.6409 F1_train:0.6347 AUC_train:0.6546\n",
      "acc_val:0.6700 pre_val:0.6667 recall_val:0.6800 F1_val:0.673267 AUC_val:0.7032\n",
      "Epoch:0012\n",
      "acc_train:0.6333 pre_train:0.6452 recall_train:0.6452 F1_train:0.6452 AUC_train:0.6774\n",
      "acc_val:0.6600 pre_val:0.6818 recall_val:0.6000 F1_val:0.638298 AUC_val:0.7120\n",
      "Epoch:0013\n",
      "acc_train:0.6178 pre_train:0.6193 recall_train:0.6753 F1_train:0.6461 AUC_train:0.6732\n",
      "acc_val:0.6600 pre_val:0.6818 recall_val:0.6000 F1_val:0.638298 AUC_val:0.7160\n",
      "Epoch:0014\n",
      "acc_train:0.6144 pre_train:0.6157 recall_train:0.6753 F1_train:0.6441 AUC_train:0.6639\n",
      "acc_val:0.6900 pre_val:0.6863 recall_val:0.7000 F1_val:0.693069 AUC_val:0.7220\n",
      "Epoch:0015\n",
      "acc_train:0.6167 pre_train:0.6095 recall_train:0.7183 F1_train:0.6594 AUC_train:0.6733\n",
      "acc_val:0.6800 pre_val:0.6800 recall_val:0.6800 F1_val:0.680000 AUC_val:0.7280\n",
      "Epoch:0016\n",
      "acc_train:0.6389 pre_train:0.6394 recall_train:0.6903 F1_train:0.6639 AUC_train:0.6930\n",
      "acc_val:0.6800 pre_val:0.6800 recall_val:0.6800 F1_val:0.680000 AUC_val:0.7352\n",
      "Epoch:0017\n",
      "acc_train:0.6344 pre_train:0.6323 recall_train:0.6989 F1_train:0.6639 AUC_train:0.6864\n",
      "acc_val:0.6900 pre_val:0.6863 recall_val:0.7000 F1_val:0.693069 AUC_val:0.7436\n",
      "Epoch:0018\n",
      "acc_train:0.6411 pre_train:0.6473 recall_train:0.6710 F1_train:0.6589 AUC_train:0.7098\n",
      "acc_val:0.6800 pre_val:0.6800 recall_val:0.6800 F1_val:0.680000 AUC_val:0.7500\n",
      "Epoch:0019\n",
      "acc_train:0.6411 pre_train:0.6530 recall_train:0.6516 F1_train:0.6523 AUC_train:0.7020\n",
      "acc_val:0.7000 pre_val:0.7000 recall_val:0.7000 F1_val:0.700000 AUC_val:0.7516\n",
      "Epoch:0020\n",
      "acc_train:0.6556 pre_train:0.6443 recall_train:0.7441 F1_train:0.6906 AUC_train:0.7114\n",
      "acc_val:0.7100 pre_val:0.6981 recall_val:0.7400 F1_val:0.718447 AUC_val:0.7592\n",
      "Epoch:0021\n",
      "acc_train:0.6800 pre_train:0.6648 recall_train:0.7677 F1_train:0.7126 AUC_train:0.7416\n",
      "acc_val:0.7000 pre_val:0.6852 recall_val:0.7400 F1_val:0.711538 AUC_val:0.7668\n",
      "Epoch:0022\n",
      "acc_train:0.6322 pre_train:0.6523 recall_train:0.6172 F1_train:0.6343 AUC_train:0.7177\n",
      "acc_val:0.7000 pre_val:0.6852 recall_val:0.7400 F1_val:0.711538 AUC_val:0.7724\n",
      "Epoch:0023\n",
      "acc_train:0.7000 pre_train:0.6796 recall_train:0.7935 F1_train:0.7321 AUC_train:0.7497\n",
      "acc_val:0.7100 pre_val:0.6780 recall_val:0.8000 F1_val:0.733945 AUC_val:0.7868\n",
      "Epoch:0024\n",
      "acc_train:0.6578 pre_train:0.6901 recall_train:0.6129 F1_train:0.6492 AUC_train:0.7598\n",
      "acc_val:0.7100 pre_val:0.6780 recall_val:0.8000 F1_val:0.733945 AUC_val:0.7964\n",
      "Epoch:0025\n",
      "acc_train:0.7056 pre_train:0.6992 recall_train:0.7548 F1_train:0.7260 AUC_train:0.7783\n",
      "acc_val:0.7200 pre_val:0.6897 recall_val:0.8000 F1_val:0.740741 AUC_val:0.8160\n",
      "Epoch:0026\n",
      "acc_train:0.7556 pre_train:0.7281 recall_train:0.8409 F1_train:0.7804 AUC_train:0.8170\n",
      "acc_val:0.7500 pre_val:0.7119 recall_val:0.8400 F1_val:0.770642 AUC_val:0.8292\n",
      "Epoch:0027\n",
      "acc_train:0.7589 pre_train:0.7263 recall_train:0.8559 F1_train:0.7858 AUC_train:0.8115\n",
      "acc_val:0.7500 pre_val:0.6984 recall_val:0.8800 F1_val:0.778761 AUC_val:0.8316\n",
      "Epoch:0028\n",
      "acc_train:0.7889 pre_train:0.7691 recall_train:0.8452 F1_train:0.8053 AUC_train:0.8347\n",
      "acc_val:0.7600 pre_val:0.7097 recall_val:0.8800 F1_val:0.785714 AUC_val:0.8316\n",
      "Epoch:0029\n",
      "acc_train:0.7989 pre_train:0.7572 recall_train:0.8989 F1_train:0.8220 AUC_train:0.8696\n",
      "acc_val:0.7500 pre_val:0.7049 recall_val:0.8600 F1_val:0.774775 AUC_val:0.8324\n",
      "Epoch:0030\n",
      "acc_train:0.7822 pre_train:0.7685 recall_train:0.8280 F1_train:0.7971 AUC_train:0.8512\n",
      "acc_val:0.7200 pre_val:0.6897 recall_val:0.8000 F1_val:0.740741 AUC_val:0.8320\n",
      "Epoch:0031\n",
      "acc_train:0.7600 pre_train:0.7495 recall_train:0.8043 F1_train:0.7759 AUC_train:0.8380\n",
      "acc_val:0.7400 pre_val:0.7143 recall_val:0.8000 F1_val:0.754717 AUC_val:0.8284\n",
      "Epoch:0032\n",
      "acc_train:0.8122 pre_train:0.7824 recall_train:0.8817 F1_train:0.8291 AUC_train:0.8583\n",
      "acc_val:0.7500 pre_val:0.7358 recall_val:0.7800 F1_val:0.757282 AUC_val:0.8224\n",
      "Epoch:0033\n",
      "acc_train:0.7700 pre_train:0.7560 recall_train:0.8194 F1_train:0.7864 AUC_train:0.8328\n",
      "acc_val:0.7400 pre_val:0.7143 recall_val:0.8000 F1_val:0.754717 AUC_val:0.8264\n",
      "Epoch:0034\n",
      "acc_train:0.8033 pre_train:0.7738 recall_train:0.8753 F1_train:0.8214 AUC_train:0.8596\n",
      "acc_val:0.7400 pre_val:0.7143 recall_val:0.8000 F1_val:0.754717 AUC_val:0.8304\n",
      "Epoch:0035\n",
      "acc_train:0.8122 pre_train:0.7751 recall_train:0.8968 F1_train:0.8315 AUC_train:0.8727\n",
      "acc_val:0.7400 pre_val:0.7143 recall_val:0.8000 F1_val:0.754717 AUC_val:0.8268\n",
      "Epoch:0036\n",
      "acc_train:0.8222 pre_train:0.7840 recall_train:0.9054 F1_train:0.8403 AUC_train:0.8808\n",
      "acc_val:0.7300 pre_val:0.7018 recall_val:0.8000 F1_val:0.747664 AUC_val:0.8272\n",
      "Epoch:0037\n",
      "acc_train:0.7878 pre_train:0.7566 recall_train:0.8688 F1_train:0.8088 AUC_train:0.8565\n",
      "acc_val:0.7400 pre_val:0.7000 recall_val:0.8400 F1_val:0.763636 AUC_val:0.8310\n",
      "Epoch:0038\n",
      "acc_train:0.8078 pre_train:0.7645 recall_train:0.9075 F1_train:0.8299 AUC_train:0.8694\n",
      "acc_val:0.7500 pre_val:0.7049 recall_val:0.8600 F1_val:0.774775 AUC_val:0.8408\n",
      "Epoch:0039\n",
      "acc_train:0.8000 pre_train:0.7478 recall_train:0.9247 F1_train:0.8269 AUC_train:0.8765\n",
      "acc_val:0.7900 pre_val:0.7231 recall_val:0.9400 F1_val:0.817391 AUC_val:0.8548\n",
      "Epoch:0040\n",
      "acc_train:0.8189 pre_train:0.7745 recall_train:0.9161 F1_train:0.8394 AUC_train:0.8878\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.8752\n",
      "Epoch:0041\n",
      "acc_train:0.8211 pre_train:0.7676 recall_train:0.9376 F1_train:0.8441 AUC_train:0.8846\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.8804\n",
      "Epoch:0042\n",
      "acc_train:0.8367 pre_train:0.7829 recall_train:0.9462 F1_train:0.8569 AUC_train:0.9116\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.8804\n",
      "Epoch:0043\n",
      "acc_train:0.8256 pre_train:0.7674 recall_train:0.9505 F1_train:0.8492 AUC_train:0.9186\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.8840\n",
      "Epoch:0044\n",
      "acc_train:0.8567 pre_train:0.8000 recall_train:0.9634 F1_train:0.8741 AUC_train:0.9371\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.8848\n",
      "Epoch:0045\n",
      "acc_train:0.8378 pre_train:0.7843 recall_train:0.9462 F1_train:0.8577 AUC_train:0.9302\n",
      "acc_val:0.8000 pre_val:0.7500 recall_val:0.9000 F1_val:0.818182 AUC_val:0.8796\n",
      "Epoch:0046\n",
      "acc_train:0.8667 pre_train:0.8224 recall_train:0.9462 F1_train:0.8800 AUC_train:0.9352\n",
      "acc_val:0.8200 pre_val:0.7667 recall_val:0.9200 F1_val:0.836364 AUC_val:0.8788\n",
      "Epoch:0047\n",
      "acc_train:0.8500 pre_train:0.8000 recall_train:0.9462 F1_train:0.8670 AUC_train:0.9385\n",
      "acc_val:0.8200 pre_val:0.7667 recall_val:0.9200 F1_val:0.836364 AUC_val:0.8812\n",
      "Epoch:0048\n",
      "acc_train:0.8611 pre_train:0.8172 recall_train:0.9419 F1_train:0.8751 AUC_train:0.9415\n",
      "acc_val:0.8100 pre_val:0.7541 recall_val:0.9200 F1_val:0.828829 AUC_val:0.8844\n",
      "Epoch:0049\n",
      "acc_train:0.8744 pre_train:0.8259 recall_train:0.9591 F1_train:0.8876 AUC_train:0.9396\n",
      "acc_val:0.8200 pre_val:0.7581 recall_val:0.9400 F1_val:0.839286 AUC_val:0.8854\n",
      "Epoch:0050\n",
      "acc_train:0.8900 pre_train:0.8466 recall_train:0.9613 F1_train:0.9003 AUC_train:0.9521\n",
      "acc_val:0.8300 pre_val:0.7619 recall_val:0.9600 F1_val:0.849558 AUC_val:0.8886\n",
      "Epoch:0051\n",
      "acc_train:0.8922 pre_train:0.8511 recall_train:0.9591 F1_train:0.9019 AUC_train:0.9561\n",
      "acc_val:0.8100 pre_val:0.7385 recall_val:0.9600 F1_val:0.834783 AUC_val:0.8866\n",
      "Epoch:0052\n",
      "acc_train:0.8978 pre_train:0.8499 recall_train:0.9742 F1_train:0.9078 AUC_train:0.9602\n",
      "acc_val:0.8100 pre_val:0.7385 recall_val:0.9600 F1_val:0.834783 AUC_val:0.8914\n",
      "Epoch:0053\n",
      "acc_train:0.8833 pre_train:0.8409 recall_train:0.9548 F1_train:0.8943 AUC_train:0.9537\n",
      "acc_val:0.8300 pre_val:0.7619 recall_val:0.9600 F1_val:0.849558 AUC_val:0.8950\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0054\n",
      "acc_train:0.8889 pre_train:0.8437 recall_train:0.9634 F1_train:0.8996 AUC_train:0.9520\n",
      "acc_val:0.8200 pre_val:0.7500 recall_val:0.9600 F1_val:0.842105 AUC_val:0.8986\n",
      "Epoch:0055\n",
      "acc_train:0.9033 pre_train:0.8677 recall_train:0.9591 F1_train:0.9111 AUC_train:0.9586\n",
      "acc_val:0.8100 pre_val:0.7460 recall_val:0.9400 F1_val:0.831858 AUC_val:0.9102\n",
      "Epoch:0056\n",
      "acc_train:0.9100 pre_train:0.8735 recall_train:0.9656 F1_train:0.9173 AUC_train:0.9617\n",
      "acc_val:0.8200 pre_val:0.7581 recall_val:0.9400 F1_val:0.839286 AUC_val:0.9224\n",
      "Epoch:0057\n",
      "acc_train:0.9122 pre_train:0.8891 recall_train:0.9484 F1_train:0.9178 AUC_train:0.9561\n",
      "acc_val:0.8000 pre_val:0.7586 recall_val:0.8800 F1_val:0.814815 AUC_val:0.9270\n",
      "Epoch:0058\n",
      "acc_train:0.9133 pre_train:0.9023 recall_train:0.9333 F1_train:0.9175 AUC_train:0.9691\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.9012\n",
      "Epoch:0059\n",
      "acc_train:0.8889 pre_train:0.8657 recall_train:0.9290 F1_train:0.8963 AUC_train:0.9497\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9140\n",
      "Epoch:0060\n",
      "acc_train:0.8878 pre_train:0.8487 recall_train:0.9527 F1_train:0.8977 AUC_train:0.9558\n",
      "acc_val:0.8500 pre_val:0.7966 recall_val:0.9400 F1_val:0.862385 AUC_val:0.9292\n",
      "Epoch:0061\n",
      "acc_train:0.9156 pre_train:0.8836 recall_train:0.9634 F1_train:0.9218 AUC_train:0.9680\n",
      "acc_val:0.8500 pre_val:0.7869 recall_val:0.9600 F1_val:0.864865 AUC_val:0.9304\n",
      "Epoch:0062\n",
      "acc_train:0.9156 pre_train:0.8961 recall_train:0.9462 F1_train:0.9205 AUC_train:0.9667\n",
      "acc_val:0.8200 pre_val:0.7500 recall_val:0.9600 F1_val:0.842105 AUC_val:0.9332\n",
      "Epoch:0063\n",
      "acc_train:0.9144 pre_train:0.8789 recall_train:0.9677 F1_train:0.9212 AUC_train:0.9714\n",
      "acc_val:0.8400 pre_val:0.7656 recall_val:0.9800 F1_val:0.859649 AUC_val:0.9326\n",
      "Epoch:0064\n",
      "acc_train:0.9267 pre_train:0.8982 recall_train:0.9677 F1_train:0.9317 AUC_train:0.9723\n",
      "acc_val:0.8400 pre_val:0.7656 recall_val:0.9800 F1_val:0.859649 AUC_val:0.9302\n",
      "Epoch:0065\n",
      "acc_train:0.9244 pre_train:0.9043 recall_train:0.9548 F1_train:0.9289 AUC_train:0.9717\n",
      "acc_val:0.8300 pre_val:0.7538 recall_val:0.9800 F1_val:0.852174 AUC_val:0.9314\n",
      "Epoch:0066\n",
      "acc_train:0.9300 pre_train:0.9069 recall_train:0.9634 F1_train:0.9343 AUC_train:0.9727\n",
      "acc_val:0.8300 pre_val:0.7538 recall_val:0.9800 F1_val:0.852174 AUC_val:0.9336\n",
      "Epoch:0067\n",
      "acc_train:0.9200 pre_train:0.8922 recall_train:0.9613 F1_train:0.9255 AUC_train:0.9770\n",
      "acc_val:0.8200 pre_val:0.7424 recall_val:0.9800 F1_val:0.844828 AUC_val:0.9434\n",
      "Epoch:0068\n",
      "acc_train:0.9289 pre_train:0.9034 recall_train:0.9656 F1_train:0.9335 AUC_train:0.9662\n",
      "acc_val:0.8300 pre_val:0.7538 recall_val:0.9800 F1_val:0.852174 AUC_val:0.9278\n",
      "Epoch:0069\n",
      "acc_train:0.9278 pre_train:0.8968 recall_train:0.9720 F1_train:0.9329 AUC_train:0.9761\n",
      "acc_val:0.8300 pre_val:0.7538 recall_val:0.9800 F1_val:0.852174 AUC_val:0.9216\n",
      "Epoch:0070\n",
      "acc_train:0.9289 pre_train:0.9034 recall_train:0.9656 F1_train:0.9335 AUC_train:0.9760\n",
      "acc_val:0.8200 pre_val:0.7424 recall_val:0.9800 F1_val:0.844828 AUC_val:0.9208\n",
      "Epoch:0071\n",
      "acc_train:0.9400 pre_train:0.9255 recall_train:0.9613 F1_train:0.9430 AUC_train:0.9780\n",
      "acc_val:0.8400 pre_val:0.7656 recall_val:0.9800 F1_val:0.859649 AUC_val:0.9148\n",
      "Epoch:0072\n",
      "acc_train:0.9444 pre_train:0.9261 recall_train:0.9699 F1_train:0.9475 AUC_train:0.9796\n",
      "acc_val:0.8500 pre_val:0.7778 recall_val:0.9800 F1_val:0.867257 AUC_val:0.9144\n",
      "Epoch:0073\n",
      "acc_train:0.9456 pre_train:0.9211 recall_train:0.9785 F1_train:0.9489 AUC_train:0.9802\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9168\n",
      "Epoch:0074\n",
      "acc_train:0.9400 pre_train:0.9152 recall_train:0.9742 F1_train:0.9437 AUC_train:0.9744\n",
      "acc_val:0.8400 pre_val:0.7742 recall_val:0.9600 F1_val:0.857143 AUC_val:0.9164\n",
      "Epoch:0075\n",
      "acc_train:0.9467 pre_train:0.9281 recall_train:0.9720 F1_train:0.9496 AUC_train:0.9841\n",
      "acc_val:0.8200 pre_val:0.7500 recall_val:0.9600 F1_val:0.842105 AUC_val:0.9150\n",
      "Epoch:0076\n",
      "acc_train:0.9422 pre_train:0.9189 recall_train:0.9742 F1_train:0.9457 AUC_train:0.9792\n",
      "acc_val:0.8200 pre_val:0.7500 recall_val:0.9600 F1_val:0.842105 AUC_val:0.9272\n",
      "Epoch:0077\n",
      "acc_train:0.9456 pre_train:0.9194 recall_train:0.9806 F1_train:0.9490 AUC_train:0.9805\n",
      "acc_val:0.8200 pre_val:0.7500 recall_val:0.9600 F1_val:0.842105 AUC_val:0.9320\n",
      "Epoch:0078\n",
      "acc_train:0.9511 pre_train:0.9340 recall_train:0.9742 F1_train:0.9537 AUC_train:0.9844\n",
      "acc_val:0.8400 pre_val:0.7656 recall_val:0.9800 F1_val:0.859649 AUC_val:0.9328\n",
      "Epoch:0079\n",
      "acc_train:0.9533 pre_train:0.9397 recall_train:0.9720 F1_train:0.9556 AUC_train:0.9832\n",
      "acc_val:0.8400 pre_val:0.7656 recall_val:0.9800 F1_val:0.859649 AUC_val:0.9358\n",
      "Epoch:0080\n",
      "acc_train:0.9489 pre_train:0.9267 recall_train:0.9785 F1_train:0.9519 AUC_train:0.9808\n",
      "acc_val:0.8400 pre_val:0.7656 recall_val:0.9800 F1_val:0.859649 AUC_val:0.9314\n",
      "Epoch:0081\n",
      "acc_train:0.9556 pre_train:0.9436 recall_train:0.9720 F1_train:0.9576 AUC_train:0.9826\n",
      "acc_val:0.8500 pre_val:0.7869 recall_val:0.9600 F1_val:0.864865 AUC_val:0.9178\n",
      "Epoch:0082\n",
      "acc_train:0.9500 pre_train:0.9234 recall_train:0.9849 F1_train:0.9532 AUC_train:0.9819\n",
      "acc_val:0.8700 pre_val:0.8136 recall_val:0.9600 F1_val:0.880734 AUC_val:0.9154\n",
      "Epoch:0083\n",
      "acc_train:0.9522 pre_train:0.9254 recall_train:0.9871 F1_train:0.9553 AUC_train:0.9738\n",
      "acc_val:0.8800 pre_val:0.8167 recall_val:0.9800 F1_val:0.890909 AUC_val:0.9134\n",
      "Epoch:0084\n",
      "acc_train:0.9478 pre_train:0.9283 recall_train:0.9742 F1_train:0.9507 AUC_train:0.9778\n",
      "acc_val:0.8700 pre_val:0.8033 recall_val:0.9800 F1_val:0.882883 AUC_val:0.9166\n",
      "Epoch:0085\n",
      "acc_train:0.9633 pre_train:0.9500 recall_train:0.9806 F1_train:0.9651 AUC_train:0.9845\n",
      "acc_val:0.8400 pre_val:0.7656 recall_val:0.9800 F1_val:0.859649 AUC_val:0.9260\n",
      "Epoch:0086\n",
      "acc_train:0.9600 pre_train:0.9423 recall_train:0.9828 F1_train:0.9621 AUC_train:0.9848\n",
      "acc_val:0.8300 pre_val:0.7538 recall_val:0.9800 F1_val:0.852174 AUC_val:0.9356\n",
      "Epoch:0087\n",
      "acc_train:0.9656 pre_train:0.9521 recall_train:0.9828 F1_train:0.9672 AUC_train:0.9847\n",
      "acc_val:0.8000 pre_val:0.7206 recall_val:0.9800 F1_val:0.830508 AUC_val:0.9370\n",
      "Epoch:0088\n",
      "acc_train:0.9633 pre_train:0.9481 recall_train:0.9828 F1_train:0.9652 AUC_train:0.9819\n",
      "acc_val:0.7800 pre_val:0.7000 recall_val:0.9800 F1_val:0.816667 AUC_val:0.9358\n",
      "Epoch:0089\n",
      "acc_train:0.9456 pre_train:0.9194 recall_train:0.9806 F1_train:0.9490 AUC_train:0.9861\n",
      "acc_val:0.7800 pre_val:0.7000 recall_val:0.9800 F1_val:0.816667 AUC_val:0.9290\n",
      "Epoch:0090\n",
      "acc_train:0.9578 pre_train:0.9402 recall_train:0.9806 F1_train:0.9600 AUC_train:0.9889\n",
      "acc_val:0.8000 pre_val:0.7206 recall_val:0.9800 F1_val:0.830508 AUC_val:0.9280\n",
      "Epoch:0091\n",
      "acc_train:0.9444 pre_train:0.9209 recall_train:0.9763 F1_train:0.9478 AUC_train:0.9878\n",
      "acc_val:0.8200 pre_val:0.7424 recall_val:0.9800 F1_val:0.844828 AUC_val:0.9248\n",
      "Epoch:0092\n",
      "acc_train:0.9656 pre_train:0.9429 recall_train:0.9935 F1_train:0.9675 AUC_train:0.9899\n",
      "acc_val:0.8300 pre_val:0.7538 recall_val:0.9800 F1_val:0.852174 AUC_val:0.9236\n",
      "Epoch:0093\n",
      "acc_train:0.9656 pre_train:0.9540 recall_train:0.9806 F1_train:0.9671 AUC_train:0.9880\n",
      "acc_val:0.8300 pre_val:0.7619 recall_val:0.9600 F1_val:0.849558 AUC_val:0.9236\n",
      "Epoch:0094\n",
      "acc_train:0.9611 pre_train:0.9406 recall_train:0.9871 F1_train:0.9633 AUC_train:0.9825\n",
      "acc_val:0.8300 pre_val:0.7619 recall_val:0.9600 F1_val:0.849558 AUC_val:0.9244\n",
      "Epoch:0095\n",
      "acc_train:0.9600 pre_train:0.9441 recall_train:0.9806 F1_train:0.9620 AUC_train:0.9854\n",
      "acc_val:0.8100 pre_val:0.7385 recall_val:0.9600 F1_val:0.834783 AUC_val:0.9292\n",
      "Epoch:0096\n",
      "acc_train:0.9578 pre_train:0.9366 recall_train:0.9849 F1_train:0.9602 AUC_train:0.9867\n",
      "acc_val:0.7800 pre_val:0.7059 recall_val:0.9600 F1_val:0.813559 AUC_val:0.9324\n",
      "Epoch:0097\n",
      "acc_train:0.9722 pre_train:0.9622 recall_train:0.9849 F1_train:0.9734 AUC_train:0.9895\n",
      "acc_val:0.7900 pre_val:0.7164 recall_val:0.9600 F1_val:0.820513 AUC_val:0.9320\n",
      "Epoch:0098\n",
      "acc_train:0.9711 pre_train:0.9621 recall_train:0.9828 F1_train:0.9723 AUC_train:0.9918\n",
      "acc_val:0.8100 pre_val:0.7385 recall_val:0.9600 F1_val:0.834783 AUC_val:0.9316\n",
      "Epoch:0099\n",
      "acc_train:0.9778 pre_train:0.9764 recall_train:0.9806 F1_train:0.9785 AUC_train:0.9935\n",
      "acc_val:0.8100 pre_val:0.7460 recall_val:0.9400 F1_val:0.831858 AUC_val:0.9284\n",
      "Early Stopping!!! epoch：98\n",
      "=================================================================== 0 _ 9\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'files\\rois_cc200'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 220\u001b[0m\n\u001b[0;32m    218\u001b[0m epoch_file_path \u001b[38;5;241m=\u001b[39m  \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./files/rois_cc200/file_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mj\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    219\u001b[0m data_file \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(data);\n\u001b[1;32m--> 220\u001b[0m data_file\u001b[38;5;241m.\u001b[39mto_csv(epoch_file_path , index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m);\n\u001b[0;32m    221\u001b[0m count\u001b[38;5;241m=\u001b[39mcount\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m;\n\u001b[0;32m    222\u001b[0m \u001b[38;5;66;03m# test\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\core\\generic.py:3772\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3761\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_frame()\n\u001b[0;32m   3763\u001b[0m formatter \u001b[38;5;241m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3764\u001b[0m     frame\u001b[38;5;241m=\u001b[39mdf,\n\u001b[0;32m   3765\u001b[0m     header\u001b[38;5;241m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3769\u001b[0m     decimal\u001b[38;5;241m=\u001b[39mdecimal,\n\u001b[0;32m   3770\u001b[0m )\n\u001b[1;32m-> 3772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[38;5;241m.\u001b[39mto_csv(\n\u001b[0;32m   3773\u001b[0m     path_or_buf,\n\u001b[0;32m   3774\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[0;32m   3775\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   3776\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m   3777\u001b[0m     errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m   3778\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m   3779\u001b[0m     quoting\u001b[38;5;241m=\u001b[39mquoting,\n\u001b[0;32m   3780\u001b[0m     columns\u001b[38;5;241m=\u001b[39mcolumns,\n\u001b[0;32m   3781\u001b[0m     index_label\u001b[38;5;241m=\u001b[39mindex_label,\n\u001b[0;32m   3782\u001b[0m     mode\u001b[38;5;241m=\u001b[39mmode,\n\u001b[0;32m   3783\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m   3784\u001b[0m     quotechar\u001b[38;5;241m=\u001b[39mquotechar,\n\u001b[0;32m   3785\u001b[0m     date_format\u001b[38;5;241m=\u001b[39mdate_format,\n\u001b[0;32m   3786\u001b[0m     doublequote\u001b[38;5;241m=\u001b[39mdoublequote,\n\u001b[0;32m   3787\u001b[0m     escapechar\u001b[38;5;241m=\u001b[39mescapechar,\n\u001b[0;32m   3788\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m   3789\u001b[0m )\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\format.py:1186\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1165\u001b[0m     created_buffer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1167\u001b[0m csv_formatter \u001b[38;5;241m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1168\u001b[0m     path_or_buf\u001b[38;5;241m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1169\u001b[0m     lineterminator\u001b[38;5;241m=\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1184\u001b[0m     formatter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfmt,\n\u001b[0;32m   1185\u001b[0m )\n\u001b[1;32m-> 1186\u001b[0m csv_formatter\u001b[38;5;241m.\u001b[39msave()\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1189\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\formats\\csvs.py:240\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    237\u001b[0m \u001b[38;5;124;03mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_handle(\n\u001b[0;32m    241\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilepath_or_buffer,\n\u001b[0;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    243\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    244\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39merrors,\n\u001b[0;32m    245\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompression,\n\u001b[0;32m    246\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options,\n\u001b[0;32m    247\u001b[0m ) \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;241m=\u001b[39m csvlib\u001b[38;5;241m.\u001b[39mwriter(\n\u001b[0;32m    250\u001b[0m         handles\u001b[38;5;241m.\u001b[39mhandle,\n\u001b[0;32m    251\u001b[0m         lineterminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlineterminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         quotechar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquotechar,\n\u001b[0;32m    257\u001b[0m     )\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save()\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:737\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[0;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[1;32m--> 737\u001b[0m     check_parent_directory(\u001b[38;5;28mstr\u001b[39m(handle))\n\u001b[0;32m    739\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[0;32m    740\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m compression \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzstd\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    741\u001b[0m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mC:\\ProgramData\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:600\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    598\u001b[0m parent \u001b[38;5;241m=\u001b[39m Path(path)\u001b[38;5;241m.\u001b[39mparent\n\u001b[0;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent\u001b[38;5;241m.\u001b[39mis_dir():\n\u001b[1;32m--> 600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[38;5;124mrf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot save file into a non-existent directory: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'files\\rois_cc200'"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import io\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# from dataloader import dataloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "if hasattr(sys.stdout, 'buffer'):\n",
    "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.no_cuda = False\n",
    "        self.seed = 46\n",
    "        self.epochs = 200\n",
    "        self.lr = 0.001\n",
    "        self.weight_decay = 5e-5\n",
    "        self.hidden = 16\n",
    "        self.dropout = 0.2\n",
    "        self.atlas = 'ho'\n",
    "        self.num_features = 2000\n",
    "        self.folds = 10\n",
    "        self.connectivity = 'correlation'\n",
    "        self.max_degree = 3\n",
    "        self.ngl = 8\n",
    "        self.edropout = 0.3\n",
    "        self.train = 1\n",
    "        self.ckpt_path = '../folds/rois_cc200_pth'\n",
    "        self.early_stopping = True\n",
    "        self.early_stopping_patience = 20\n",
    "\n",
    "# Instantiate Args class\n",
    "args = Args()\n",
    "\n",
    "# Check if CUDA is available\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Create params dictionary\n",
    "params = vars(args)\n",
    "\n",
    "# Print Hyperparameters\n",
    "print('Hyperparameters:')\n",
    "for key, value in params.items():\n",
    "    print(key + \":\", value)\n",
    "\n",
    "corrects = np.zeros(args.folds, dtype=np.int32) \n",
    "accs = np.zeros(args.folds, dtype=np.float32) \n",
    "aucs = np.zeros(args.folds, dtype=np.float32)\n",
    "prfs = np.zeros([args.folds,3], dtype=np.float32) # Save Precision, Recall, F1\n",
    "test_num = np.zeros(args.folds, dtype=np.float32)\n",
    "\n",
    "\n",
    "print('  Loading dataset ...')\n",
    "dataloader = dataloader()\n",
    "raw_features, y, nonimg = dataloader.load_data(params) \n",
    "cv_splits = dataloader.data_split(args.folds)\n",
    "features=raw_features\n",
    "\n",
    "t1 = time.time()\n",
    "count=1;\n",
    "for i in range(args.folds):\n",
    "    \n",
    "    \n",
    "    \n",
    "    t_start = time.time()\n",
    "    train_ind, test_ind = cv_splits[i]\n",
    "\n",
    "    train_ind, valid_ind = train_test_split(train_ind, test_size=0.1, random_state = 24)\n",
    "    \n",
    "    cv_splits[i] = (train_ind, valid_ind)\n",
    "    cv_splits[i] = cv_splits[i] + (test_ind,)\n",
    "    print('Size of the {}-fold Training, Validation, and Test Sets:{},{},{}' .format(i+1, len(cv_splits[i][0]), len(cv_splits[i][1]), len(cv_splits[i][2])))\n",
    "\n",
    "    if args.train == 1:\n",
    "        for j in range(args.folds):\n",
    "            print(' Starting the {}-{} Fold:：'.format(i+1,j+1))\n",
    "            node_ftr = dataloader.get_node_features(train_ind)\n",
    "            edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "            edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "            \n",
    "            model = GCN(input_dim = args.num_features,\n",
    "                        nhid = args.hidden, \n",
    "                        num_classes = 2, \n",
    "                        ngl = args.ngl, \n",
    "                        dropout = args.dropout, \n",
    "                        edge_dropout = args.edropout, \n",
    "                        edgenet_input_dim = 2*nonimg.shape[1])\n",
    "            optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "            \n",
    "#             if args.cuda:\n",
    "            model\n",
    "            features = torch.tensor(node_ftr, dtype=torch.float32)\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "            edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "            labels = torch.tensor(y, dtype=torch.long)\n",
    "            fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "                \n",
    "            acc = 0\n",
    "            best_val_loss = float('inf') # early stoppping: Initialized to positive infinity\n",
    "            current_patience = 0 # early stopping: Used to record the epochs of the current early stopping\n",
    "            \n",
    "            epoch_store = []\n",
    "            acc_train_store =[]        \n",
    "            pre_train_store =[]\n",
    "            recall_train_store =[]\n",
    "            F1_train_store =[]\n",
    "            AUC_train_store =[]\n",
    "            acc_val_store=[]\n",
    "            pre_val_store=[]\n",
    "            recall_val_store=[]\n",
    "            F1_val_store=[]\n",
    "            AUC_val_store=[]\n",
    "            \n",
    "            for epoch in range(args.epochs):\n",
    "                # train\n",
    "                model.train()\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    optimizer.zero_grad()\n",
    "                    output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "                    loss_train = torch.nn.CrossEntropyLoss()(output[train_ind], labels[train_ind])\n",
    "                    loss_train.backward()\n",
    "                    optimizer.step()\n",
    "                acc_train = torchmetrics_accuracy(output[train_ind], labels[train_ind])\n",
    "                auc_train = torchmetrics_auc(output[train_ind], labels[train_ind])\n",
    "                logits_train = output[train_ind].detach().cpu().numpy()\n",
    "                prf_train = prf(logits_train, y[train_ind])\n",
    "\n",
    "                \n",
    "                # valid\n",
    "                model.eval()\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "                loss_val = torch.nn.CrossEntropyLoss()(output[valid_ind], labels[valid_ind])\n",
    "                acc_val = torchmetrics_accuracy(output[valid_ind], labels[valid_ind])\n",
    "                auc_val = torchmetrics_auc(output[valid_ind], labels[valid_ind])\n",
    "                logits_val = output[valid_ind].detach().cpu().numpy()\n",
    "                prf_val = prf(logits_val, y[valid_ind])\n",
    "\n",
    "                \n",
    "                print('Epoch:{:04d}'.format(epoch+1))\n",
    "                print('acc_train:{:.4f}'.format(acc_train),\n",
    "                      'pre_train:{:.4f}'.format(prf_train[0]),\n",
    "                      'recall_train:{:.4f}'.format(prf_train[1]),\n",
    "                      'F1_train:{:.4f}'.format(prf_train[2]),\n",
    "                      'AUC_train:{:.4f}'.format(auc_train))\n",
    "                print('acc_val:{:.4f}'.format(acc_val),\n",
    "                      'pre_val:{:.4f}'.format(prf_val[0]),\n",
    "                      'recall_val:{:.4f}'.format(prf_val[1]),\n",
    "                      'F1_val:{:4f}'.format(prf_val[2]),\n",
    "                      'AUC_val:{:.4f}'.format(auc_val))\n",
    "                \n",
    "                epoch_store.append(epoch+1)\n",
    "                acc_train_store.append(acc_train)       \n",
    "                pre_train_store.append(prf_train[0])\n",
    "                recall_train_store.append(prf_train[1])\n",
    "                F1_train_store.append(prf_train[2])\n",
    "                AUC_train_store.append(auc_train)\n",
    "                acc_val_store.append(acc_val)\n",
    "                pre_val_store.append(prf_val[0])\n",
    "                recall_val_store.append(prf_val[1])\n",
    "                F1_val_store.append(prf_val[2])\n",
    "                AUC_val_store.append(auc_val)\n",
    "                \n",
    "                # save pth\n",
    "                if acc_val > acc and epoch > 50:\n",
    "                    acc = acc_val\n",
    "                    if args.ckpt_path != '':\n",
    "                        if not os.path.exists(args.ckpt_path):\n",
    "                            os.makedirs(args.ckpt_path)\n",
    "                        torch.save(model.state_dict(), fold_model_path)\n",
    "                \n",
    "                # Early Stopping\n",
    "                if epoch > 50 and args.early_stopping == True:\n",
    "                    if loss_val < best_val_loss:\n",
    "                        best_val_loss = loss_val\n",
    "                        current_patience = 0\n",
    "                    else:\n",
    "                        current_patience += 1\n",
    "                    if current_patience >= args.early_stopping_patience:\n",
    "                        print('Early Stopping!!! epoch：{}'.format(epoch))\n",
    "                        break\n",
    "        print(\"===================================================================\",i,\"_\",j)\n",
    "        data  = { \n",
    "              \"epoch\" : epoch_store ,\n",
    "              \"acc_train\" : acc_train_store ,        \n",
    "              \"pre_train\" : pre_train_store ,\n",
    "              \"recall_train\" : recall_train_store ,\n",
    "              \"F1_train\" : F1_train_store ,\n",
    "              \"AUC_train\" : AUC_train_store ,\n",
    "              \"acc_val\" : acc_val_store,\n",
    "               \"pre_val\" : pre_val_store ,\n",
    "              \"recall_val\" : recall_val_store ,\n",
    "              \"F1_val\" : F1_val_store ,\n",
    "              \"AUC_val\" : AUC_val_store  \n",
    "        }\n",
    "        \n",
    "        \n",
    "        epoch_file_path =  f'../files/rois_cc200/file_{i}_{j}_{count}.csv'\n",
    "        data_file = pd.DataFrame(data);\n",
    "        data_file.to_csv(epoch_file_path , index=False);\n",
    "        count=count+1;\n",
    "        # test\n",
    "        print(\"Loading the Model for the {}-th Fold:... ...\".format(i+1),\n",
    "              \"Size of samples in the test set:{}\".format(len(test_ind)))\n",
    "        model.load_state_dict(torch.load(fold_model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "        acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "        auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "        logits_test = output[test_ind].detach().cpu().numpy()\n",
    "        correct_test = correct_num(logits_test, y[test_ind])\n",
    "        prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "        t_end = time.time()\n",
    "        t = t_end - t_start\n",
    "        print('Fold {} Results:'.format(i+1),\n",
    "              'test acc:{:.4f}'.format(acc_test),\n",
    "              'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "              'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "              'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "              'test_AUC:{:.4f}'.format(auc_test),\n",
    "              'time:{:.3f}s'.format(t))\n",
    "        \n",
    "        correct = correct_test\n",
    "        aucs[i] = auc_test\n",
    "        prfs[i] = prf_test\n",
    "        corrects[i] = correct\n",
    "        test_num[i] = len(test_ind)\n",
    "    \n",
    "    \n",
    "    if args.train == 0:\n",
    "        node_ftr = dataloader.get_node_features(train_ind)\n",
    "        edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "        edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "        \n",
    "        model = GCN(input_dim = args.num_features,\n",
    "                    nhid = args.hidden, \n",
    "                    num_classes = 2, \n",
    "                    ngl = args.ngl, \n",
    "                    dropout = args.dropout, \n",
    "                    edge_dropout = args.edropout, \n",
    "                    edgenet_input_dim = 2*nonimg.shape[1])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "#         if args.cuda\n",
    "        model\n",
    "        features = torch.tensor(node_ftr, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "        labels = torch.tensor(y, dtype=torch.long)\n",
    "        fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "        \n",
    "        model.load_state_dict(torch.load(fold_model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "        acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "        auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "        logits_test = output[test_ind].detach().cpu().numpy()\n",
    "        correct_test = correct_num(logits_test, y[test_ind])\n",
    "        prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "        t_end = time.time()\n",
    "        t = t_end - t_start\n",
    "        print('Fold {} Results:'.format(i+1),\n",
    "              'test acc:{:.4f}'.format(acc_test),\n",
    "              'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "              'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "              'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "              'test_AUC:{:.4f}'.format(auc_test),\n",
    "              'time:{:.3f}s'.format(t))\n",
    "        \n",
    "        correct = correct_test\n",
    "        aucs[i] = auc_test\n",
    "        prfs[i] = prf_test\n",
    "        corrects[i] = correct\n",
    "        test_num[i] = len(test_ind)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print('\\r\\n======Finish Results for Nested 10-fold cross-validation======')\n",
    "Nested10kCV_acc = np.sum(corrects) / np.sum(test_num)\n",
    "Nested10kCV_auc = np.mean(aucs)\n",
    "Nested10kCV_precision, Nested10kCV_recall, Nested10kCV_F1 = np.mean(prfs, axis=0)\n",
    "print('Test:',\n",
    "      'acc:{}'.format(Nested10kCV_acc),\n",
    "      'precision:{}'.format(Nested10kCV_precision),\n",
    "      'recall:{}'.format(Nested10kCV_recall),\n",
    "      'F1:{}'.format(Nested10kCV_F1),\n",
    "      'AUC:{}'.format(Nested10kCV_auc))\n",
    "print('Total duration:{}'.format(t2 - t1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d845c726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "no_cuda: False\n",
      "seed: 46\n",
      "epochs: 200\n",
      "lr: 0.001\n",
      "weight_decay: 5e-05\n",
      "hidden: 16\n",
      "dropout: 0.2\n",
      "atlas: cc200\n",
      "num_features: 2000\n",
      "folds: 10\n",
      "connectivity: correlation\n",
      "max_degree: 3\n",
      "ngl: 8\n",
      "edropout: 0.3\n",
      "train: 0\n",
      "ckpt_path: ./pth\n",
      "early_stopping: True\n",
      "early_stopping_patience: 20\n",
      "cuda: False\n",
      "  Loading dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_23528\\2821594311.py:216: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the 1-fold Training, Validation, and Test Sets:900,100,112\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Fold 1 Results: test acc:0.6071 test_pre:0.5946 test_recall:0.7586 test_F1:0.6667 test_AUC:0.6437 time:44.829s\n",
      "Size of the 2-fold Training, Validation, and Test Sets:900,100,112\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Fold 2 Results: test acc:0.7768 test_pre:0.7705 test_recall:0.8103 test_F1:0.7899 test_AUC:0.8413 time:44.733s\n",
      "Size of the 3-fold Training, Validation, and Test Sets:900,101,111\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Fold 3 Results: test acc:0.7568 test_pre:0.6923 test_recall:0.9474 test_F1:0.8000 test_AUC:0.9185 time:46.675s\n",
      "Size of the 4-fold Training, Validation, and Test Sets:900,101,111\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Fold 4 Results: test acc:0.5225 test_pre:1.0000 test_recall:0.0702 test_F1:0.1311 test_AUC:0.4607 time:46.857s\n",
      "Size of the 5-fold Training, Validation, and Test Sets:900,101,111\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Fold 5 Results: test acc:0.6306 test_pre:0.6026 test_recall:0.8246 test_F1:0.6963 test_AUC:0.5832 time:47.592s\n",
      "Size of the 6-fold Training, Validation, and Test Sets:900,101,111\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Fold 6 Results: test acc:0.6036 test_pre:0.5942 test_recall:0.7193 test_F1:0.6508 test_AUC:0.7068 time:48.197s\n",
      "Size of the 7-fold Training, Validation, and Test Sets:900,101,111\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Fold 7 Results: test acc:0.8559 test_pre:0.8154 test_recall:0.9298 test_F1:0.8689 test_AUC:0.9360 time:47.777s\n",
      "Size of the 8-fold Training, Validation, and Test Sets:900,101,111\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Fold 8 Results: test acc:0.8108 test_pre:0.7571 test_recall:0.9298 test_F1:0.8346 test_AUC:0.9405 time:49.270s\n",
      "Size of the 9-fold Training, Validation, and Test Sets:900,101,111\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n",
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Fold 9 Results: test acc:0.7477 test_pre:0.6883 test_recall:0.9298 test_F1:0.7910 test_AUC:0.9090 time:48.770s\n",
      "Size of the 10-fold Training, Validation, and Test Sets:900,101,111\n",
      "Fitting estimator with 19900 features.\n",
      "Fitting estimator with 19800 features.\n",
      "Fitting estimator with 19700 features.\n",
      "Fitting estimator with 19600 features.\n",
      "Fitting estimator with 19500 features.\n",
      "Fitting estimator with 19400 features.\n",
      "Fitting estimator with 19300 features.\n",
      "Fitting estimator with 19200 features.\n",
      "Fitting estimator with 19100 features.\n",
      "Fitting estimator with 19000 features.\n",
      "Fitting estimator with 18900 features.\n",
      "Fitting estimator with 18800 features.\n",
      "Fitting estimator with 18700 features.\n",
      "Fitting estimator with 18600 features.\n",
      "Fitting estimator with 18500 features.\n",
      "Fitting estimator with 18400 features.\n",
      "Fitting estimator with 18300 features.\n",
      "Fitting estimator with 18200 features.\n",
      "Fitting estimator with 18100 features.\n",
      "Fitting estimator with 18000 features.\n",
      "Fitting estimator with 17900 features.\n",
      "Fitting estimator with 17800 features.\n",
      "Fitting estimator with 17700 features.\n",
      "Fitting estimator with 17600 features.\n",
      "Fitting estimator with 17500 features.\n",
      "Fitting estimator with 17400 features.\n",
      "Fitting estimator with 17300 features.\n",
      "Fitting estimator with 17200 features.\n",
      "Fitting estimator with 17100 features.\n",
      "Fitting estimator with 17000 features.\n",
      "Fitting estimator with 16900 features.\n",
      "Fitting estimator with 16800 features.\n",
      "Fitting estimator with 16700 features.\n",
      "Fitting estimator with 16600 features.\n",
      "Fitting estimator with 16500 features.\n",
      "Fitting estimator with 16400 features.\n",
      "Fitting estimator with 16300 features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting estimator with 16200 features.\n",
      "Fitting estimator with 16100 features.\n",
      "Fitting estimator with 16000 features.\n",
      "Fitting estimator with 15900 features.\n",
      "Fitting estimator with 15800 features.\n",
      "Fitting estimator with 15700 features.\n",
      "Fitting estimator with 15600 features.\n",
      "Fitting estimator with 15500 features.\n",
      "Fitting estimator with 15400 features.\n",
      "Fitting estimator with 15300 features.\n",
      "Fitting estimator with 15200 features.\n",
      "Fitting estimator with 15100 features.\n",
      "Fitting estimator with 15000 features.\n",
      "Fitting estimator with 14900 features.\n",
      "Fitting estimator with 14800 features.\n",
      "Fitting estimator with 14700 features.\n",
      "Fitting estimator with 14600 features.\n",
      "Fitting estimator with 14500 features.\n",
      "Fitting estimator with 14400 features.\n",
      "Fitting estimator with 14300 features.\n",
      "Fitting estimator with 14200 features.\n",
      "Fitting estimator with 14100 features.\n",
      "Fitting estimator with 14000 features.\n",
      "Fitting estimator with 13900 features.\n",
      "Fitting estimator with 13800 features.\n",
      "Fitting estimator with 13700 features.\n",
      "Fitting estimator with 13600 features.\n",
      "Fitting estimator with 13500 features.\n",
      "Fitting estimator with 13400 features.\n",
      "Fitting estimator with 13300 features.\n",
      "Fitting estimator with 13200 features.\n",
      "Fitting estimator with 13100 features.\n",
      "Fitting estimator with 13000 features.\n",
      "Fitting estimator with 12900 features.\n",
      "Fitting estimator with 12800 features.\n",
      "Fitting estimator with 12700 features.\n",
      "Fitting estimator with 12600 features.\n",
      "Fitting estimator with 12500 features.\n",
      "Fitting estimator with 12400 features.\n",
      "Fitting estimator with 12300 features.\n",
      "Fitting estimator with 12200 features.\n",
      "Fitting estimator with 12100 features.\n",
      "Fitting estimator with 12000 features.\n",
      "Fitting estimator with 11900 features.\n",
      "Fitting estimator with 11800 features.\n",
      "Fitting estimator with 11700 features.\n",
      "Fitting estimator with 11600 features.\n",
      "Fitting estimator with 11500 features.\n",
      "Fitting estimator with 11400 features.\n",
      "Fitting estimator with 11300 features.\n",
      "Fitting estimator with 11200 features.\n",
      "Fitting estimator with 11100 features.\n",
      "Fitting estimator with 11000 features.\n",
      "Fitting estimator with 10900 features.\n",
      "Fitting estimator with 10800 features.\n",
      "Fitting estimator with 10700 features.\n",
      "Fitting estimator with 10600 features.\n",
      "Fitting estimator with 10500 features.\n",
      "Fitting estimator with 10400 features.\n",
      "Fitting estimator with 10300 features.\n",
      "Fitting estimator with 10200 features.\n",
      "Fitting estimator with 10100 features.\n",
      "Fitting estimator with 10000 features.\n",
      "Fitting estimator with 9900 features.\n",
      "Fitting estimator with 9800 features.\n",
      "Fitting estimator with 9700 features.\n",
      "Fitting estimator with 9600 features.\n",
      "Fitting estimator with 9500 features.\n",
      "Fitting estimator with 9400 features.\n",
      "Fitting estimator with 9300 features.\n",
      "Fitting estimator with 9200 features.\n",
      "Fitting estimator with 9100 features.\n",
      "Fitting estimator with 9000 features.\n",
      "Fitting estimator with 8900 features.\n",
      "Fitting estimator with 8800 features.\n",
      "Fitting estimator with 8700 features.\n",
      "Fitting estimator with 8600 features.\n",
      "Fitting estimator with 8500 features.\n",
      "Fitting estimator with 8400 features.\n",
      "Fitting estimator with 8300 features.\n",
      "Fitting estimator with 8200 features.\n",
      "Fitting estimator with 8100 features.\n",
      "Fitting estimator with 8000 features.\n",
      "Fitting estimator with 7900 features.\n",
      "Fitting estimator with 7800 features.\n",
      "Fitting estimator with 7700 features.\n",
      "Fitting estimator with 7600 features.\n",
      "Fitting estimator with 7500 features.\n",
      "Fitting estimator with 7400 features.\n",
      "Fitting estimator with 7300 features.\n",
      "Fitting estimator with 7200 features.\n",
      "Fitting estimator with 7100 features.\n",
      "Fitting estimator with 7000 features.\n",
      "Fitting estimator with 6900 features.\n",
      "Fitting estimator with 6800 features.\n",
      "Fitting estimator with 6700 features.\n",
      "Fitting estimator with 6600 features.\n",
      "Fitting estimator with 6500 features.\n",
      "Fitting estimator with 6400 features.\n",
      "Fitting estimator with 6300 features.\n",
      "Fitting estimator with 6200 features.\n",
      "Fitting estimator with 6100 features.\n",
      "Fitting estimator with 6000 features.\n",
      "Fitting estimator with 5900 features.\n",
      "Fitting estimator with 5800 features.\n",
      "Fitting estimator with 5700 features.\n",
      "Fitting estimator with 5600 features.\n",
      "Fitting estimator with 5500 features.\n",
      "Fitting estimator with 5400 features.\n",
      "Fitting estimator with 5300 features.\n",
      "Fitting estimator with 5200 features.\n",
      "Fitting estimator with 5100 features.\n",
      "Fitting estimator with 5000 features.\n",
      "Fitting estimator with 4900 features.\n",
      "Fitting estimator with 4800 features.\n",
      "Fitting estimator with 4700 features.\n",
      "Fitting estimator with 4600 features.\n",
      "Fitting estimator with 4500 features.\n",
      "Fitting estimator with 4400 features.\n",
      "Fitting estimator with 4300 features.\n",
      "Fitting estimator with 4200 features.\n",
      "Fitting estimator with 4100 features.\n",
      "Fitting estimator with 4000 features.\n",
      "Fitting estimator with 3900 features.\n",
      "Fitting estimator with 3800 features.\n",
      "Fitting estimator with 3700 features.\n",
      "Fitting estimator with 3600 features.\n",
      "Fitting estimator with 3500 features.\n",
      "Fitting estimator with 3400 features.\n",
      "Fitting estimator with 3300 features.\n",
      "Fitting estimator with 3200 features.\n",
      "Fitting estimator with 3100 features.\n",
      "Fitting estimator with 3000 features.\n",
      "Fitting estimator with 2900 features.\n",
      "Fitting estimator with 2800 features.\n",
      "Fitting estimator with 2700 features.\n",
      "Fitting estimator with 2600 features.\n",
      "Fitting estimator with 2500 features.\n",
      "Fitting estimator with 2400 features.\n",
      "Fitting estimator with 2300 features.\n",
      "Fitting estimator with 2200 features.\n",
      "Fitting estimator with 2100 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Fold 10 Results: test acc:0.5495 test_pre:0.6250 test_recall:0.3448 test_F1:0.4444 test_AUC:0.6278 time:49.396s\n",
      "\n",
      "======Finish Results for Nested 10-fold cross-validation======\n",
      "Test: acc:0.6861510791366906 precision:0.7140000462532043 recall:0.7264670133590698 F1:0.6673808097839355 AUC:0.7567518353462219\n",
      "Total duration:474.1001989841461\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03838a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
