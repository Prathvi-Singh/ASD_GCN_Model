{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc5d23a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "from nilearn import connectome\n",
    "\n",
    "from scipy.spatial import distance\n",
    "\n",
    "\n",
    "# Reading and computing the input data\n",
    "\n",
    "# Selected pipeline\n",
    "pipeline = 'cpac'\n",
    "\n",
    "# Input data variables\n",
    "root_folder = '../ABIDE/'\n",
    "data_folder = os.path.join(root_folder, 'ABIDE_pcp/cpac/filt_noglobal')\n",
    "phenotype = os.path.join(root_folder, 'ABIDE_pcp/Phenotypic_V1_0b_preprocessed1.csv')\n",
    "\n",
    "\n",
    "def fetch_filenames(subject_IDs, file_type):\n",
    "\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        file_type    : must be one of the available file types\n",
    "\n",
    "    returns:\n",
    "\n",
    "        filenames    : list of filetypes (same length as subject_list)\n",
    "    \"\"\"\n",
    "\n",
    "    import glob\n",
    "\n",
    "    # Specify file mappings for the possible file types\n",
    "    filemapping = {'func_preproc': '_func_preproc.nii.gz',\n",
    "                   'rois_ho': '_rois_ho.1D'}\n",
    "\n",
    "    # The list to be filled\n",
    "    filenames = []\n",
    "\n",
    "    # Fill list with requested file paths\n",
    "    for i in range(len(subject_IDs)):\n",
    "        os.chdir(data_folder)  # os.path.join(data_folder, subject_IDs[i]))\n",
    "        try:\n",
    "            filenames.append(glob.glob('*' + subject_IDs[i] + filemapping[file_type])[0])\n",
    "        except IndexError:\n",
    "            # Return N/A if subject ID is not found\n",
    "            filenames.append('N/A')\n",
    "\n",
    "    return filenames\n",
    "\n",
    "\n",
    "# Get timeseries arrays for list of subjects\n",
    "def get_timeseries(subject_list, atlas_name):\n",
    "    \"\"\"\n",
    "        subject_list : list of short subject IDs in string format\n",
    "        atlas_name   : the atlas based on which the timeseries are generated e.g. aal, cc200\n",
    "\n",
    "    returns:\n",
    "        time_series  : list of timeseries arrays, each of shape (timepoints x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    timeseries = []\n",
    "    for i in range(len(subject_list)):\n",
    "        subject_folder = os.path.join(data_folder, subject_list[i])\n",
    "        ro_file = [f for f in os.listdir(subject_folder) if f.endswith('_rois_' + atlas_name + '.1D')]\n",
    "        fl = os.path.join(subject_folder, ro_file[0])\n",
    "        print(\"Reading timeseries file %s\" %fl)\n",
    "        timeseries.append(np.loadtxt(fl, skiprows=0))\n",
    "\n",
    "    return timeseries\n",
    "\n",
    "\n",
    "# Compute connectivity matrices\n",
    "def subject_connectivity(timeseries, subject, atlas_name, kind, save=True, save_path=data_folder):\n",
    "    \"\"\"\n",
    "        timeseries   : timeseries table for subject (timepoints x regions)\n",
    "        subject      : the subject ID\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        save         : save the connectivity matrix to a file\n",
    "        save_path    : specify path to save the matrix if different from subject folder\n",
    "\n",
    "    returns:\n",
    "        connectivity : connectivity matrix (regions x regions)\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Estimating %s matrix for subject %s\" % (kind, subject))\n",
    "\n",
    "    if kind in ['tangent', 'partial correlation', 'correlation']:\n",
    "        conn_measure = connectome.ConnectivityMeasure(kind=kind)\n",
    "        connectivity = conn_measure.fit_transform([timeseries])[0]\n",
    "\n",
    "    if save:\n",
    "        subject_file = os.path.join(save_path, subject,\n",
    "                                    subject + '_' + atlas_name + '_' + kind.replace(' ', '_') + '.mat')\n",
    "        sio.savemat(subject_file, {'connectivity': connectivity})\n",
    "\n",
    "    return connectivity\n",
    "\n",
    "\n",
    "# Get the list of subject IDs\n",
    "def get_ids(num_subjects=None):\n",
    "    \"\"\"\n",
    "\n",
    "    return:\n",
    "        subject_IDs    : list of all subject IDs\n",
    "    \"\"\"\n",
    "\n",
    "    subject_IDs = np.genfromtxt(os.path.join(data_folder, 'subject_IDs.txt'), dtype=str)\n",
    "\n",
    "    if num_subjects is not None:\n",
    "        subject_IDs = subject_IDs[:num_subjects]\n",
    "\n",
    "    return subject_IDs\n",
    "\n",
    "\n",
    "# Get phenotype values for a list of subjects\n",
    "def get_subject_score(subject_list, score):\n",
    "    scores_dict = {}\n",
    "\n",
    "    with open(phenotype) as csv_file:\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        for row in reader:\n",
    "            if row['SUB_ID'] in subject_list:\n",
    "                scores_dict[row['SUB_ID']] = row[score]\n",
    "\n",
    "    return scores_dict\n",
    "\n",
    "\n",
    "# Dimensionality reduction step for the feature vector using a ridge classifier\n",
    "def feature_selection(matrix, labels, train_ind, fnum):\n",
    "    \"\"\"\n",
    "        matrix       : feature matrix (num_subjects x num_features)\n",
    "        labels       : ground truth labels (num_subjects x 1)\n",
    "        train_ind    : indices of the training samples\n",
    "        fnum         : size of the feature vector after feature selection\n",
    "\n",
    "    return:\n",
    "        x_data      : feature matrix of lower dimension (num_subjects x fnum)\n",
    "    \"\"\"\n",
    "\n",
    "    estimator = RidgeClassifier()\n",
    "    selector = RFE(estimator, n_features_to_select=fnum, step=100, verbose=1)\n",
    "\n",
    "    featureX = matrix[train_ind, :]\n",
    "    featureY = labels[train_ind]\n",
    "    selector = selector.fit(featureX, featureY.ravel())\n",
    "    x_data = selector.transform(matrix)\n",
    "\n",
    "    print(\"Number of labeled samples %d\" % len(train_ind))\n",
    "    print(\"Number of features selected %d\" % x_data.shape[1])\n",
    "\n",
    "    return x_data\n",
    "\n",
    "\n",
    "# Make sure each site is represented in the training set when selecting a subset of the training set\n",
    "def site_percentage(train_ind, perc, subject_list):\n",
    "    \"\"\"\n",
    "        train_ind    : indices of the training samples\n",
    "        perc         : percentage of training set used\n",
    "        subject_list : list of subject IDs\n",
    "\n",
    "    return:\n",
    "        labeled_indices      : indices of the subset of training samples\n",
    "    \"\"\"\n",
    "\n",
    "    train_list = subject_list[train_ind]\n",
    "    sites = get_subject_score(train_list, score='SITE_ID')\n",
    "    unique = np.unique(list(sites.values())).tolist()\n",
    "    site = np.array([unique.index(sites[train_list[x]]) for x in range(len(train_list))])\n",
    "\n",
    "    labeled_indices = []\n",
    "\n",
    "    for i in np.unique(site):\n",
    "        id_in_site = np.argwhere(site == i).flatten()\n",
    "\n",
    "        num_nodes = len(id_in_site)\n",
    "        labeled_num = int(round(perc * num_nodes))\n",
    "        labeled_indices.extend(train_ind[id_in_site[:labeled_num]])\n",
    "\n",
    "    return labeled_indices\n",
    "\n",
    "\n",
    "# Load precomputed fMRI connectivity networks\n",
    "def get_networks(subject_list, kind, atlas_name=\"aal\", variable='connectivity'):\n",
    "    \"\"\"\n",
    "        subject_list : list of subject IDs\n",
    "        kind         : the kind of connectivity to be used, e.g. lasso, partial correlation, correlation\n",
    "        atlas_name   : name of the parcellation atlas used\n",
    "        variable     : variable name in the .mat file that has been used to save the precomputed networks\n",
    "\n",
    "\n",
    "    return:\n",
    "        matrix      : feature matrix of connectivity networks (num_subjects x network_size)\n",
    "    \"\"\"\n",
    "\n",
    "    all_networks1 = []\n",
    "    all_networks2 = []\n",
    "    all_networks3 = []\n",
    "    all_networks4 = []\n",
    "    all_networks5 = []\n",
    "    all_networks6 = []\n",
    "    all_networks7 = []\n",
    " \n",
    "    \n",
    "    \n",
    "    \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_ez/matrix_rois_ez_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks1.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_ez/matrix_rois_ez_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks1.append(matrix)\n",
    "            \n",
    "    \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_cc400/matrix_rois_cc400_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks2.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_cc400/matrix_rois_cc400_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks2.append(matrix)\n",
    "            \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_aal/matrix_rois_aal_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks3.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_aal/matrix_rois_aal_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks3.append(matrix)\n",
    "            \n",
    "    \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_ho/matrix_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks4.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_ho/matrix_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks4.append(matrix)\n",
    "            \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_cc200/matrix_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks5.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_cc200/matrix_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks5.append(matrix)\n",
    "            \n",
    "            \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_dosenbach160/matrix_rois_dosenbach160_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks6.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_dosenbach160/matrix_rois_dosenbach160_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks6.append(matrix)\n",
    "            \n",
    "            \n",
    "            \n",
    "    for subject1 in subject_list:\n",
    "        fl = f'../Datasets/all_fc_matrix_rois_tt/matrix_rois_tt_{subject1}.mat'\n",
    "        try:  \n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks7.append(matrix)\n",
    "        except FileNotFoundError:\n",
    "            fl = f'../Datasets/all_fc_matrix_rois_tt/matrix_rois_tt_{50002}.mat'\n",
    "            matrix = sio.loadmat(fl)[variable]\n",
    "            all_networks7.append(matrix)            \n",
    "            \n",
    "            \n",
    "\n",
    "    idx1 = np.triu_indices_from(all_networks1[0], 1)\n",
    "    norm_networks1 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks1]\n",
    "    vec_networks1 = [mat[idx1] for mat in norm_networks1]\n",
    "    matrix1 = np.vstack(vec_networks1)\n",
    "    \n",
    "    idx2 = np.triu_indices_from(all_networks2[0], 1)\n",
    "    norm_networks2 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks2]\n",
    "    vec_networks2 = [mat[idx2] for mat in norm_networks2]\n",
    "    matrix2 = np.vstack(vec_networks2)\n",
    "    \n",
    "    idx3 = np.triu_indices_from(all_networks3[0], 1)\n",
    "    norm_networks3 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks3]\n",
    "    vec_networks3 = [mat[idx3] for mat in norm_networks3]\n",
    "    matrix3 = np.vstack(vec_networks3)\n",
    "    \n",
    "    idx4 = np.triu_indices_from(all_networks4[0], 1)\n",
    "    norm_networks4 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks4]\n",
    "    vec_networks4 = [mat[idx4] for mat in norm_networks4]\n",
    "    matrix4 = np.vstack(vec_networks4)\n",
    "    \n",
    "    idx5 = np.triu_indices_from(all_networks5[0], 1)\n",
    "    norm_networks5 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks5]\n",
    "    vec_networks5 = [mat[idx5] for mat in norm_networks5]\n",
    "    matrix5 = np.vstack(vec_networks5)\n",
    "    \n",
    "    idx6 = np.triu_indices_from(all_networks6[0], 1)\n",
    "    norm_networks6 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks6]\n",
    "    vec_networks6 = [mat[idx6] for mat in norm_networks6]\n",
    "    matrix6 = np.vstack(vec_networks6)\n",
    "    \n",
    "    idx7 = np.triu_indices_from(all_networks7[0], 1)\n",
    "    norm_networks7 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks7]\n",
    "    vec_networks7 = [mat[idx7] for mat in norm_networks7]\n",
    "    matrix7 = np.vstack(vec_networks7)\n",
    "    \n",
    "    matrix_a =np.concatenate((matrix1,matrix2), axis=1)\n",
    "    matrix_b =np.concatenate((matrix3,matrix4), axis=1)\n",
    "    matrix_c =np.concatenate((matrix5,matrix6), axis=1)\n",
    "    \n",
    "    matrix_a_b = np.concatenate((matrix_a,matrix_b), axis=1)\n",
    "    matrix_c_7 = np.concatenate((matrix_c,matrix7), axis=1)\n",
    "    matrix = np.concatenate((matrix_a_b,matrix_c_7), axis=1)                           \n",
    "    \n",
    "    print(len(matrix[0]));\n",
    "    return matrix\n",
    "\n",
    "\n",
    "# Construct the adjacency matrix of the population from phenotypic scores\n",
    "def create_affinity_graph_from_scores(scores, pd_dict):\n",
    "    num_nodes = len(pd_dict[scores[0]]) \n",
    "    graph = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "    for l in scores:\n",
    "        label_dict = pd_dict[l]\n",
    "\n",
    "        if l in ['AGE_AT_SCAN', 'FIQ']:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    try:\n",
    "                        val = abs(float(label_dict[k]) - float(label_dict[j]))\n",
    "                        if val < 2:\n",
    "                            graph[k, j] += 1\n",
    "                            graph[j, k] += 1\n",
    "                    except ValueError:  # missing label\n",
    "                        pass\n",
    "\n",
    "        else:\n",
    "            for k in range(num_nodes):\n",
    "                for j in range(k + 1, num_nodes):\n",
    "                    if label_dict[k] == label_dict[j]:\n",
    "                        graph[k, j] += 1\n",
    "                        graph[j, k] += 1\n",
    "\n",
    "    return graph\n",
    "\n",
    "def get_static_affinity_adj(features, pd_dict):\n",
    "    pd_affinity = create_affinity_graph_from_scores(['SEX', 'SITE_ID'], pd_dict) \n",
    "    distv = distance.pdist(features, metric='correlation') \n",
    "    dist = distance.squareform(distv)  \n",
    "    sigma = np.mean(dist)\n",
    "    feature_sim = np.exp(- dist ** 2 / (2 * sigma ** 2))\n",
    "    adj = pd_affinity * feature_sim  \n",
    "\n",
    "    return adj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "50e2f282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10368\\3119403776.py:8: DeprecationWarning: Please use `eigsh` from the `scipy.sparse.linalg` namespace, the `scipy.sparse.linalg.eigen` namespace is deprecated.\n",
      "  from scipy.sparse.linalg.eigen import eigsh\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.spatial import distance\n",
    "from scipy.sparse.linalg.eigen import eigsh\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "def sample_mask(idx, l):\n",
    "    \"\"\"Create mask.\"\"\"\n",
    "    mask = np.zeros(l)\n",
    "    mask[idx] = 1\n",
    "    return np.array(mask, dtype=np.bool)\n",
    "\n",
    "def get_train_test_masks(labels, idx_train, idx_val, idx_test):\n",
    "    train_mask = sample_mask(idx_train, labels.shape[0])\n",
    "    val_mask = sample_mask(idx_val, labels.shape[0])\n",
    "    test_mask = sample_mask(idx_test, labels.shape[0])\n",
    "\n",
    "    y_train = np.zeros(labels.shape)\n",
    "    y_val = np.zeros(labels.shape)\n",
    "    y_test = np.zeros(labels.shape)\n",
    "    y_train[train_mask, :] = labels[train_mask, :]\n",
    "    y_val[val_mask, :] = labels[val_mask, :]\n",
    "    y_test[test_mask, :] = labels[test_mask, :]\n",
    "\n",
    "    return y_train, y_val, y_test, train_mask, val_mask, test_mask\n",
    "\n",
    "def load_data(subject_IDs, params): \n",
    "    \n",
    "    # labels\n",
    "    num_classes = 2\n",
    "    num_nodes = len(subject_IDs)\n",
    "    \n",
    "    # 初始化y_data(), y\n",
    "    y_data = np.zeros([num_nodes, num_classes])\n",
    "    y = np.zeros([num_nodes, 1])\n",
    "    \n",
    "    labels = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "    features = get_networks(subject_IDs, kind=params['connectivity'], atlas_name=params['atlas'])\n",
    "    \n",
    "    for i in range(num_nodes):\n",
    "        y_data[i, int(labels[subject_IDs[i]]) - 1] = 1 # (871,2)\n",
    "        y[i] = int(labels[subject_IDs[i]]) # (871,)\n",
    "        \n",
    "    skf = StratifiedKFold(n_splits=10)\n",
    "    cv_splits = list(skf.split(features, np.squeeze(y)))\n",
    "    train = cv_splits[params['folds']][0]\n",
    "    test = cv_splits[params['folds']][1]\n",
    "    val = test\n",
    "    \n",
    "    print('Number of train sample:{}' .format(len(train)))\n",
    "        \n",
    "    y_train, y_val, y_test, train_mask, val_mask, test_mask = get_train_test_masks(y_data, train, val, test)\n",
    "    \n",
    "    y_data = torch.LongTensor(np.where(y_data)[1])\n",
    "    y = torch.LongTensor(y)\n",
    "    y_train = torch.LongTensor(y_train[1])\n",
    "    y_val = torch.LongTensor(y_val[1])\n",
    "    y_test = torch.LongTensor(y_test[1])\n",
    "    \n",
    "    train = torch.LongTensor(train)\n",
    "    val = torch.LongTensor(val)\n",
    "    test = torch.LongTensor(test)\n",
    "    train_mask = torch.LongTensor(train_mask)\n",
    "    val_mask = torch.LongTensor(val_mask)\n",
    "    test_mask = torch.LongTensor(test_mask)\n",
    "    \n",
    "    # Eigenvector\n",
    "    labeled_ind = site_percentage(train, params['num_training'], subject_IDs)\n",
    "    x_data = feature_selection(features, y, labeled_ind, params['num_features'])\n",
    "    features = preprocess_features(sp.coo_matrix(x_data).tolil())\n",
    "    features = torch.FloatTensor(np.array(features.todense()))\n",
    "    \n",
    "    # Adjacency matrix\n",
    "    graph = create_affinity_graph_from_scores(['SEX', 'SITE_ID'], subject_IDs)\n",
    "    distv = distance.pdist(x_data, metric='correlation')\n",
    "    dist = distance.squareform(distv)\n",
    "    sigma = np.mean(dist)\n",
    "    sparse_graph = np.exp(- dist ** 2 / (2 * sigma ** 2))\n",
    "    final_graph = graph * sparse_graph\n",
    "\n",
    "    return final_graph, features, y, y_data, y_train, y_val, y_test, train, val, test, train_mask, val_mask, test_mask\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "def sparse_to_tuple(sparse_mx):\n",
    "    \"\"\"Convert sparse matrix to tuple representation.\"\"\"\n",
    "    def to_tuple(mx):\n",
    "        if not sp.isspmatrix_coo(mx):\n",
    "            mx = mx.tocoo()\n",
    "        coords = np.vstack((mx.row, mx.col)).transpose()\n",
    "        values = mx.data\n",
    "        shape = mx.shape\n",
    "        coords = torch.from_numpy(coords)\n",
    "        values = torch.from_numpy(values)\n",
    "        shape = torch.tensor(shape)\n",
    "        return coords, values, shape\n",
    "\n",
    "    if isinstance(sparse_mx, list):\n",
    "        for i in range(len(sparse_mx)):\n",
    "            sparse_mx[i] = to_tuple(sparse_mx[i])\n",
    "    else:\n",
    "        sparse_mx = to_tuple(sparse_mx)\n",
    "\n",
    "    return sparse_mx\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)\n",
    "\n",
    "\n",
    "def preprocess_features(features):\n",
    "    \"\"\"Row-normalize feature matrix\"\"\"\n",
    "    rowsum = np.array(features.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    features = r_mat_inv.dot(features)\n",
    "    return features\n",
    "\n",
    "def normalize_adj(adj):\n",
    "    \"\"\"Symmetrically normalize adjacency matrix.\"\"\"\n",
    "    adj = sp.coo_matrix(adj)\n",
    "    rowsum = np.array(adj.sum(1))\n",
    "    d_inv_sqrt = np.power(rowsum, -0.5).flatten()\n",
    "    d_inv_sqrt[np.isinf(d_inv_sqrt)] = 0.\n",
    "    d_mat_inv_sqrt = sp.diags(d_inv_sqrt)\n",
    "    return adj.dot(d_mat_inv_sqrt).transpose().dot(d_mat_inv_sqrt).tocoo()\n",
    "\n",
    "def preprocess_adj(adj):\n",
    "    \"\"\"Preprocessing of adjacency matrix for simple GCN model.\"\"\"\n",
    "    adj_normalized = normalize_adj(adj + sp.eye(adj.shape[0]))\n",
    "    return adj_normalized\n",
    "\n",
    "def chebyshev_polynomials(adj, k):\n",
    "    \"\"\"Calculate Chebyshev polynomials up to order k. Return a list of sparse matrices (tuple representation).\"\"\"\n",
    "    print(\"Calculating Chebyshev polynomials up to order {}...\".format(k))\n",
    "\n",
    "    adj_normalized = normalize_adj(adj)\n",
    "    laplacian = sp.eye(adj.shape[0]) - adj_normalized\n",
    "    largest_eigval, _ = eigsh(laplacian, 1, which='LM')\n",
    "    scaled_laplacian = (2. / largest_eigval[0]) * laplacian - sp.eye(adj.shape[0])\n",
    "\n",
    "    t_k = list()\n",
    "    t_k.append(sp.eye(adj.shape[0]))\n",
    "    t_k.append(scaled_laplacian)\n",
    "\n",
    "    def chebyshev_recurrence(t_k_minus_one, t_k_minus_two, scaled_lap):\n",
    "        s_lap = sp.csr_matrix(scaled_lap, copy=True)\n",
    "        return 2 * s_lap.dot(t_k_minus_one) - t_k_minus_two\n",
    "\n",
    "    for i in range(2, k+1):\n",
    "        t_k.append(chebyshev_recurrence(t_k[-1], t_k[-2], scaled_laplacian))\n",
    "\n",
    "    return t_k\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7980a930",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "# from utils import preprocess_features\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "class dataloader():\n",
    "    def __init__(self): \n",
    "        self.pd_dict = {}\n",
    "        self.node_ftr_dim = 2000\n",
    "        self.num_classes = 2 \n",
    "\n",
    "    def load_data(self, params, connectivity='correlation', atlas='ho'):\n",
    "        ''' load multimodal data from ABIDE\n",
    "        return: imaging features (raw), labels, non-image data\n",
    "        '''\n",
    "        subject_IDs = get_ids()\n",
    "        labels = get_subject_score(subject_IDs, score='DX_GROUP')\n",
    "        num_nodes = len(subject_IDs)\n",
    "\n",
    "        sites = get_subject_score(subject_IDs, score='SITE_ID')\n",
    "        unique = np.unique(list(sites.values())).tolist()\n",
    "        ages = get_subject_score(subject_IDs, score='AGE_AT_SCAN')\n",
    "        genders = get_subject_score(subject_IDs, score='SEX') \n",
    "\n",
    "        y_onehot = np.zeros([num_nodes, self.num_classes])\n",
    "        y = np.zeros([num_nodes])\n",
    "        site = np.zeros([num_nodes], dtype=int)\n",
    "        age = np.zeros([num_nodes], dtype=np.float32)\n",
    "        gender = np.zeros([num_nodes], dtype=int)\n",
    "        for i in range(num_nodes):\n",
    "            y_onehot[i, int(labels[subject_IDs[i]])-1] = 1\n",
    "            y[i] = int(labels[subject_IDs[i]])\n",
    "            site[i] = unique.index(sites[subject_IDs[i]])\n",
    "            age[i] = float(ages[subject_IDs[i]])\n",
    "            gender[i] = genders[subject_IDs[i]]\n",
    "        \n",
    "        self.y = y -1  \n",
    "\n",
    "        self.raw_features = get_networks(subject_IDs, kind=connectivity, atlas_name=atlas)\n",
    "\n",
    "        phonetic_data = np.zeros([num_nodes, 3], dtype=np.float32)\n",
    "        phonetic_data[:,0] = site \n",
    "        phonetic_data[:,1] = gender \n",
    "        phonetic_data[:,2] = age \n",
    "\n",
    "        self.pd_dict['SITE_ID'] = np.copy(phonetic_data[:,0])\n",
    "        self.pd_dict['SEX'] = np.copy(phonetic_data[:,1])\n",
    "        self.pd_dict['AGE_AT_SCAN'] = np.copy(phonetic_data[:,2]) \n",
    "        \n",
    "        return self.raw_features, self.y, phonetic_data\n",
    "\n",
    "    def data_split(self, n_folds):\n",
    "        # split data by k-fold CV\n",
    "        skf = StratifiedKFold(n_splits=n_folds)\n",
    "        cv_splits = list(skf.split(self.raw_features, self.y))\n",
    "        return cv_splits \n",
    "\n",
    "    def get_node_features(self, train_ind):\n",
    "        '''preprocess node features for wl-deepgcn\n",
    "        '''\n",
    "        node_ftr = feature_selection(self.raw_features, self.y, train_ind, self.node_ftr_dim)\n",
    "        self.node_ftr = preprocess_features(node_ftr) \n",
    "        return self.node_ftr\n",
    "\n",
    "    def get_WL_inputs(self, nonimg):\n",
    "        '''get WL inputs for wl-deepgcn \n",
    "        '''\n",
    "        # construct edge network inputs \n",
    "        n = self.node_ftr.shape[0] \n",
    "        num_edge = n*(1+n)//2 - n  # n*(n-1)//2,HO=6105\n",
    "        pd_ftr_dim = nonimg.shape[1]\n",
    "        edge_index = np.zeros([2, num_edge], dtype=np.int64) \n",
    "        edgenet_input = np.zeros([num_edge, 2*pd_ftr_dim], dtype=np.float32)  \n",
    "        aff_score = np.zeros(num_edge, dtype=np.float32)\n",
    "        # static affinity score used to pre-prune edges \n",
    "        aff_adj = get_static_affinity_adj(self.node_ftr, self.pd_dict)  \n",
    "        flatten_ind = 0 \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                edge_index[:,flatten_ind] = [i,j]\n",
    "                edgenet_input[flatten_ind]  = np.concatenate((nonimg[i], nonimg[j]))\n",
    "                aff_score[flatten_ind] = aff_adj[i][j]  \n",
    "                flatten_ind +=1\n",
    "\n",
    "        assert flatten_ind == num_edge, \"Error in computing edge input\"\n",
    "        \n",
    "        keep_ind = np.where(aff_score > 1.1)[0]  \n",
    "        edge_index = edge_index[:, keep_ind]\n",
    "        edgenet_input = edgenet_input[keep_ind]\n",
    "\n",
    "        return edge_index, edgenet_input\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abc69f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from torch.nn import Linear as Lin, Sequential as Seq\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "class WL(torch.nn.Module):\n",
    "    def __init__(self, input_dim, dropout=0.3):\n",
    "        super(WL, self).__init__()\n",
    "        h1=256\n",
    "        h2=128\n",
    "        self.parser =nn.Sequential(\n",
    "                nn.Linear(input_dim, h1, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.BatchNorm1d(h1),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(h1, h2, bias=True),\n",
    "                nn.LeakyReLU(inplace=True),\n",
    "                nn.BatchNorm1d(h2),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(h2, h2, bias=True),\n",
    "                )\n",
    "        self.cos = nn.CosineSimilarity(dim=1, eps=1e-8)\n",
    "        self.input_dim = input_dim\n",
    "        self.model_init()\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.elu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:,0:self.input_dim]\n",
    "        x2 = x[:,self.input_dim:]\n",
    "        h1 = self.parser(x1) \n",
    "        h2 = self.parser(x2) \n",
    "        p = (self.cos(h1,h2) + 1)*0.5\n",
    "        return p\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Lin):\n",
    "                torch.nn.init.kaiming_normal_(m.weight)\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6db8243e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.nn import Linear as Lin, Sequential as Seq\n",
    "import torch_geometric as tg\n",
    "\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, nhid):\n",
    "        super(MLP,self).__init__()\n",
    "        self.cls = nn.Sequential(\n",
    "            torch.nn.Linear(input_dim,nhid))\n",
    "        \n",
    "    def forward(self, features):\n",
    "        output = self.cls(features)\n",
    "        return output\n",
    "            \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, input_dim, nhid, num_classes, ngl, dropout, edge_dropout, edgenet_input_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        K=3   \n",
    "        hidden = [nhid for i in range(ngl)] \n",
    "        self.dropout = dropout\n",
    "        self.edge_dropout = edge_dropout \n",
    "        bias = False \n",
    "        self.relu = torch.nn.ReLU(inplace=True) \n",
    "        self.ngl = ngl \n",
    "        self.gconv = nn.ModuleList()\n",
    "        for i in range(ngl):\n",
    "            in_channels = input_dim if i==0  else hidden[i-1]\n",
    "            self.gconv.append(tg.nn.ChebConv(in_channels, hidden[i], K, normalization='sym', bias=bias)) \n",
    "          \n",
    "        self.cls = nn.Sequential(\n",
    "                torch.nn.Linear(32, 128),\n",
    "                torch.nn.ReLU(inplace=True),\n",
    "                nn.BatchNorm1d(128), \n",
    "                torch.nn.Linear(128, num_classes))\n",
    "\n",
    "        self.edge_net = WL(input_dim=edgenet_input_dim//2, dropout=dropout)\n",
    "        self.model_init()\n",
    "\n",
    "    def model_init(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Lin):\n",
    "                torch.nn.init.kaiming_normal_(m.weight) # He init\n",
    "                m.weight.requires_grad = True\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.zero_()\n",
    "                    m.bias.requires_grad = True\n",
    "\n",
    "    def forward(self, features, edge_index, edgenet_input, enforce_edropout=False): \n",
    "        if self.edge_dropout>0:\n",
    "            if enforce_edropout or self.training:\n",
    "                one_mask = torch.ones([edgenet_input.shape[0],1])\n",
    "                self.drop_mask = F.dropout(one_mask, self.edge_dropout, True)\n",
    "                self.bool_mask = torch.squeeze(self.drop_mask.type(torch.bool))\n",
    "                edge_index = edge_index[:, self.bool_mask] \n",
    "                edgenet_input = edgenet_input[self.bool_mask] # Weights\n",
    "            \n",
    "        edge_weight = torch.squeeze(self.edge_net(edgenet_input))\n",
    "        \n",
    "\n",
    "        # GCN residual connection\n",
    "        # input layer\n",
    "        features = F.dropout(features, self.dropout, self.training)\n",
    "        x = self.relu(self.gconv[0](features, edge_index, edge_weight)) \n",
    "        x_temp = x\n",
    "        \n",
    "        # hidden layers\n",
    "        for i in range(1, self.ngl - 1): # self.ngl→7\n",
    "            x = F.dropout(x_temp, self.dropout, self.training)\n",
    "            x = self.relu(self.gconv[i](x, edge_index, edge_weight)) \n",
    "            x_temp = x_temp + x # ([871,64])\n",
    "\n",
    "        # output layer\n",
    "        x = F.dropout(x_temp, self.dropout, self.training)\n",
    "        x = self.relu(self.gconv[self.ngl - 1](x, edge_index, edge_weight))\n",
    "        x_temp = x_temp + x\n",
    "\n",
    "        output = x # Final output is not cumulative\n",
    "        output = self.cls(output) \n",
    "        \n",
    "        return output, edge_weight\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "165d107b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5856d199",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchmetrics\n",
    "from torchmetrics.classification import MulticlassSpecificity\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "def torchmetrics_accuracy(preds, labels):\n",
    "    acc = torchmetrics.functional.accuracy(preds, labels,task=\"multiclass\", num_classes=2)\n",
    "    return acc\n",
    "\n",
    "def torchmetrics_spef(preds, labels):\n",
    "    metric = MulticlassSpecificity(num_classes=2)\n",
    "    spef = metric(preds, labels)\n",
    "    return spef\n",
    "\n",
    "def torchmetrics_auc(preds, labels):\n",
    "    auc = torchmetrics.functional.auroc(preds, labels, task=\"multiclass\", num_classes=2)\n",
    "    return auc\n",
    "\n",
    "def confusion_matrix(preds, labels):\n",
    "    conf_matrix = torch.zeros(2, 2)\n",
    "    preds = torch.argmax(preds, 1)\n",
    "    for p, t in zip(preds, labels):\n",
    "        conf_matrix[t, p] += 1 \n",
    "    return conf_matrix\n",
    "def plot_confusion_matrix(cm, normalize=False, title='Confusion matrix', cmap=plt.cm.Oranges):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    Input\n",
    "    - cm : computer the value of confusion matrix\n",
    "    - normalize : True: %, False: 123\n",
    "    \"\"\"\n",
    "    classes = ['0:ASD','1:TC']\n",
    "    if normalize:\n",
    "        cm = cm.numpy()\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    fmt = '.2f' if normalize else '.0f'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "def correct_num(preds, labels):\n",
    "    \"\"\"Accuracy, auc with masking.Acc of the masked samples\"\"\"\n",
    "    correct_prediction = np.equal(np.argmax(preds, 1), labels).astype(np.float32)\n",
    "    return np.sum(correct_prediction)\n",
    "\n",
    "def prf(preds, labels, is_logit=True):\n",
    "    ''' input: logits, labels  ''' \n",
    "    pred_lab= np.argmax(preds, 1)\n",
    "    p,r,f,s  = precision_recall_fscore_support(labels, pred_lab, average='binary')\n",
    "    return [p,r,f]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae068d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "parser = argparse.ArgumentParser()\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba9682d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters:\n",
      "no_cuda: False\n",
      "seed: 46\n",
      "epochs: 200\n",
      "lr: 0.001\n",
      "weight_decay: 5e-05\n",
      "hidden: 32\n",
      "dropout: 0.2\n",
      "atlas: ez\n",
      "num_features: 2000\n",
      "folds: 10\n",
      "connectivity: correlation\n",
      "max_degree: 3\n",
      "ngl: 8\n",
      "edropout: 0.3\n",
      "train: 1\n",
      "ckpt_path: ../folds/rois_seven_32_pth\n",
      "early_stopping: True\n",
      "early_stopping_patience: 20\n",
      "cuda: False\n",
      "  Loading dataset ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10368\\339213281.py:290: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks1 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks1]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10368\\339213281.py:295: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks2 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks2]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10368\\339213281.py:300: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks3 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks3]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10368\\339213281.py:305: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks4 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks4]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10368\\339213281.py:310: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks5 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks5]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10368\\339213281.py:315: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks6 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks6]\n",
      "C:\\Users\\KSB\\AppData\\Local\\Temp\\ipykernel_10368\\339213281.py:320: RuntimeWarning: divide by zero encountered in arctanh\n",
      "  norm_networks7 = [np.arctanh(mat) if not np.all(np.abs(mat) == 1) else mat for mat in all_networks7]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "131503\n",
      "Size of the 1-fold Training, Validation, and Test Sets:900,100,112\n",
      " Starting the 1-1 Fold:：\n",
      "Fitting estimator with 131503 features.\n",
      "Fitting estimator with 121503 features.\n",
      "Fitting estimator with 111503 features.\n",
      "Fitting estimator with 101503 features.\n",
      "Fitting estimator with 91503 features.\n",
      "Fitting estimator with 81503 features.\n",
      "Fitting estimator with 71503 features.\n",
      "Fitting estimator with 61503 features.\n",
      "Fitting estimator with 51503 features.\n",
      "Fitting estimator with 41503 features.\n",
      "Fitting estimator with 31503 features.\n",
      "Fitting estimator with 21503 features.\n",
      "Fitting estimator with 11503 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.4056 pre_train:0.4384 recall_train:0.5355 F1_train:0.4821 AUC_train:0.3688\n",
      "acc_val:0.4000 pre_val:0.2500 recall_val:0.1000 F1_val:0.142857 AUC_val:0.3220\n",
      "Epoch:0002\n",
      "acc_train:0.4400 pre_train:0.4690 recall_train:0.6344 F1_train:0.5393 AUC_train:0.4025\n",
      "acc_val:0.4800 pre_val:0.3333 recall_val:0.0400 F1_val:0.071429 AUC_val:0.3728\n",
      "Epoch:0003\n",
      "acc_train:0.5222 pre_train:0.5296 recall_train:0.6731 F1_train:0.5928 AUC_train:0.5238\n",
      "acc_val:0.5400 pre_val:0.8333 recall_val:0.1000 F1_val:0.178571 AUC_val:0.4520\n",
      "Epoch:0004\n",
      "acc_train:0.5267 pre_train:0.5373 recall_train:0.6043 F1_train:0.5688 AUC_train:0.5298\n",
      "acc_val:0.5500 pre_val:0.6667 recall_val:0.2000 F1_val:0.307692 AUC_val:0.5132\n",
      "Epoch:0005\n",
      "acc_train:0.5300 pre_train:0.5393 recall_train:0.6194 F1_train:0.5766 AUC_train:0.5467\n",
      "acc_val:0.6200 pre_val:0.7727 recall_val:0.3400 F1_val:0.472222 AUC_val:0.5904\n",
      "Epoch:0006\n",
      "acc_train:0.5544 pre_train:0.5672 recall_train:0.5806 F1_train:0.5739 AUC_train:0.5714\n",
      "acc_val:0.6300 pre_val:0.7407 recall_val:0.4000 F1_val:0.519481 AUC_val:0.6132\n",
      "Epoch:0007\n",
      "acc_train:0.5856 pre_train:0.6009 recall_train:0.5892 F1_train:0.5950 AUC_train:0.6024\n",
      "acc_val:0.6300 pre_val:0.7407 recall_val:0.4000 F1_val:0.519481 AUC_val:0.6048\n",
      "Epoch:0008\n",
      "acc_train:0.5667 pre_train:0.5817 recall_train:0.5742 F1_train:0.5779 AUC_train:0.5844\n",
      "acc_val:0.6200 pre_val:0.7143 recall_val:0.4000 F1_val:0.512821 AUC_val:0.6060\n",
      "Epoch:0009\n",
      "acc_train:0.5833 pre_train:0.5941 recall_train:0.6108 F1_train:0.6023 AUC_train:0.6076\n",
      "acc_val:0.6300 pre_val:0.7241 recall_val:0.4200 F1_val:0.531646 AUC_val:0.6160\n",
      "Epoch:0010\n",
      "acc_train:0.6011 pre_train:0.6210 recall_train:0.5849 F1_train:0.6024 AUC_train:0.6465\n",
      "acc_val:0.6400 pre_val:0.7333 recall_val:0.4400 F1_val:0.550000 AUC_val:0.6460\n",
      "Epoch:0011\n",
      "acc_train:0.5911 pre_train:0.6115 recall_train:0.5720 F1_train:0.5911 AUC_train:0.6089\n",
      "acc_val:0.6500 pre_val:0.7419 recall_val:0.4600 F1_val:0.567901 AUC_val:0.6488\n",
      "Epoch:0012\n",
      "acc_train:0.6044 pre_train:0.6236 recall_train:0.5914 F1_train:0.6071 AUC_train:0.6312\n",
      "acc_val:0.6400 pre_val:0.7188 recall_val:0.4600 F1_val:0.560976 AUC_val:0.6580\n",
      "Epoch:0013\n",
      "acc_train:0.5978 pre_train:0.6178 recall_train:0.5806 F1_train:0.5987 AUC_train:0.6373\n",
      "acc_val:0.6400 pre_val:0.7188 recall_val:0.4600 F1_val:0.560976 AUC_val:0.6792\n",
      "Epoch:0014\n",
      "acc_train:0.5967 pre_train:0.6114 recall_train:0.6022 F1_train:0.6067 AUC_train:0.6219\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.6924\n",
      "Epoch:0015\n",
      "acc_train:0.6122 pre_train:0.6324 recall_train:0.5957 F1_train:0.6135 AUC_train:0.6376\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.7012\n",
      "Epoch:0016\n",
      "acc_train:0.6100 pre_train:0.6228 recall_train:0.6215 F1_train:0.6222 AUC_train:0.6464\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.7080\n",
      "Epoch:0017\n",
      "acc_train:0.6300 pre_train:0.6493 recall_train:0.6172 F1_train:0.6329 AUC_train:0.6626\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.7108\n",
      "Epoch:0018\n",
      "acc_train:0.5989 pre_train:0.6268 recall_train:0.5527 F1_train:0.5874 AUC_train:0.6296\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7120\n",
      "Epoch:0019\n",
      "acc_train:0.5978 pre_train:0.6241 recall_train:0.5570 F1_train:0.5886 AUC_train:0.6423\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7132\n",
      "Epoch:0020\n",
      "acc_train:0.6056 pre_train:0.6185 recall_train:0.6172 F1_train:0.6179 AUC_train:0.6555\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.7148\n",
      "Epoch:0021\n",
      "acc_train:0.6089 pre_train:0.6253 recall_train:0.6065 F1_train:0.6157 AUC_train:0.6311\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7188\n",
      "Epoch:0022\n",
      "acc_train:0.5933 pre_train:0.6051 recall_train:0.6129 F1_train:0.6090 AUC_train:0.6432\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7212\n",
      "Epoch:0023\n",
      "acc_train:0.6178 pre_train:0.6307 recall_train:0.6280 F1_train:0.6293 AUC_train:0.6695\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.7184\n",
      "Epoch:0024\n",
      "acc_train:0.6300 pre_train:0.6447 recall_train:0.6323 F1_train:0.6384 AUC_train:0.6815\n",
      "acc_val:0.6300 pre_val:0.6585 recall_val:0.5400 F1_val:0.593407 AUC_val:0.7188\n",
      "Epoch:0025\n",
      "acc_train:0.6556 pre_train:0.6638 recall_train:0.6753 F1_train:0.6695 AUC_train:0.6950\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.7196\n",
      "Epoch:0026\n",
      "acc_train:0.6278 pre_train:0.6413 recall_train:0.6344 F1_train:0.6378 AUC_train:0.6859\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.7232\n",
      "Epoch:0027\n",
      "acc_train:0.6267 pre_train:0.6293 recall_train:0.6753 F1_train:0.6515 AUC_train:0.6710\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.7240\n",
      "Epoch:0028\n",
      "acc_train:0.6167 pre_train:0.6339 recall_train:0.6108 F1_train:0.6221 AUC_train:0.6642\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.7244\n",
      "Epoch:0029\n",
      "acc_train:0.6422 pre_train:0.6558 recall_train:0.6473 F1_train:0.6515 AUC_train:0.6829\n",
      "acc_val:0.6600 pre_val:0.7000 recall_val:0.5600 F1_val:0.622222 AUC_val:0.7256\n",
      "Epoch:0030\n",
      "acc_train:0.6311 pre_train:0.6436 recall_train:0.6409 F1_train:0.6422 AUC_train:0.6825\n",
      "acc_val:0.6600 pre_val:0.7000 recall_val:0.5600 F1_val:0.622222 AUC_val:0.7280\n",
      "Epoch:0031\n",
      "acc_train:0.6200 pre_train:0.6279 recall_train:0.6495 F1_train:0.6385 AUC_train:0.6703\n",
      "acc_val:0.6600 pre_val:0.7000 recall_val:0.5600 F1_val:0.622222 AUC_val:0.7296\n",
      "Epoch:0032\n",
      "acc_train:0.6367 pre_train:0.6375 recall_train:0.6882 F1_train:0.6618 AUC_train:0.6904\n",
      "acc_val:0.6400 pre_val:0.6842 recall_val:0.5200 F1_val:0.590909 AUC_val:0.7332\n",
      "Epoch:0033\n",
      "acc_train:0.6578 pre_train:0.6674 recall_train:0.6731 F1_train:0.6702 AUC_train:0.7070\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.7340\n",
      "Epoch:0034\n",
      "acc_train:0.6600 pre_train:0.6862 recall_train:0.6301 F1_train:0.6570 AUC_train:0.7201\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.7376\n",
      "Epoch:0035\n",
      "acc_train:0.6444 pre_train:0.6600 recall_train:0.6430 F1_train:0.6514 AUC_train:0.7118\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7500\n",
      "Epoch:0036\n",
      "acc_train:0.6689 pre_train:0.6851 recall_train:0.6645 F1_train:0.6747 AUC_train:0.7335\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7656\n",
      "Epoch:0037\n",
      "acc_train:0.6622 pre_train:0.6633 recall_train:0.7032 F1_train:0.6827 AUC_train:0.7292\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.7648\n",
      "Epoch:0038\n",
      "acc_train:0.6767 pre_train:0.6843 recall_train:0.6946 F1_train:0.6894 AUC_train:0.7453\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.7760\n",
      "Epoch:0039\n",
      "acc_train:0.6767 pre_train:0.6859 recall_train:0.6903 F1_train:0.6881 AUC_train:0.7258\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.7816\n",
      "Epoch:0040\n",
      "acc_train:0.6667 pre_train:0.6715 recall_train:0.6946 F1_train:0.6829 AUC_train:0.7393\n",
      "acc_val:0.6900 pre_val:0.7568 recall_val:0.5600 F1_val:0.643678 AUC_val:0.7876\n",
      "Epoch:0041\n",
      "acc_train:0.7000 pre_train:0.7273 recall_train:0.6710 F1_train:0.6980 AUC_train:0.7762\n",
      "acc_val:0.7000 pre_val:0.7632 recall_val:0.5800 F1_val:0.659091 AUC_val:0.7896\n",
      "Epoch:0042\n",
      "acc_train:0.7144 pre_train:0.7114 recall_train:0.7527 F1_train:0.7315 AUC_train:0.7768\n",
      "acc_val:0.7300 pre_val:0.7805 recall_val:0.6400 F1_val:0.703297 AUC_val:0.8004\n",
      "Epoch:0043\n",
      "acc_train:0.6889 pre_train:0.7166 recall_train:0.6581 F1_train:0.6861 AUC_train:0.7747\n",
      "acc_val:0.7700 pre_val:0.8000 recall_val:0.7200 F1_val:0.757895 AUC_val:0.8120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0044\n",
      "acc_train:0.7311 pre_train:0.7328 recall_train:0.7548 F1_train:0.7436 AUC_train:0.7948\n",
      "acc_val:0.7900 pre_val:0.7959 recall_val:0.7800 F1_val:0.787879 AUC_val:0.8208\n",
      "Epoch:0045\n",
      "acc_train:0.7522 pre_train:0.7531 recall_train:0.7742 F1_train:0.7635 AUC_train:0.8075\n",
      "acc_val:0.7500 pre_val:0.7119 recall_val:0.8400 F1_val:0.770642 AUC_val:0.8296\n",
      "Epoch:0046\n",
      "acc_train:0.7822 pre_train:0.7762 recall_train:0.8129 F1_train:0.7941 AUC_train:0.8348\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.8344\n",
      "Epoch:0047\n",
      "acc_train:0.8033 pre_train:0.7727 recall_train:0.8774 F1_train:0.8218 AUC_train:0.8432\n",
      "acc_val:0.7300 pre_val:0.6620 recall_val:0.9400 F1_val:0.776860 AUC_val:0.8436\n",
      "Epoch:0048\n",
      "acc_train:0.8111 pre_train:0.7932 recall_train:0.8581 F1_train:0.8244 AUC_train:0.8572\n",
      "acc_val:0.7300 pre_val:0.6620 recall_val:0.9400 F1_val:0.776860 AUC_val:0.8616\n",
      "Epoch:0049\n",
      "acc_train:0.8256 pre_train:0.7939 recall_train:0.8946 F1_train:0.8413 AUC_train:0.8875\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.8688\n",
      "Epoch:0050\n",
      "acc_train:0.8411 pre_train:0.8220 recall_train:0.8839 F1_train:0.8518 AUC_train:0.8893\n",
      "acc_val:0.7900 pre_val:0.7302 recall_val:0.9200 F1_val:0.814159 AUC_val:0.8680\n",
      "Epoch:0051\n",
      "acc_train:0.8300 pre_train:0.8000 recall_train:0.8946 F1_train:0.8447 AUC_train:0.8877\n",
      "acc_val:0.7900 pre_val:0.7302 recall_val:0.9200 F1_val:0.814159 AUC_val:0.8752\n",
      "Epoch:0052\n",
      "acc_train:0.8722 pre_train:0.8445 recall_train:0.9226 F1_train:0.8818 AUC_train:0.8948\n",
      "acc_val:0.8000 pre_val:0.7344 recall_val:0.9400 F1_val:0.824561 AUC_val:0.8852\n",
      "Epoch:0053\n",
      "acc_train:0.8322 pre_train:0.7897 recall_train:0.9204 F1_train:0.8500 AUC_train:0.8988\n",
      "acc_val:0.8000 pre_val:0.7500 recall_val:0.9000 F1_val:0.818182 AUC_val:0.8940\n",
      "Epoch:0054\n",
      "acc_train:0.8489 pre_train:0.8170 recall_train:0.9118 F1_train:0.8618 AUC_train:0.9035\n",
      "acc_val:0.8100 pre_val:0.7460 recall_val:0.9400 F1_val:0.831858 AUC_val:0.9048\n",
      "Epoch:0055\n",
      "acc_train:0.8867 pre_train:0.8484 recall_train:0.9505 F1_train:0.8966 AUC_train:0.9287\n",
      "acc_val:0.7600 pre_val:0.6912 recall_val:0.9400 F1_val:0.796610 AUC_val:0.9112\n",
      "Epoch:0056\n",
      "acc_train:0.8800 pre_train:0.8453 recall_train:0.9398 F1_train:0.8900 AUC_train:0.9289\n",
      "acc_val:0.7500 pre_val:0.6812 recall_val:0.9400 F1_val:0.789916 AUC_val:0.9168\n",
      "Epoch:0057\n",
      "acc_train:0.8600 pre_train:0.8304 recall_train:0.9161 F1_train:0.8712 AUC_train:0.9340\n",
      "acc_val:0.7500 pre_val:0.6812 recall_val:0.9400 F1_val:0.789916 AUC_val:0.9196\n",
      "Epoch:0058\n",
      "acc_train:0.8933 pre_train:0.8569 recall_train:0.9527 F1_train:0.9022 AUC_train:0.9408\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9076\n",
      "Epoch:0059\n",
      "acc_train:0.8778 pre_train:0.8460 recall_train:0.9333 F1_train:0.8875 AUC_train:0.9464\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9028\n",
      "Epoch:0060\n",
      "acc_train:0.8956 pre_train:0.8588 recall_train:0.9548 F1_train:0.9043 AUC_train:0.9428\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.9012\n",
      "Epoch:0061\n",
      "acc_train:0.8900 pre_train:0.8602 recall_train:0.9398 F1_train:0.8983 AUC_train:0.9378\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.8992\n",
      "Epoch:0062\n",
      "acc_train:0.8967 pre_train:0.8619 recall_train:0.9527 F1_train:0.9050 AUC_train:0.9486\n",
      "acc_val:0.7700 pre_val:0.6957 recall_val:0.9600 F1_val:0.806723 AUC_val:0.9016\n",
      "Epoch:0063\n",
      "acc_train:0.8989 pre_train:0.8582 recall_train:0.9634 F1_train:0.9078 AUC_train:0.9639\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9088\n",
      "Epoch:0064\n",
      "acc_train:0.9122 pre_train:0.8683 recall_train:0.9785 F1_train:0.9201 AUC_train:0.9578\n",
      "acc_val:0.8600 pre_val:0.8103 recall_val:0.9400 F1_val:0.870370 AUC_val:0.9092\n",
      "Epoch:0065\n",
      "acc_train:0.8967 pre_train:0.8647 recall_train:0.9484 F1_train:0.9046 AUC_train:0.9470\n",
      "acc_val:0.8700 pre_val:0.8246 recall_val:0.9400 F1_val:0.878505 AUC_val:0.9148\n",
      "Epoch:0066\n",
      "acc_train:0.9200 pre_train:0.8861 recall_train:0.9699 F1_train:0.9261 AUC_train:0.9711\n",
      "acc_val:0.8500 pre_val:0.7966 recall_val:0.9400 F1_val:0.862385 AUC_val:0.9252\n",
      "Epoch:0067\n",
      "acc_train:0.8978 pre_train:0.8679 recall_train:0.9462 F1_train:0.9053 AUC_train:0.9561\n",
      "acc_val:0.8200 pre_val:0.7581 recall_val:0.9400 F1_val:0.839286 AUC_val:0.9264\n",
      "Epoch:0068\n",
      "acc_train:0.9200 pre_train:0.8876 recall_train:0.9677 F1_train:0.9259 AUC_train:0.9735\n",
      "acc_val:0.8000 pre_val:0.7273 recall_val:0.9600 F1_val:0.827586 AUC_val:0.9284\n",
      "Epoch:0069\n",
      "acc_train:0.9256 pre_train:0.8980 recall_train:0.9656 F1_train:0.9306 AUC_train:0.9662\n",
      "acc_val:0.8400 pre_val:0.7833 recall_val:0.9400 F1_val:0.854545 AUC_val:0.9228\n",
      "Epoch:0070\n",
      "acc_train:0.9133 pre_train:0.8941 recall_train:0.9441 F1_train:0.9184 AUC_train:0.9589\n",
      "acc_val:0.8800 pre_val:0.8519 recall_val:0.9200 F1_val:0.884615 AUC_val:0.9128\n",
      "Epoch:0071\n",
      "acc_train:0.9167 pre_train:0.8900 recall_train:0.9570 F1_train:0.9223 AUC_train:0.9624\n",
      "acc_val:0.8800 pre_val:0.8519 recall_val:0.9200 F1_val:0.884615 AUC_val:0.9080\n",
      "Epoch:0072\n",
      "acc_train:0.9233 pre_train:0.8929 recall_train:0.9677 F1_train:0.9288 AUC_train:0.9645\n",
      "acc_val:0.8800 pre_val:0.8519 recall_val:0.9200 F1_val:0.884615 AUC_val:0.9096\n",
      "Epoch:0073\n",
      "acc_train:0.9211 pre_train:0.8956 recall_train:0.9591 F1_train:0.9263 AUC_train:0.9722\n",
      "acc_val:0.8200 pre_val:0.7667 recall_val:0.9200 F1_val:0.836364 AUC_val:0.9088\n",
      "Epoch:0074\n",
      "acc_train:0.9256 pre_train:0.9012 recall_train:0.9613 F1_train:0.9303 AUC_train:0.9704\n",
      "acc_val:0.8400 pre_val:0.7833 recall_val:0.9400 F1_val:0.854545 AUC_val:0.9096\n",
      "Epoch:0075\n",
      "acc_train:0.9456 pre_train:0.9211 recall_train:0.9785 F1_train:0.9489 AUC_train:0.9832\n",
      "acc_val:0.8500 pre_val:0.7966 recall_val:0.9400 F1_val:0.862385 AUC_val:0.9072\n",
      "Epoch:0076\n",
      "acc_train:0.9256 pre_train:0.8917 recall_train:0.9742 F1_train:0.9311 AUC_train:0.9643\n",
      "acc_val:0.8500 pre_val:0.7966 recall_val:0.9400 F1_val:0.862385 AUC_val:0.9108\n",
      "Epoch:0077\n",
      "acc_train:0.9322 pre_train:0.9056 recall_train:0.9699 F1_train:0.9367 AUC_train:0.9771\n",
      "acc_val:0.8100 pre_val:0.7460 recall_val:0.9400 F1_val:0.831858 AUC_val:0.9156\n",
      "Epoch:0078\n",
      "acc_train:0.9433 pre_train:0.9140 recall_train:0.9828 F1_train:0.9472 AUC_train:0.9780\n",
      "acc_val:0.8000 pre_val:0.7273 recall_val:0.9600 F1_val:0.827586 AUC_val:0.9124\n",
      "Epoch:0079\n",
      "acc_train:0.9478 pre_train:0.9248 recall_train:0.9785 F1_train:0.9509 AUC_train:0.9784\n",
      "acc_val:0.8300 pre_val:0.7619 recall_val:0.9600 F1_val:0.849558 AUC_val:0.9136\n",
      "Epoch:0080\n",
      "acc_train:0.9400 pre_train:0.9118 recall_train:0.9785 F1_train:0.9440 AUC_train:0.9776\n",
      "acc_val:0.8400 pre_val:0.7833 recall_val:0.9400 F1_val:0.854545 AUC_val:0.9148\n",
      "Epoch:0081\n",
      "acc_train:0.9422 pre_train:0.9223 recall_train:0.9699 F1_train:0.9455 AUC_train:0.9772\n",
      "acc_val:0.8400 pre_val:0.7833 recall_val:0.9400 F1_val:0.854545 AUC_val:0.9144\n",
      "Epoch:0082\n",
      "acc_train:0.9456 pre_train:0.9370 recall_train:0.9591 F1_train:0.9479 AUC_train:0.9796\n",
      "acc_val:0.8300 pre_val:0.7705 recall_val:0.9400 F1_val:0.846847 AUC_val:0.9140\n",
      "Epoch:0083\n",
      "acc_train:0.9433 pre_train:0.9207 recall_train:0.9742 F1_train:0.9467 AUC_train:0.9777\n",
      "acc_val:0.8000 pre_val:0.7273 recall_val:0.9600 F1_val:0.827586 AUC_val:0.9138\n",
      "Epoch:0084\n",
      "acc_train:0.9322 pre_train:0.9073 recall_train:0.9677 F1_train:0.9365 AUC_train:0.9735\n",
      "acc_val:0.7700 pre_val:0.6901 recall_val:0.9800 F1_val:0.809917 AUC_val:0.9200\n",
      "Epoch:0085\n",
      "acc_train:0.9544 pre_train:0.9344 recall_train:0.9806 F1_train:0.9570 AUC_train:0.9815\n",
      "acc_val:0.7700 pre_val:0.6901 recall_val:0.9800 F1_val:0.809917 AUC_val:0.9090\n",
      "Epoch:0086\n",
      "acc_train:0.9522 pre_train:0.9378 recall_train:0.9720 F1_train:0.9546 AUC_train:0.9858\n",
      "acc_val:0.7900 pre_val:0.7231 recall_val:0.9400 F1_val:0.817391 AUC_val:0.9086\n",
      "Epoch:0087\n",
      "acc_train:0.9556 pre_train:0.9363 recall_train:0.9806 F1_train:0.9580 AUC_train:0.9802\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9208\n",
      "Epoch:0088\n",
      "acc_train:0.9500 pre_train:0.9357 recall_train:0.9699 F1_train:0.9525 AUC_train:0.9849\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9168\n",
      "Epoch:0089\n",
      "acc_train:0.9556 pre_train:0.9363 recall_train:0.9806 F1_train:0.9580 AUC_train:0.9866\n",
      "acc_val:0.8200 pre_val:0.7667 recall_val:0.9200 F1_val:0.836364 AUC_val:0.9172\n",
      "Epoch:0090\n",
      "acc_train:0.9522 pre_train:0.9254 recall_train:0.9871 F1_train:0.9553 AUC_train:0.9919\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.9144\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0091\n",
      "acc_train:0.9556 pre_train:0.9346 recall_train:0.9828 F1_train:0.9581 AUC_train:0.9938\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9124\n",
      "Epoch:0092\n",
      "acc_train:0.9600 pre_train:0.9574 recall_train:0.9656 F1_train:0.9615 AUC_train:0.9874\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.9078\n",
      "Epoch:0093\n",
      "acc_train:0.9611 pre_train:0.9536 recall_train:0.9720 F1_train:0.9627 AUC_train:0.9896\n",
      "acc_val:0.8400 pre_val:0.8269 recall_val:0.8600 F1_val:0.843137 AUC_val:0.9090\n",
      "Epoch:0094\n",
      "acc_train:0.9556 pre_train:0.9474 recall_train:0.9677 F1_train:0.9574 AUC_train:0.9886\n",
      "acc_val:0.8500 pre_val:0.8431 recall_val:0.8600 F1_val:0.851485 AUC_val:0.9064\n",
      "Epoch:0095\n",
      "acc_train:0.9500 pre_train:0.9430 recall_train:0.9613 F1_train:0.9521 AUC_train:0.9841\n",
      "acc_val:0.8400 pre_val:0.8148 recall_val:0.8800 F1_val:0.846154 AUC_val:0.9060\n",
      "Epoch:0096\n",
      "acc_train:0.9656 pre_train:0.9597 recall_train:0.9742 F1_train:0.9669 AUC_train:0.9880\n",
      "acc_val:0.8100 pre_val:0.7541 recall_val:0.9200 F1_val:0.828829 AUC_val:0.9096\n",
      "Epoch:0097\n",
      "acc_train:0.9567 pre_train:0.9456 recall_train:0.9720 F1_train:0.9586 AUC_train:0.9904\n",
      "acc_val:0.7600 pre_val:0.7031 recall_val:0.9000 F1_val:0.789474 AUC_val:0.9044\n",
      "Epoch:0098\n",
      "acc_train:0.9600 pre_train:0.9593 recall_train:0.9634 F1_train:0.9614 AUC_train:0.9843\n",
      "acc_val:0.7800 pre_val:0.7258 recall_val:0.9000 F1_val:0.803571 AUC_val:0.8986\n",
      "Epoch:0099\n",
      "acc_train:0.9622 pre_train:0.9518 recall_train:0.9763 F1_train:0.9639 AUC_train:0.9884\n",
      "acc_val:0.8100 pre_val:0.7719 recall_val:0.8800 F1_val:0.822430 AUC_val:0.8986\n",
      "Epoch:0100\n",
      "acc_train:0.9600 pre_train:0.9497 recall_train:0.9742 F1_train:0.9618 AUC_train:0.9861\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.9024\n",
      "Epoch:0101\n",
      "acc_train:0.9711 pre_train:0.9621 recall_train:0.9828 F1_train:0.9723 AUC_train:0.9898\n",
      "acc_val:0.8000 pre_val:0.7500 recall_val:0.9000 F1_val:0.818182 AUC_val:0.9036\n",
      "Epoch:0102\n",
      "acc_train:0.9722 pre_train:0.9622 recall_train:0.9849 F1_train:0.9734 AUC_train:0.9829\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9046\n",
      "Epoch:0103\n",
      "acc_train:0.9789 pre_train:0.9685 recall_train:0.9914 F1_train:0.9798 AUC_train:0.9910\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9094\n",
      "Epoch:0104\n",
      "acc_train:0.9711 pre_train:0.9660 recall_train:0.9785 F1_train:0.9722 AUC_train:0.9879\n",
      "acc_val:0.7700 pre_val:0.6957 recall_val:0.9600 F1_val:0.806723 AUC_val:0.9130\n",
      "Epoch:0105\n",
      "acc_train:0.9656 pre_train:0.9559 recall_train:0.9785 F1_train:0.9671 AUC_train:0.9931\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9174\n",
      "Epoch:0106\n",
      "acc_train:0.9644 pre_train:0.9464 recall_train:0.9871 F1_train:0.9663 AUC_train:0.9926\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9170\n",
      "Epoch:0107\n",
      "acc_train:0.9744 pre_train:0.9585 recall_train:0.9935 F1_train:0.9757 AUC_train:0.9937\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9186\n",
      "Epoch:0108\n",
      "acc_train:0.9711 pre_train:0.9526 recall_train:0.9935 F1_train:0.9726 AUC_train:0.9937\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9150\n",
      "Epoch:0109\n",
      "acc_train:0.9800 pre_train:0.9686 recall_train:0.9935 F1_train:0.9809 AUC_train:0.9946\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9082\n",
      "Epoch:0110\n",
      "acc_train:0.9756 pre_train:0.9683 recall_train:0.9849 F1_train:0.9765 AUC_train:0.9941\n",
      "acc_val:0.7900 pre_val:0.7302 recall_val:0.9200 F1_val:0.814159 AUC_val:0.9014\n",
      "Epoch:0111\n",
      "acc_train:0.9733 pre_train:0.9682 recall_train:0.9806 F1_train:0.9744 AUC_train:0.9937\n",
      "acc_val:0.7800 pre_val:0.7258 recall_val:0.9000 F1_val:0.803571 AUC_val:0.8930\n",
      "Epoch:0112\n",
      "acc_train:0.9656 pre_train:0.9578 recall_train:0.9763 F1_train:0.9670 AUC_train:0.9922\n",
      "acc_val:0.7800 pre_val:0.7258 recall_val:0.9000 F1_val:0.803571 AUC_val:0.8882\n",
      "Epoch:0113\n",
      "acc_train:0.9778 pre_train:0.9724 recall_train:0.9849 F1_train:0.9786 AUC_train:0.9954\n",
      "acc_val:0.7800 pre_val:0.7258 recall_val:0.9000 F1_val:0.803571 AUC_val:0.8904\n",
      "Early Stopping!!! epoch：112\n",
      " Starting the 1-2 Fold:：\n",
      "Fitting estimator with 131503 features.\n",
      "Fitting estimator with 121503 features.\n",
      "Fitting estimator with 111503 features.\n",
      "Fitting estimator with 101503 features.\n",
      "Fitting estimator with 91503 features.\n",
      "Fitting estimator with 81503 features.\n",
      "Fitting estimator with 71503 features.\n",
      "Fitting estimator with 61503 features.\n",
      "Fitting estimator with 51503 features.\n",
      "Fitting estimator with 41503 features.\n",
      "Fitting estimator with 31503 features.\n",
      "Fitting estimator with 21503 features.\n",
      "Fitting estimator with 11503 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.5322 pre_train:0.5846 recall_train:0.3269 F1_train:0.4193 AUC_train:0.5470\n",
      "acc_val:0.5200 pre_val:0.5106 recall_val:0.9600 F1_val:0.666667 AUC_val:0.6836\n",
      "Epoch:0002\n",
      "acc_train:0.5733 pre_train:0.6441 recall_train:0.3892 F1_train:0.4853 AUC_train:0.5970\n",
      "acc_val:0.5800 pre_val:0.5526 recall_val:0.8400 F1_val:0.666667 AUC_val:0.6772\n",
      "Epoch:0003\n",
      "acc_train:0.5467 pre_train:0.5817 recall_train:0.4366 F1_train:0.4988 AUC_train:0.5880\n",
      "acc_val:0.6000 pre_val:0.5694 recall_val:0.8200 F1_val:0.672131 AUC_val:0.6776\n",
      "Epoch:0004\n",
      "acc_train:0.5844 pre_train:0.6207 recall_train:0.5032 F1_train:0.5558 AUC_train:0.6155\n",
      "acc_val:0.5900 pre_val:0.5652 recall_val:0.7800 F1_val:0.655462 AUC_val:0.6732\n",
      "Epoch:0005\n",
      "acc_train:0.5844 pre_train:0.6226 recall_train:0.4968 F1_train:0.5526 AUC_train:0.6095\n",
      "acc_val:0.6000 pre_val:0.5781 recall_val:0.7400 F1_val:0.649123 AUC_val:0.6572\n",
      "Epoch:0006\n",
      "acc_train:0.5956 pre_train:0.6376 recall_train:0.5032 F1_train:0.5625 AUC_train:0.6092\n",
      "acc_val:0.6400 pre_val:0.6250 recall_val:0.7000 F1_val:0.660377 AUC_val:0.6580\n",
      "Epoch:0007\n",
      "acc_train:0.6011 pre_train:0.6387 recall_train:0.5247 F1_train:0.5762 AUC_train:0.6228\n",
      "acc_val:0.6500 pre_val:0.6415 recall_val:0.6800 F1_val:0.660194 AUC_val:0.6588\n",
      "Epoch:0008\n",
      "acc_train:0.5944 pre_train:0.6190 recall_train:0.5591 F1_train:0.5876 AUC_train:0.6225\n",
      "acc_val:0.6300 pre_val:0.6226 recall_val:0.6600 F1_val:0.640777 AUC_val:0.6560\n",
      "Epoch:0009\n",
      "acc_train:0.5989 pre_train:0.6226 recall_train:0.5677 F1_train:0.5939 AUC_train:0.6248\n",
      "acc_val:0.6200 pre_val:0.6154 recall_val:0.6400 F1_val:0.627451 AUC_val:0.6680\n",
      "Epoch:0010\n",
      "acc_train:0.6156 pre_train:0.6313 recall_train:0.6151 F1_train:0.6231 AUC_train:0.6675\n",
      "acc_val:0.6300 pre_val:0.6275 recall_val:0.6400 F1_val:0.633663 AUC_val:0.6624\n",
      "Epoch:0011\n",
      "acc_train:0.6156 pre_train:0.6420 recall_train:0.5785 F1_train:0.6086 AUC_train:0.6325\n",
      "acc_val:0.6400 pre_val:0.6400 recall_val:0.6400 F1_val:0.640000 AUC_val:0.6688\n",
      "Epoch:0012\n",
      "acc_train:0.6078 pre_train:0.6353 recall_train:0.5656 F1_train:0.5984 AUC_train:0.6409\n",
      "acc_val:0.6300 pre_val:0.6327 recall_val:0.6200 F1_val:0.626263 AUC_val:0.6736\n",
      "Epoch:0013\n",
      "acc_train:0.6233 pre_train:0.6486 recall_train:0.5914 F1_train:0.6187 AUC_train:0.6426\n",
      "acc_val:0.6300 pre_val:0.6327 recall_val:0.6200 F1_val:0.626263 AUC_val:0.6784\n",
      "Epoch:0014\n",
      "acc_train:0.6300 pre_train:0.6521 recall_train:0.6086 F1_train:0.6296 AUC_train:0.6576\n",
      "acc_val:0.6400 pre_val:0.6458 recall_val:0.6200 F1_val:0.632653 AUC_val:0.6808\n",
      "Epoch:0015\n",
      "acc_train:0.5844 pre_train:0.6036 recall_train:0.5699 F1_train:0.5863 AUC_train:0.6220\n",
      "acc_val:0.6300 pre_val:0.6383 recall_val:0.6000 F1_val:0.618557 AUC_val:0.6820\n",
      "Epoch:0016\n",
      "acc_train:0.5922 pre_train:0.5980 recall_train:0.6430 F1_train:0.6197 AUC_train:0.6298\n",
      "acc_val:0.6400 pre_val:0.6522 recall_val:0.6000 F1_val:0.625000 AUC_val:0.6788\n",
      "Epoch:0017\n",
      "acc_train:0.6133 pre_train:0.6489 recall_train:0.5484 F1_train:0.5944 AUC_train:0.6458\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6804\n",
      "Epoch:0018\n",
      "acc_train:0.6167 pre_train:0.6523 recall_train:0.5527 F1_train:0.5984 AUC_train:0.6479\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6800\n",
      "Epoch:0019\n",
      "acc_train:0.6111 pre_train:0.6340 recall_train:0.5849 F1_train:0.6085 AUC_train:0.6410\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6840\n",
      "Epoch:0020\n",
      "acc_train:0.6178 pre_train:0.6378 recall_train:0.6022 F1_train:0.6195 AUC_train:0.6554\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6848\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0021\n",
      "acc_train:0.5922 pre_train:0.6047 recall_train:0.6086 F1_train:0.6066 AUC_train:0.6394\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6900\n",
      "Epoch:0022\n",
      "acc_train:0.6256 pre_train:0.6517 recall_train:0.5914 F1_train:0.6201 AUC_train:0.6523\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6928\n",
      "Epoch:0023\n",
      "acc_train:0.5967 pre_train:0.6250 recall_train:0.5484 F1_train:0.5842 AUC_train:0.6317\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6940\n",
      "Epoch:0024\n",
      "acc_train:0.6067 pre_train:0.6220 recall_train:0.6086 F1_train:0.6152 AUC_train:0.6304\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6888\n",
      "Epoch:0025\n",
      "acc_train:0.6167 pre_train:0.6449 recall_train:0.5742 F1_train:0.6075 AUC_train:0.6463\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6744\n",
      "Epoch:0026\n",
      "acc_train:0.6211 pre_train:0.6435 recall_train:0.5978 F1_train:0.6198 AUC_train:0.6447\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6732\n",
      "Epoch:0027\n",
      "acc_train:0.5989 pre_train:0.6156 recall_train:0.5957 F1_train:0.6055 AUC_train:0.6363\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6772\n",
      "Epoch:0028\n",
      "acc_train:0.5933 pre_train:0.6088 recall_train:0.5957 F1_train:0.6022 AUC_train:0.6297\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6748\n",
      "Epoch:0029\n",
      "acc_train:0.6100 pre_train:0.6351 recall_train:0.5763 F1_train:0.6043 AUC_train:0.6188\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6736\n",
      "Epoch:0030\n",
      "acc_train:0.6222 pre_train:0.6478 recall_train:0.5892 F1_train:0.6171 AUC_train:0.6541\n",
      "acc_val:0.6300 pre_val:0.6585 recall_val:0.5400 F1_val:0.593407 AUC_val:0.6760\n",
      "Epoch:0031\n",
      "acc_train:0.6200 pre_train:0.6388 recall_train:0.6086 F1_train:0.6233 AUC_train:0.6495\n",
      "acc_val:0.6300 pre_val:0.6585 recall_val:0.5400 F1_val:0.593407 AUC_val:0.6704\n",
      "Epoch:0032\n",
      "acc_train:0.6211 pre_train:0.6490 recall_train:0.5806 F1_train:0.6129 AUC_train:0.6537\n",
      "acc_val:0.6300 pre_val:0.6585 recall_val:0.5400 F1_val:0.593407 AUC_val:0.6696\n",
      "Epoch:0033\n",
      "acc_train:0.6322 pre_train:0.6658 recall_train:0.5785 F1_train:0.6191 AUC_train:0.6718\n",
      "acc_val:0.6400 pre_val:0.6842 recall_val:0.5200 F1_val:0.590909 AUC_val:0.6688\n",
      "Epoch:0034\n",
      "acc_train:0.6300 pre_train:0.6710 recall_train:0.5570 F1_train:0.6087 AUC_train:0.6693\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6672\n",
      "Epoch:0035\n",
      "acc_train:0.6289 pre_train:0.6586 recall_train:0.5849 F1_train:0.6196 AUC_train:0.6678\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6656\n",
      "Epoch:0036\n",
      "acc_train:0.6533 pre_train:0.6987 recall_train:0.5785 F1_train:0.6329 AUC_train:0.6927\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6640\n",
      "Epoch:0037\n",
      "acc_train:0.5833 pre_train:0.5953 recall_train:0.6043 F1_train:0.5998 AUC_train:0.6224\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.6164\n",
      "Epoch:0038\n",
      "acc_train:0.6367 pre_train:0.6806 recall_train:0.5591 F1_train:0.6139 AUC_train:0.6642\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.6156\n",
      "Epoch:0039\n",
      "acc_train:0.6389 pre_train:0.6813 recall_train:0.5656 F1_train:0.6181 AUC_train:0.6567\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.6168\n",
      "Epoch:0040\n",
      "acc_train:0.6344 pre_train:0.6753 recall_train:0.5634 F1_train:0.6143 AUC_train:0.6652\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.6220\n",
      "Epoch:0041\n",
      "acc_train:0.6389 pre_train:0.6842 recall_train:0.5591 F1_train:0.6154 AUC_train:0.6763\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.6264\n",
      "Epoch:0042\n",
      "acc_train:0.6100 pre_train:0.6344 recall_train:0.5785 F1_train:0.6052 AUC_train:0.6579\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.6308\n",
      "Epoch:0043\n",
      "acc_train:0.6089 pre_train:0.6355 recall_train:0.5699 F1_train:0.6009 AUC_train:0.6393\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.6388\n",
      "Epoch:0044\n",
      "acc_train:0.6567 pre_train:0.7063 recall_train:0.5742 F1_train:0.6335 AUC_train:0.6727\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.6440\n",
      "Epoch:0045\n",
      "acc_train:0.6367 pre_train:0.6788 recall_train:0.5634 F1_train:0.6157 AUC_train:0.6548\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6520\n",
      "Epoch:0046\n",
      "acc_train:0.6544 pre_train:0.6974 recall_train:0.5849 F1_train:0.6363 AUC_train:0.6771\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6668\n",
      "Epoch:0047\n",
      "acc_train:0.6367 pre_train:0.6675 recall_train:0.5914 F1_train:0.6271 AUC_train:0.6691\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.6784\n",
      "Epoch:0048\n",
      "acc_train:0.6322 pre_train:0.6603 recall_train:0.5935 F1_train:0.6251 AUC_train:0.6739\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.6940\n",
      "Epoch:0049\n",
      "acc_train:0.6289 pre_train:0.6586 recall_train:0.5849 F1_train:0.6196 AUC_train:0.6686\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.6992\n",
      "Epoch:0050\n",
      "acc_train:0.6422 pre_train:0.6819 recall_train:0.5763 F1_train:0.6247 AUC_train:0.6821\n",
      "acc_val:0.6800 pre_val:0.7368 recall_val:0.5600 F1_val:0.636364 AUC_val:0.6984\n",
      "Epoch:0051\n",
      "acc_train:0.6256 pre_train:0.6509 recall_train:0.5935 F1_train:0.6209 AUC_train:0.6787\n",
      "acc_val:0.6700 pre_val:0.7179 recall_val:0.5600 F1_val:0.629213 AUC_val:0.7112\n",
      "Epoch:0052\n",
      "acc_train:0.6433 pre_train:0.6722 recall_train:0.6043 F1_train:0.6365 AUC_train:0.6880\n",
      "acc_val:0.6700 pre_val:0.7179 recall_val:0.5600 F1_val:0.629213 AUC_val:0.7168\n",
      "Epoch:0053\n",
      "acc_train:0.6278 pre_train:0.6617 recall_train:0.5720 F1_train:0.6136 AUC_train:0.6811\n",
      "acc_val:0.6700 pre_val:0.7179 recall_val:0.5600 F1_val:0.629213 AUC_val:0.7184\n",
      "Epoch:0054\n",
      "acc_train:0.6422 pre_train:0.6748 recall_train:0.5935 F1_train:0.6316 AUC_train:0.6793\n",
      "acc_val:0.6700 pre_val:0.7179 recall_val:0.5600 F1_val:0.629213 AUC_val:0.7236\n",
      "Epoch:0055\n",
      "acc_train:0.6378 pre_train:0.6651 recall_train:0.6022 F1_train:0.6321 AUC_train:0.6713\n",
      "acc_val:0.6700 pre_val:0.7179 recall_val:0.5600 F1_val:0.629213 AUC_val:0.7320\n",
      "Epoch:0056\n",
      "acc_train:0.6433 pre_train:0.6706 recall_train:0.6086 F1_train:0.6381 AUC_train:0.7013\n",
      "acc_val:0.6800 pre_val:0.7368 recall_val:0.5600 F1_val:0.636364 AUC_val:0.7428\n",
      "Epoch:0057\n",
      "acc_train:0.6422 pre_train:0.6748 recall_train:0.5935 F1_train:0.6316 AUC_train:0.7083\n",
      "acc_val:0.6800 pre_val:0.7368 recall_val:0.5600 F1_val:0.636364 AUC_val:0.7512\n",
      "Epoch:0058\n",
      "acc_train:0.6500 pre_train:0.6884 recall_train:0.5892 F1_train:0.6350 AUC_train:0.7029\n",
      "acc_val:0.6900 pre_val:0.7568 recall_val:0.5600 F1_val:0.643678 AUC_val:0.7592\n",
      "Epoch:0059\n",
      "acc_train:0.6578 pre_train:0.7018 recall_train:0.5871 F1_train:0.6393 AUC_train:0.7088\n",
      "acc_val:0.6900 pre_val:0.7568 recall_val:0.5600 F1_val:0.643678 AUC_val:0.7732\n",
      "Epoch:0060\n",
      "acc_train:0.6567 pre_train:0.7010 recall_train:0.5849 F1_train:0.6377 AUC_train:0.7191\n",
      "acc_val:0.6900 pre_val:0.7568 recall_val:0.5600 F1_val:0.643678 AUC_val:0.7808\n",
      "Epoch:0061\n",
      "acc_train:0.6667 pre_train:0.7224 recall_train:0.5763 F1_train:0.6411 AUC_train:0.7271\n",
      "acc_val:0.6900 pre_val:0.7568 recall_val:0.5600 F1_val:0.643678 AUC_val:0.7912\n",
      "Epoch:0062\n",
      "acc_train:0.6422 pre_train:0.6801 recall_train:0.5806 F1_train:0.6265 AUC_train:0.7302\n",
      "acc_val:0.6900 pre_val:0.7568 recall_val:0.5600 F1_val:0.643678 AUC_val:0.8016\n",
      "Epoch:0063\n",
      "acc_train:0.6689 pre_train:0.7103 recall_train:0.6065 F1_train:0.6543 AUC_train:0.7540\n",
      "acc_val:0.6800 pre_val:0.7368 recall_val:0.5600 F1_val:0.636364 AUC_val:0.8096\n",
      "Epoch:0064\n",
      "acc_train:0.6711 pre_train:0.7086 recall_train:0.6172 F1_train:0.6598 AUC_train:0.7662\n",
      "acc_val:0.6900 pre_val:0.7568 recall_val:0.5600 F1_val:0.643678 AUC_val:0.8224\n",
      "Epoch:0065\n",
      "acc_train:0.6678 pre_train:0.7161 recall_train:0.5914 F1_train:0.6478 AUC_train:0.7506\n",
      "acc_val:0.6800 pre_val:0.7368 recall_val:0.5600 F1_val:0.636364 AUC_val:0.8232\n",
      "Epoch:0066\n",
      "acc_train:0.6467 pre_train:0.6682 recall_train:0.6280 F1_train:0.6475 AUC_train:0.7545\n",
      "acc_val:0.7000 pre_val:0.7632 recall_val:0.5800 F1_val:0.659091 AUC_val:0.8268\n",
      "Epoch:0067\n",
      "acc_train:0.6878 pre_train:0.7447 recall_train:0.6022 F1_train:0.6659 AUC_train:0.7783\n",
      "acc_val:0.7100 pre_val:0.7838 recall_val:0.5800 F1_val:0.666667 AUC_val:0.8300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0068\n",
      "acc_train:0.7100 pre_train:0.7656 recall_train:0.6323 F1_train:0.6926 AUC_train:0.7903\n",
      "acc_val:0.7200 pre_val:0.8056 recall_val:0.5800 F1_val:0.674419 AUC_val:0.8376\n",
      "Epoch:0069\n",
      "acc_train:0.6933 pre_train:0.7429 recall_train:0.6215 F1_train:0.6768 AUC_train:0.8001\n",
      "acc_val:0.7200 pre_val:0.8056 recall_val:0.5800 F1_val:0.674419 AUC_val:0.8444\n",
      "Epoch:0070\n",
      "acc_train:0.7078 pre_train:0.7672 recall_train:0.6237 F1_train:0.6880 AUC_train:0.8082\n",
      "acc_val:0.7000 pre_val:0.7632 recall_val:0.5800 F1_val:0.659091 AUC_val:0.8496\n",
      "Epoch:0071\n",
      "acc_train:0.7011 pre_train:0.7513 recall_train:0.6301 F1_train:0.6854 AUC_train:0.8319\n",
      "acc_val:0.7100 pre_val:0.7692 recall_val:0.6000 F1_val:0.674157 AUC_val:0.8500\n",
      "Epoch:0072\n",
      "acc_train:0.7078 pre_train:0.7672 recall_train:0.6237 F1_train:0.6880 AUC_train:0.8270\n",
      "acc_val:0.7300 pre_val:0.7674 recall_val:0.6600 F1_val:0.709677 AUC_val:0.8452\n",
      "Epoch:0073\n",
      "acc_train:0.7244 pre_train:0.8127 recall_train:0.6065 F1_train:0.6946 AUC_train:0.8276\n",
      "acc_val:0.7500 pre_val:0.7660 recall_val:0.7200 F1_val:0.742268 AUC_val:0.8488\n",
      "Epoch:0074\n",
      "acc_train:0.7378 pre_train:0.8300 recall_train:0.6194 F1_train:0.7094 AUC_train:0.8460\n",
      "acc_val:0.7500 pre_val:0.7451 recall_val:0.7600 F1_val:0.752475 AUC_val:0.8444\n",
      "Epoch:0075\n",
      "acc_train:0.7233 pre_train:0.7919 recall_train:0.6301 F1_train:0.7018 AUC_train:0.8464\n",
      "acc_val:0.7600 pre_val:0.7321 recall_val:0.8200 F1_val:0.773585 AUC_val:0.8396\n",
      "Epoch:0076\n",
      "acc_train:0.7367 pre_train:0.7850 recall_train:0.6753 F1_train:0.7260 AUC_train:0.8590\n",
      "acc_val:0.7900 pre_val:0.7458 recall_val:0.8800 F1_val:0.807339 AUC_val:0.8608\n",
      "Epoch:0077\n",
      "acc_train:0.7622 pre_train:0.7770 recall_train:0.7570 F1_train:0.7669 AUC_train:0.8618\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.8940\n",
      "Epoch:0078\n",
      "acc_train:0.7811 pre_train:0.7587 recall_train:0.8452 F1_train:0.7996 AUC_train:0.8854\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9068\n",
      "Epoch:0079\n",
      "acc_train:0.8011 pre_train:0.7629 recall_train:0.8925 F1_train:0.8226 AUC_train:0.9181\n",
      "acc_val:0.8100 pre_val:0.7541 recall_val:0.9200 F1_val:0.828829 AUC_val:0.9092\n",
      "Epoch:0080\n",
      "acc_train:0.8333 pre_train:0.8000 recall_train:0.9032 F1_train:0.8485 AUC_train:0.9256\n",
      "acc_val:0.8300 pre_val:0.8235 recall_val:0.8400 F1_val:0.831683 AUC_val:0.8928\n",
      "Epoch:0081\n",
      "acc_train:0.7967 pre_train:0.7776 recall_train:0.8495 F1_train:0.8119 AUC_train:0.9031\n",
      "acc_val:0.8700 pre_val:0.8364 recall_val:0.9200 F1_val:0.876190 AUC_val:0.9064\n",
      "Epoch:0082\n",
      "acc_train:0.8389 pre_train:0.8113 recall_train:0.8968 F1_train:0.8519 AUC_train:0.9244\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9020\n",
      "Epoch:0083\n",
      "acc_train:0.8544 pre_train:0.8300 recall_train:0.9032 F1_train:0.8651 AUC_train:0.9443\n",
      "acc_val:0.7000 pre_val:0.6316 recall_val:0.9600 F1_val:0.761905 AUC_val:0.9188\n",
      "Epoch:0084\n",
      "acc_train:0.8500 pre_train:0.8173 recall_train:0.9140 F1_train:0.8629 AUC_train:0.9463\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.9160\n",
      "Epoch:0085\n",
      "acc_train:0.8678 pre_train:0.8419 recall_train:0.9161 F1_train:0.8774 AUC_train:0.9493\n",
      "acc_val:0.8400 pre_val:0.8148 recall_val:0.8800 F1_val:0.846154 AUC_val:0.9076\n",
      "Epoch:0086\n",
      "acc_train:0.8411 pre_train:0.8157 recall_train:0.8946 F1_train:0.8533 AUC_train:0.9331\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9232\n",
      "Epoch:0087\n",
      "acc_train:0.8567 pre_train:0.8268 recall_train:0.9140 F1_train:0.8682 AUC_train:0.9477\n",
      "acc_val:0.7900 pre_val:0.7164 recall_val:0.9600 F1_val:0.820513 AUC_val:0.9332\n",
      "Epoch:0088\n",
      "acc_train:0.8800 pre_train:0.8349 recall_train:0.9570 F1_train:0.8918 AUC_train:0.9674\n",
      "acc_val:0.7300 pre_val:0.6620 recall_val:0.9400 F1_val:0.776860 AUC_val:0.9180\n",
      "Epoch:0089\n",
      "acc_train:0.8789 pre_train:0.8371 recall_train:0.9505 F1_train:0.8902 AUC_train:0.9636\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9296\n",
      "Epoch:0090\n",
      "acc_train:0.8722 pre_train:0.8253 recall_train:0.9548 F1_train:0.8853 AUC_train:0.9644\n",
      "acc_val:0.8700 pre_val:0.8246 recall_val:0.9400 F1_val:0.878505 AUC_val:0.9224\n",
      "Epoch:0091\n",
      "acc_train:0.8789 pre_train:0.8423 recall_train:0.9419 F1_train:0.8893 AUC_train:0.9620\n",
      "acc_val:0.8400 pre_val:0.8148 recall_val:0.8800 F1_val:0.846154 AUC_val:0.9128\n",
      "Epoch:0092\n",
      "acc_train:0.8533 pre_train:0.8271 recall_train:0.9054 F1_train:0.8645 AUC_train:0.9534\n",
      "acc_val:0.8300 pre_val:0.8113 recall_val:0.8600 F1_val:0.834951 AUC_val:0.9120\n",
      "Epoch:0093\n",
      "acc_train:0.8700 pre_train:0.8308 recall_train:0.9398 F1_train:0.8819 AUC_train:0.9609\n",
      "acc_val:0.8400 pre_val:0.8148 recall_val:0.8800 F1_val:0.846154 AUC_val:0.9276\n",
      "Epoch:0094\n",
      "acc_train:0.8711 pre_train:0.8469 recall_train:0.9161 F1_train:0.8802 AUC_train:0.9601\n",
      "acc_val:0.8600 pre_val:0.8103 recall_val:0.9400 F1_val:0.870370 AUC_val:0.9284\n",
      "Epoch:0095\n",
      "acc_train:0.8922 pre_train:0.8651 recall_train:0.9376 F1_train:0.8999 AUC_train:0.9684\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9168\n",
      "Epoch:0096\n",
      "acc_train:0.8911 pre_train:0.8522 recall_train:0.9548 F1_train:0.9006 AUC_train:0.9669\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9240\n",
      "Epoch:0097\n",
      "acc_train:0.9033 pre_train:0.8706 recall_train:0.9548 F1_train:0.9108 AUC_train:0.9746\n",
      "acc_val:0.8500 pre_val:0.8302 recall_val:0.8800 F1_val:0.854369 AUC_val:0.9100\n",
      "Epoch:0098\n",
      "acc_train:0.8878 pre_train:0.8699 recall_train:0.9204 F1_train:0.8945 AUC_train:0.9652\n",
      "acc_val:0.8300 pre_val:0.8113 recall_val:0.8600 F1_val:0.834951 AUC_val:0.9240\n",
      "Epoch:0099\n",
      "acc_train:0.9078 pre_train:0.8866 recall_train:0.9419 F1_train:0.9135 AUC_train:0.9756\n",
      "acc_val:0.8400 pre_val:0.8148 recall_val:0.8800 F1_val:0.846154 AUC_val:0.9288\n",
      "Epoch:0100\n",
      "acc_train:0.9033 pre_train:0.8873 recall_train:0.9312 F1_train:0.9087 AUC_train:0.9751\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9222\n",
      "Epoch:0101\n",
      "acc_train:0.9278 pre_train:0.9032 recall_train:0.9634 F1_train:0.9324 AUC_train:0.9777\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9280\n",
      "Epoch:0102\n",
      "acc_train:0.9189 pre_train:0.9016 recall_train:0.9462 F1_train:0.9234 AUC_train:0.9772\n",
      "acc_val:0.8400 pre_val:0.7833 recall_val:0.9400 F1_val:0.854545 AUC_val:0.9328\n",
      "Epoch:0103\n",
      "acc_train:0.9122 pre_train:0.8891 recall_train:0.9484 F1_train:0.9178 AUC_train:0.9725\n",
      "acc_val:0.8600 pre_val:0.8103 recall_val:0.9400 F1_val:0.870370 AUC_val:0.9364\n",
      "Epoch:0104\n",
      "acc_train:0.9133 pre_train:0.9126 recall_train:0.9204 F1_train:0.9165 AUC_train:0.9738\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9328\n",
      "Epoch:0105\n",
      "acc_train:0.9189 pre_train:0.9000 recall_train:0.9484 F1_train:0.9236 AUC_train:0.9779\n",
      "acc_val:0.8600 pre_val:0.8333 recall_val:0.9000 F1_val:0.865385 AUC_val:0.9234\n",
      "Epoch:0106\n",
      "acc_train:0.9256 pre_train:0.9289 recall_train:0.9269 F1_train:0.9279 AUC_train:0.9792\n",
      "acc_val:0.8500 pre_val:0.8302 recall_val:0.8800 F1_val:0.854369 AUC_val:0.9218\n",
      "Epoch:0107\n",
      "acc_train:0.9267 pre_train:0.9165 recall_train:0.9441 F1_train:0.9301 AUC_train:0.9769\n",
      "acc_val:0.8700 pre_val:0.8491 recall_val:0.9000 F1_val:0.873786 AUC_val:0.9220\n",
      "Epoch:0108\n",
      "acc_train:0.9411 pre_train:0.9364 recall_train:0.9505 F1_train:0.9434 AUC_train:0.9831\n",
      "acc_val:0.8800 pre_val:0.8519 recall_val:0.9200 F1_val:0.884615 AUC_val:0.9248\n",
      "Epoch:0109\n",
      "acc_train:0.9333 pre_train:0.9245 recall_train:0.9484 F1_train:0.9363 AUC_train:0.9822\n",
      "acc_val:0.8700 pre_val:0.8364 recall_val:0.9200 F1_val:0.876190 AUC_val:0.9326\n",
      "Epoch:0110\n",
      "acc_train:0.9400 pre_train:0.9382 recall_train:0.9462 F1_train:0.9422 AUC_train:0.9862\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9370\n",
      "Epoch:0111\n",
      "acc_train:0.9467 pre_train:0.9427 recall_train:0.9548 F1_train:0.9487 AUC_train:0.9861\n",
      "acc_val:0.8900 pre_val:0.8545 recall_val:0.9400 F1_val:0.895238 AUC_val:0.9394\n",
      "Epoch:0112\n",
      "acc_train:0.9444 pre_train:0.9482 recall_train:0.9441 F1_train:0.9461 AUC_train:0.9867\n",
      "acc_val:0.8700 pre_val:0.8491 recall_val:0.9000 F1_val:0.873786 AUC_val:0.9286\n",
      "Epoch:0113\n",
      "acc_train:0.9433 pre_train:0.9386 recall_train:0.9527 F1_train:0.9456 AUC_train:0.9870\n",
      "acc_val:0.8800 pre_val:0.8519 recall_val:0.9200 F1_val:0.884615 AUC_val:0.9318\n",
      "Epoch:0114\n",
      "acc_train:0.9311 pre_train:0.9296 recall_train:0.9376 F1_train:0.9336 AUC_train:0.9835\n",
      "acc_val:0.8700 pre_val:0.8364 recall_val:0.9200 F1_val:0.876190 AUC_val:0.9310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0115\n",
      "acc_train:0.9389 pre_train:0.9457 recall_train:0.9355 F1_train:0.9405 AUC_train:0.9840\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9280\n",
      "Epoch:0116\n",
      "acc_train:0.9456 pre_train:0.9426 recall_train:0.9527 F1_train:0.9476 AUC_train:0.9867\n",
      "acc_val:0.8700 pre_val:0.8246 recall_val:0.9400 F1_val:0.878505 AUC_val:0.9224\n",
      "Epoch:0117\n",
      "acc_train:0.9378 pre_train:0.9455 recall_train:0.9333 F1_train:0.9394 AUC_train:0.9827\n",
      "acc_val:0.8800 pre_val:0.8393 recall_val:0.9400 F1_val:0.886792 AUC_val:0.9176\n",
      "Epoch:0118\n",
      "acc_train:0.9433 pre_train:0.9520 recall_train:0.9376 F1_train:0.9447 AUC_train:0.9889\n",
      "acc_val:0.8300 pre_val:0.8113 recall_val:0.8600 F1_val:0.834951 AUC_val:0.9008\n",
      "Epoch:0119\n",
      "acc_train:0.9511 pre_train:0.9566 recall_train:0.9484 F1_train:0.9525 AUC_train:0.9870\n",
      "acc_val:0.8400 pre_val:0.8269 recall_val:0.8600 F1_val:0.843137 AUC_val:0.9118\n",
      "Epoch:0120\n",
      "acc_train:0.9411 pre_train:0.9364 recall_train:0.9505 F1_train:0.9434 AUC_train:0.9851\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9060\n",
      "Epoch:0121\n",
      "acc_train:0.9344 pre_train:0.9394 recall_train:0.9333 F1_train:0.9364 AUC_train:0.9821\n",
      "acc_val:0.8500 pre_val:0.8070 recall_val:0.9200 F1_val:0.859813 AUC_val:0.9260\n",
      "Epoch:0122\n",
      "acc_train:0.9444 pre_train:0.9601 recall_train:0.9312 F1_train:0.9454 AUC_train:0.9889\n",
      "acc_val:0.8900 pre_val:0.8545 recall_val:0.9400 F1_val:0.895238 AUC_val:0.9362\n",
      "Epoch:0123\n",
      "acc_train:0.9378 pre_train:0.9342 recall_train:0.9462 F1_train:0.9402 AUC_train:0.9864\n",
      "acc_val:0.8800 pre_val:0.8519 recall_val:0.9200 F1_val:0.884615 AUC_val:0.9362\n",
      "Epoch:0124\n",
      "acc_train:0.9411 pre_train:0.9537 recall_train:0.9312 F1_train:0.9423 AUC_train:0.9879\n",
      "acc_val:0.8900 pre_val:0.8679 recall_val:0.9200 F1_val:0.893204 AUC_val:0.9356\n",
      "Epoch:0125\n",
      "acc_train:0.9511 pre_train:0.9606 recall_train:0.9441 F1_train:0.9523 AUC_train:0.9902\n",
      "acc_val:0.8900 pre_val:0.8679 recall_val:0.9200 F1_val:0.893204 AUC_val:0.9376\n",
      "Epoch:0126\n",
      "acc_train:0.9478 pre_train:0.9485 recall_train:0.9505 F1_train:0.9495 AUC_train:0.9891\n",
      "acc_val:0.9000 pre_val:0.8704 recall_val:0.9400 F1_val:0.903846 AUC_val:0.9390\n",
      "Epoch:0127\n",
      "acc_train:0.9511 pre_train:0.9647 recall_train:0.9398 F1_train:0.9521 AUC_train:0.9887\n",
      "acc_val:0.8800 pre_val:0.8519 recall_val:0.9200 F1_val:0.884615 AUC_val:0.9390\n",
      "Epoch:0128\n",
      "acc_train:0.9600 pre_train:0.9613 recall_train:0.9613 F1_train:0.9613 AUC_train:0.9895\n",
      "acc_val:0.8600 pre_val:0.8333 recall_val:0.9000 F1_val:0.865385 AUC_val:0.9296\n",
      "Epoch:0129\n",
      "acc_train:0.9700 pre_train:0.9740 recall_train:0.9677 F1_train:0.9709 AUC_train:0.9936\n",
      "acc_val:0.8700 pre_val:0.8364 recall_val:0.9200 F1_val:0.876190 AUC_val:0.9288\n",
      "Epoch:0130\n",
      "acc_train:0.9622 pre_train:0.9556 recall_train:0.9720 F1_train:0.9638 AUC_train:0.9924\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9282\n",
      "Epoch:0131\n",
      "acc_train:0.9478 pre_train:0.9604 recall_train:0.9376 F1_train:0.9489 AUC_train:0.9879\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9272\n",
      "Epoch:0132\n",
      "acc_train:0.9533 pre_train:0.9568 recall_train:0.9527 F1_train:0.9547 AUC_train:0.9891\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9176\n",
      "Epoch:0133\n",
      "acc_train:0.9633 pre_train:0.9615 recall_train:0.9677 F1_train:0.9646 AUC_train:0.9921\n",
      "acc_val:0.8400 pre_val:0.7931 recall_val:0.9200 F1_val:0.851852 AUC_val:0.9136\n",
      "Epoch:0134\n",
      "acc_train:0.9567 pre_train:0.9610 recall_train:0.9548 F1_train:0.9579 AUC_train:0.9913\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9104\n",
      "Epoch:0135\n",
      "acc_train:0.9600 pre_train:0.9694 recall_train:0.9527 F1_train:0.9610 AUC_train:0.9928\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9084\n",
      "Epoch:0136\n",
      "acc_train:0.9467 pre_train:0.9408 recall_train:0.9570 F1_train:0.9488 AUC_train:0.9907\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9204\n",
      "Epoch:0137\n",
      "acc_train:0.9622 pre_train:0.9615 recall_train:0.9656 F1_train:0.9635 AUC_train:0.9944\n",
      "acc_val:0.8700 pre_val:0.8627 recall_val:0.8800 F1_val:0.871287 AUC_val:0.9292\n",
      "Epoch:0138\n",
      "acc_train:0.9622 pre_train:0.9615 recall_train:0.9656 F1_train:0.9635 AUC_train:0.9917\n",
      "acc_val:0.8800 pre_val:0.8654 recall_val:0.9000 F1_val:0.882353 AUC_val:0.9284\n",
      "Epoch:0139\n",
      "acc_train:0.9678 pre_train:0.9719 recall_train:0.9656 F1_train:0.9687 AUC_train:0.9962\n",
      "acc_val:0.8700 pre_val:0.8364 recall_val:0.9200 F1_val:0.876190 AUC_val:0.9318\n",
      "Epoch:0140\n",
      "acc_train:0.9656 pre_train:0.9717 recall_train:0.9613 F1_train:0.9665 AUC_train:0.9923\n",
      "acc_val:0.8500 pre_val:0.8070 recall_val:0.9200 F1_val:0.859813 AUC_val:0.9338\n",
      "Epoch:0141\n",
      "acc_train:0.9744 pre_train:0.9763 recall_train:0.9742 F1_train:0.9752 AUC_train:0.9971\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9308\n",
      "Epoch:0142\n",
      "acc_train:0.9656 pre_train:0.9617 recall_train:0.9720 F1_train:0.9668 AUC_train:0.9920\n",
      "acc_val:0.8500 pre_val:0.8070 recall_val:0.9200 F1_val:0.859813 AUC_val:0.9240\n",
      "Epoch:0143\n",
      "acc_train:0.9689 pre_train:0.9699 recall_train:0.9699 F1_train:0.9699 AUC_train:0.9927\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9256\n",
      "Early Stopping!!! epoch：142\n",
      " Starting the 1-3 Fold:：\n",
      "Fitting estimator with 131503 features.\n",
      "Fitting estimator with 121503 features.\n",
      "Fitting estimator with 111503 features.\n",
      "Fitting estimator with 101503 features.\n",
      "Fitting estimator with 91503 features.\n",
      "Fitting estimator with 81503 features.\n",
      "Fitting estimator with 71503 features.\n",
      "Fitting estimator with 61503 features.\n",
      "Fitting estimator with 51503 features.\n",
      "Fitting estimator with 41503 features.\n",
      "Fitting estimator with 31503 features.\n",
      "Fitting estimator with 21503 features.\n",
      "Fitting estimator with 11503 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.5300 pre_train:0.5673 recall_train:0.3806 F1_train:0.4556 AUC_train:0.5383\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.5756\n",
      "Epoch:0002\n",
      "acc_train:0.5522 pre_train:0.6157 recall_train:0.3548 F1_train:0.4502 AUC_train:0.5912\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.5976\n",
      "Epoch:0003\n",
      "acc_train:0.5600 pre_train:0.6332 recall_train:0.3527 F1_train:0.4530 AUC_train:0.6042\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6108\n",
      "Epoch:0004\n",
      "acc_train:0.5722 pre_train:0.6709 recall_train:0.3376 F1_train:0.4492 AUC_train:0.6182\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6284\n",
      "Epoch:0005\n",
      "acc_train:0.5800 pre_train:0.6820 recall_train:0.3505 F1_train:0.4631 AUC_train:0.6140\n",
      "acc_val:0.5500 pre_val:0.5301 recall_val:0.8800 F1_val:0.661654 AUC_val:0.6404\n",
      "Epoch:0006\n",
      "acc_train:0.5689 pre_train:0.5975 recall_train:0.5075 F1_train:0.5488 AUC_train:0.5832\n",
      "acc_val:0.5600 pre_val:0.5441 recall_val:0.7400 F1_val:0.627119 AUC_val:0.6608\n",
      "Epoch:0007\n",
      "acc_train:0.6067 pre_train:0.7382 recall_train:0.3699 F1_train:0.4928 AUC_train:0.6348\n",
      "acc_val:0.5600 pre_val:0.5455 recall_val:0.7200 F1_val:0.620690 AUC_val:0.6688\n",
      "Epoch:0008\n",
      "acc_train:0.5844 pre_train:0.6704 recall_train:0.3849 F1_train:0.4891 AUC_train:0.6093\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6552\n",
      "Epoch:0009\n",
      "acc_train:0.5989 pre_train:0.6871 recall_train:0.4108 F1_train:0.5141 AUC_train:0.5940\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6500\n",
      "Epoch:0010\n",
      "acc_train:0.5867 pre_train:0.6177 recall_train:0.5247 F1_train:0.5674 AUC_train:0.6042\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6576\n",
      "Epoch:0011\n",
      "acc_train:0.5689 pre_train:0.5699 recall_train:0.6753 F1_train:0.6181 AUC_train:0.5958\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6544\n",
      "Epoch:0012\n",
      "acc_train:0.5833 pre_train:0.6250 recall_train:0.4839 F1_train:0.5455 AUC_train:0.6048\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6508\n",
      "Epoch:0013\n",
      "acc_train:0.5433 pre_train:0.5450 recall_train:0.7032 F1_train:0.6141 AUC_train:0.5962\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6428\n",
      "Epoch:0014\n",
      "acc_train:0.5978 pre_train:0.6016 recall_train:0.6559 F1_train:0.6276 AUC_train:0.6424\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0015\n",
      "acc_train:0.5678 pre_train:0.5671 recall_train:0.6903 F1_train:0.6227 AUC_train:0.6058\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6616\n",
      "Epoch:0016\n",
      "acc_train:0.5844 pre_train:0.5962 recall_train:0.6065 F1_train:0.6013 AUC_train:0.6224\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6620\n",
      "Epoch:0017\n",
      "acc_train:0.5578 pre_train:0.5561 recall_train:0.7140 F1_train:0.6252 AUC_train:0.5942\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6640\n",
      "Epoch:0018\n",
      "acc_train:0.5833 pre_train:0.5869 recall_train:0.6538 F1_train:0.6185 AUC_train:0.6249\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6632\n",
      "Epoch:0019\n",
      "acc_train:0.6133 pre_train:0.6247 recall_train:0.6301 F1_train:0.6274 AUC_train:0.6325\n",
      "acc_val:0.4900 pre_val:0.4948 recall_val:0.9600 F1_val:0.653061 AUC_val:0.6636\n",
      "Epoch:0020\n",
      "acc_train:0.6022 pre_train:0.6051 recall_train:0.6624 F1_train:0.6324 AUC_train:0.6389\n",
      "acc_val:0.5700 pre_val:0.5507 recall_val:0.7600 F1_val:0.638655 AUC_val:0.6640\n",
      "Epoch:0021\n",
      "acc_train:0.5978 pre_train:0.6112 recall_train:0.6086 F1_train:0.6099 AUC_train:0.6225\n",
      "acc_val:0.6400 pre_val:0.6207 recall_val:0.7200 F1_val:0.666667 AUC_val:0.6644\n",
      "Epoch:0022\n",
      "acc_train:0.6033 pre_train:0.6169 recall_train:0.6129 F1_train:0.6149 AUC_train:0.6305\n",
      "acc_val:0.6500 pre_val:0.6415 recall_val:0.6800 F1_val:0.660194 AUC_val:0.6564\n",
      "Epoch:0023\n",
      "acc_train:0.6200 pre_train:0.6141 recall_train:0.7118 F1_train:0.6594 AUC_train:0.6523\n",
      "acc_val:0.6200 pre_val:0.6200 recall_val:0.6200 F1_val:0.620000 AUC_val:0.6436\n",
      "Epoch:0024\n",
      "acc_train:0.6067 pre_train:0.6164 recall_train:0.6323 F1_train:0.6242 AUC_train:0.6285\n",
      "acc_val:0.6200 pre_val:0.6304 recall_val:0.5800 F1_val:0.604167 AUC_val:0.6432\n",
      "Epoch:0025\n",
      "acc_train:0.5956 pre_train:0.6050 recall_train:0.6258 F1_train:0.6152 AUC_train:0.6219\n",
      "acc_val:0.6200 pre_val:0.6304 recall_val:0.5800 F1_val:0.604167 AUC_val:0.6432\n",
      "Epoch:0026\n",
      "acc_train:0.6022 pre_train:0.6151 recall_train:0.6151 F1_train:0.6151 AUC_train:0.6320\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.6312\n",
      "Epoch:0027\n",
      "acc_train:0.5889 pre_train:0.5963 recall_train:0.6323 F1_train:0.6138 AUC_train:0.6278\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.6320\n",
      "Epoch:0028\n",
      "acc_train:0.6011 pre_train:0.6004 recall_train:0.6817 F1_train:0.6385 AUC_train:0.6408\n",
      "acc_val:0.6100 pre_val:0.6222 recall_val:0.5600 F1_val:0.589474 AUC_val:0.6388\n",
      "Epoch:0029\n",
      "acc_train:0.5867 pre_train:0.5910 recall_train:0.6495 F1_train:0.6189 AUC_train:0.6349\n",
      "acc_val:0.6400 pre_val:0.6522 recall_val:0.6000 F1_val:0.625000 AUC_val:0.6676\n",
      "Epoch:0030\n",
      "acc_train:0.5722 pre_train:0.5775 recall_train:0.6409 F1_train:0.6075 AUC_train:0.6071\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6620\n",
      "Epoch:0031\n",
      "acc_train:0.6156 pre_train:0.6227 recall_train:0.6495 F1_train:0.6358 AUC_train:0.6369\n",
      "acc_val:0.6200 pre_val:0.6304 recall_val:0.5800 F1_val:0.604167 AUC_val:0.6588\n",
      "Epoch:0032\n",
      "acc_train:0.6244 pre_train:0.6408 recall_train:0.6215 F1_train:0.6310 AUC_train:0.6614\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6600\n",
      "Epoch:0033\n",
      "acc_train:0.5822 pre_train:0.5838 recall_train:0.6667 F1_train:0.6225 AUC_train:0.6287\n",
      "acc_val:0.6100 pre_val:0.6222 recall_val:0.5600 F1_val:0.589474 AUC_val:0.6688\n",
      "Epoch:0034\n",
      "acc_train:0.6122 pre_train:0.6169 recall_train:0.6581 F1_train:0.6368 AUC_train:0.6520\n",
      "acc_val:0.6200 pre_val:0.6304 recall_val:0.5800 F1_val:0.604167 AUC_val:0.6692\n",
      "Epoch:0035\n",
      "acc_train:0.6122 pre_train:0.6224 recall_train:0.6344 F1_train:0.6283 AUC_train:0.6445\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6680\n",
      "Epoch:0036\n",
      "acc_train:0.6033 pre_train:0.6149 recall_train:0.6215 F1_train:0.6182 AUC_train:0.6372\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6764\n",
      "Epoch:0037\n",
      "acc_train:0.6044 pre_train:0.6143 recall_train:0.6301 F1_train:0.6221 AUC_train:0.6492\n",
      "acc_val:0.6500 pre_val:0.6829 recall_val:0.5600 F1_val:0.615385 AUC_val:0.6916\n",
      "Epoch:0038\n",
      "acc_train:0.6156 pre_train:0.6308 recall_train:0.6172 F1_train:0.6239 AUC_train:0.6594\n",
      "acc_val:0.6600 pre_val:0.7105 recall_val:0.5400 F1_val:0.613636 AUC_val:0.6932\n",
      "Epoch:0039\n",
      "acc_train:0.6189 pre_train:0.6425 recall_train:0.5914 F1_train:0.6159 AUC_train:0.6527\n",
      "acc_val:0.6600 pre_val:0.7105 recall_val:0.5400 F1_val:0.613636 AUC_val:0.6948\n",
      "Epoch:0040\n",
      "acc_train:0.6322 pre_train:0.6791 recall_train:0.5462 F1_train:0.6055 AUC_train:0.6575\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7036\n",
      "Epoch:0041\n",
      "acc_train:0.6167 pre_train:0.6364 recall_train:0.6022 F1_train:0.6188 AUC_train:0.6446\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7056\n",
      "Epoch:0042\n",
      "acc_train:0.6156 pre_train:0.6319 recall_train:0.6129 F1_train:0.6223 AUC_train:0.6444\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7060\n",
      "Epoch:0043\n",
      "acc_train:0.6011 pre_train:0.6233 recall_train:0.5763 F1_train:0.5989 AUC_train:0.6476\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.6976\n",
      "Epoch:0044\n",
      "acc_train:0.6056 pre_train:0.6267 recall_train:0.5849 F1_train:0.6051 AUC_train:0.6444\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.6952\n",
      "Epoch:0045\n",
      "acc_train:0.6111 pre_train:0.6340 recall_train:0.5849 F1_train:0.6085 AUC_train:0.6464\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6948\n",
      "Epoch:0046\n",
      "acc_train:0.6167 pre_train:0.6415 recall_train:0.5849 F1_train:0.6119 AUC_train:0.6625\n",
      "acc_val:0.6600 pre_val:0.7222 recall_val:0.5200 F1_val:0.604651 AUC_val:0.6968\n",
      "Epoch:0047\n",
      "acc_train:0.6378 pre_train:0.6643 recall_train:0.6043 F1_train:0.6329 AUC_train:0.6835\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.6960\n",
      "Epoch:0048\n",
      "acc_train:0.6311 pre_train:0.6595 recall_train:0.5914 F1_train:0.6236 AUC_train:0.6726\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.6884\n",
      "Epoch:0049\n",
      "acc_train:0.6556 pre_train:0.6841 recall_train:0.6194 F1_train:0.6501 AUC_train:0.6981\n",
      "acc_val:0.6700 pre_val:0.7429 recall_val:0.5200 F1_val:0.611765 AUC_val:0.6900\n",
      "Epoch:0050\n",
      "acc_train:0.6422 pre_train:0.6731 recall_train:0.5978 F1_train:0.6333 AUC_train:0.6899\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.6896\n",
      "Epoch:0051\n",
      "acc_train:0.6211 pre_train:0.6366 recall_train:0.6215 F1_train:0.6289 AUC_train:0.6821\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.6912\n",
      "Epoch:0052\n",
      "acc_train:0.6289 pre_train:0.6527 recall_train:0.6022 F1_train:0.6264 AUC_train:0.6990\n",
      "acc_val:0.6400 pre_val:0.6842 recall_val:0.5200 F1_val:0.590909 AUC_val:0.6936\n",
      "Epoch:0053\n",
      "acc_train:0.6244 pre_train:0.6446 recall_train:0.6086 F1_train:0.6261 AUC_train:0.6890\n",
      "acc_val:0.6300 pre_val:0.6667 recall_val:0.5200 F1_val:0.584270 AUC_val:0.6980\n",
      "Epoch:0054\n",
      "acc_train:0.6311 pre_train:0.6587 recall_train:0.5935 F1_train:0.6244 AUC_train:0.6998\n",
      "acc_val:0.6300 pre_val:0.6667 recall_val:0.5200 F1_val:0.584270 AUC_val:0.7024\n",
      "Epoch:0055\n",
      "acc_train:0.6478 pre_train:0.6682 recall_train:0.6323 F1_train:0.6497 AUC_train:0.7266\n",
      "acc_val:0.6300 pre_val:0.6667 recall_val:0.5200 F1_val:0.584270 AUC_val:0.7072\n",
      "Epoch:0056\n",
      "acc_train:0.6556 pre_train:0.6734 recall_train:0.6473 F1_train:0.6601 AUC_train:0.7220\n",
      "acc_val:0.6300 pre_val:0.6667 recall_val:0.5200 F1_val:0.584270 AUC_val:0.7112\n",
      "Epoch:0057\n",
      "acc_train:0.6522 pre_train:0.6784 recall_train:0.6215 F1_train:0.6487 AUC_train:0.7179\n",
      "acc_val:0.6300 pre_val:0.6667 recall_val:0.5200 F1_val:0.584270 AUC_val:0.7128\n",
      "Epoch:0058\n",
      "acc_train:0.6633 pre_train:0.6746 recall_train:0.6731 F1_train:0.6738 AUC_train:0.7240\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.7168\n",
      "Epoch:0059\n",
      "acc_train:0.6756 pre_train:0.6970 recall_train:0.6581 F1_train:0.6770 AUC_train:0.7572\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.7124\n",
      "Epoch:0060\n",
      "acc_train:0.6844 pre_train:0.6905 recall_train:0.7054 F1_train:0.6979 AUC_train:0.7546\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.7144\n",
      "Epoch:0061\n",
      "acc_train:0.6711 pre_train:0.6934 recall_train:0.6516 F1_train:0.6718 AUC_train:0.7468\n",
      "acc_val:0.6600 pre_val:0.7000 recall_val:0.5600 F1_val:0.622222 AUC_val:0.7132\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0062\n",
      "acc_train:0.6978 pre_train:0.7102 recall_train:0.7011 F1_train:0.7056 AUC_train:0.7684\n",
      "acc_val:0.6800 pre_val:0.7368 recall_val:0.5600 F1_val:0.636364 AUC_val:0.7248\n",
      "Epoch:0063\n",
      "acc_train:0.7189 pre_train:0.7218 recall_train:0.7419 F1_train:0.7317 AUC_train:0.7797\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7388\n",
      "Epoch:0064\n",
      "acc_train:0.7100 pre_train:0.7329 recall_train:0.6903 F1_train:0.7110 AUC_train:0.7840\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7464\n",
      "Epoch:0065\n",
      "acc_train:0.7522 pre_train:0.7410 recall_train:0.8000 F1_train:0.7694 AUC_train:0.8089\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7468\n",
      "Epoch:0066\n",
      "acc_train:0.7544 pre_train:0.7531 recall_train:0.7806 F1_train:0.7666 AUC_train:0.7928\n",
      "acc_val:0.6800 pre_val:0.7368 recall_val:0.5600 F1_val:0.636364 AUC_val:0.7472\n",
      "Epoch:0067\n",
      "acc_train:0.7689 pre_train:0.7694 recall_train:0.7892 F1_train:0.7792 AUC_train:0.8244\n",
      "acc_val:0.6900 pre_val:0.7436 recall_val:0.5800 F1_val:0.651685 AUC_val:0.7548\n",
      "Epoch:0068\n",
      "acc_train:0.7622 pre_train:0.7525 recall_train:0.8043 F1_train:0.7775 AUC_train:0.8059\n",
      "acc_val:0.6900 pre_val:0.7317 recall_val:0.6000 F1_val:0.659341 AUC_val:0.7572\n",
      "Epoch:0069\n",
      "acc_train:0.7789 pre_train:0.7782 recall_train:0.8000 F1_train:0.7890 AUC_train:0.8278\n",
      "acc_val:0.7000 pre_val:0.7381 recall_val:0.6200 F1_val:0.673913 AUC_val:0.7608\n",
      "Epoch:0070\n",
      "acc_train:0.7889 pre_train:0.7756 recall_train:0.8323 F1_train:0.8029 AUC_train:0.8380\n",
      "acc_val:0.7100 pre_val:0.7333 recall_val:0.6600 F1_val:0.694737 AUC_val:0.7656\n",
      "Epoch:0071\n",
      "acc_train:0.8144 pre_train:0.7899 recall_train:0.8731 F1_train:0.8294 AUC_train:0.8606\n",
      "acc_val:0.7100 pre_val:0.7143 recall_val:0.7000 F1_val:0.707071 AUC_val:0.7708\n",
      "Epoch:0072\n",
      "acc_train:0.8111 pre_train:0.7767 recall_train:0.8903 F1_train:0.8297 AUC_train:0.8572\n",
      "acc_val:0.7000 pre_val:0.6923 recall_val:0.7200 F1_val:0.705882 AUC_val:0.7772\n",
      "Epoch:0073\n",
      "acc_train:0.8156 pre_train:0.7869 recall_train:0.8817 F1_train:0.8316 AUC_train:0.8548\n",
      "acc_val:0.7300 pre_val:0.6949 recall_val:0.8200 F1_val:0.752294 AUC_val:0.7820\n",
      "Epoch:0074\n",
      "acc_train:0.8389 pre_train:0.7952 recall_train:0.9269 F1_train:0.8560 AUC_train:0.8790\n",
      "acc_val:0.7000 pre_val:0.6613 recall_val:0.8200 F1_val:0.732143 AUC_val:0.7936\n",
      "Epoch:0075\n",
      "acc_train:0.8556 pre_train:0.8051 recall_train:0.9505 F1_train:0.8718 AUC_train:0.8977\n",
      "acc_val:0.7200 pre_val:0.6774 recall_val:0.8400 F1_val:0.750000 AUC_val:0.8052\n",
      "Epoch:0076\n",
      "acc_train:0.8444 pre_train:0.7866 recall_train:0.9591 F1_train:0.8643 AUC_train:0.8980\n",
      "acc_val:0.7400 pre_val:0.6765 recall_val:0.9200 F1_val:0.779661 AUC_val:0.8156\n",
      "Epoch:0077\n",
      "acc_train:0.8567 pre_train:0.8066 recall_train:0.9505 F1_train:0.8727 AUC_train:0.9259\n",
      "acc_val:0.7400 pre_val:0.6875 recall_val:0.8800 F1_val:0.771930 AUC_val:0.8140\n",
      "Epoch:0078\n",
      "acc_train:0.8433 pre_train:0.7967 recall_train:0.9355 F1_train:0.8605 AUC_train:0.8902\n",
      "acc_val:0.7000 pre_val:0.6667 recall_val:0.8000 F1_val:0.727273 AUC_val:0.8124\n",
      "Epoch:0079\n",
      "acc_train:0.8444 pre_train:0.7960 recall_train:0.9398 F1_train:0.8619 AUC_train:0.9018\n",
      "acc_val:0.7100 pre_val:0.6780 recall_val:0.8000 F1_val:0.733945 AUC_val:0.8204\n",
      "Epoch:0080\n",
      "acc_train:0.8467 pre_train:0.7914 recall_train:0.9548 F1_train:0.8655 AUC_train:0.8967\n",
      "acc_val:0.7500 pre_val:0.6923 recall_val:0.9000 F1_val:0.782609 AUC_val:0.8472\n",
      "Epoch:0081\n",
      "acc_train:0.8589 pre_train:0.8051 recall_train:0.9591 F1_train:0.8754 AUC_train:0.9173\n",
      "acc_val:0.7400 pre_val:0.6667 recall_val:0.9600 F1_val:0.786885 AUC_val:0.8684\n",
      "Epoch:0082\n",
      "acc_train:0.8889 pre_train:0.8312 recall_train:0.9849 F1_train:0.9016 AUC_train:0.9350\n",
      "acc_val:0.7400 pre_val:0.6667 recall_val:0.9600 F1_val:0.786885 AUC_val:0.8844\n",
      "Epoch:0083\n",
      "acc_train:0.8822 pre_train:0.8258 recall_train:0.9785 F1_train:0.8957 AUC_train:0.9402\n",
      "acc_val:0.7400 pre_val:0.6667 recall_val:0.9600 F1_val:0.786885 AUC_val:0.8856\n",
      "Epoch:0084\n",
      "acc_train:0.8944 pre_train:0.8439 recall_train:0.9763 F1_train:0.9053 AUC_train:0.9498\n",
      "acc_val:0.7900 pre_val:0.7377 recall_val:0.9000 F1_val:0.810811 AUC_val:0.8736\n",
      "Epoch:0085\n",
      "acc_train:0.8956 pre_train:0.8442 recall_train:0.9785 F1_train:0.9064 AUC_train:0.9332\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.8768\n",
      "Epoch:0086\n",
      "acc_train:0.8989 pre_train:0.8489 recall_train:0.9785 F1_train:0.9091 AUC_train:0.9401\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.8740\n",
      "Epoch:0087\n",
      "acc_train:0.8956 pre_train:0.8454 recall_train:0.9763 F1_train:0.9062 AUC_train:0.9474\n",
      "acc_val:0.8100 pre_val:0.7925 recall_val:0.8400 F1_val:0.815534 AUC_val:0.8656\n",
      "Epoch:0088\n",
      "acc_train:0.9011 pre_train:0.8521 recall_train:0.9785 F1_train:0.9109 AUC_train:0.9553\n",
      "acc_val:0.8200 pre_val:0.7857 recall_val:0.8800 F1_val:0.830189 AUC_val:0.8688\n",
      "Epoch:0089\n",
      "acc_train:0.9078 pre_train:0.8631 recall_train:0.9763 F1_train:0.9162 AUC_train:0.9559\n",
      "acc_val:0.7900 pre_val:0.7377 recall_val:0.9000 F1_val:0.810811 AUC_val:0.8784\n",
      "Epoch:0090\n",
      "acc_train:0.9078 pre_train:0.8659 recall_train:0.9720 F1_train:0.9159 AUC_train:0.9583\n",
      "acc_val:0.8000 pre_val:0.7419 recall_val:0.9200 F1_val:0.821429 AUC_val:0.8884\n",
      "Epoch:0091\n",
      "acc_train:0.9278 pre_train:0.8891 recall_train:0.9828 F1_train:0.9336 AUC_train:0.9732\n",
      "acc_val:0.8000 pre_val:0.7419 recall_val:0.9200 F1_val:0.821429 AUC_val:0.9024\n",
      "Epoch:0092\n",
      "acc_train:0.9156 pre_train:0.8733 recall_train:0.9785 F1_train:0.9229 AUC_train:0.9655\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.8964\n",
      "Epoch:0093\n",
      "acc_train:0.9156 pre_train:0.8777 recall_train:0.9720 F1_train:0.9224 AUC_train:0.9594\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9000\n",
      "Epoch:0094\n",
      "acc_train:0.9311 pre_train:0.8990 recall_train:0.9763 F1_train:0.9361 AUC_train:0.9774\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.9028\n",
      "Epoch:0095\n",
      "acc_train:0.9378 pre_train:0.9034 recall_train:0.9849 F1_train:0.9424 AUC_train:0.9776\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9012\n",
      "Epoch:0096\n",
      "acc_train:0.9311 pre_train:0.9054 recall_train:0.9677 F1_train:0.9356 AUC_train:0.9759\n",
      "acc_val:0.8500 pre_val:0.8431 recall_val:0.8600 F1_val:0.851485 AUC_val:0.8876\n",
      "Epoch:0097\n",
      "acc_train:0.9244 pre_train:0.8915 recall_train:0.9720 F1_train:0.9300 AUC_train:0.9731\n",
      "acc_val:0.8400 pre_val:0.8400 recall_val:0.8400 F1_val:0.840000 AUC_val:0.8784\n",
      "Epoch:0098\n",
      "acc_train:0.9133 pre_train:0.8847 recall_train:0.9570 F1_train:0.9194 AUC_train:0.9622\n",
      "acc_val:0.8400 pre_val:0.8269 recall_val:0.8600 F1_val:0.843137 AUC_val:0.8828\n",
      "Epoch:0099\n",
      "acc_train:0.9378 pre_train:0.9131 recall_train:0.9720 F1_train:0.9417 AUC_train:0.9775\n",
      "acc_val:0.8500 pre_val:0.8302 recall_val:0.8800 F1_val:0.854369 AUC_val:0.8928\n",
      "Epoch:0100\n",
      "acc_train:0.9356 pre_train:0.9046 recall_train:0.9785 F1_train:0.9401 AUC_train:0.9716\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.8944\n",
      "Epoch:0101\n",
      "acc_train:0.9467 pre_train:0.9246 recall_train:0.9763 F1_train:0.9498 AUC_train:0.9808\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.8960\n",
      "Epoch:0102\n",
      "acc_train:0.9400 pre_train:0.9118 recall_train:0.9785 F1_train:0.9440 AUC_train:0.9827\n",
      "acc_val:0.8000 pre_val:0.7419 recall_val:0.9200 F1_val:0.821429 AUC_val:0.8960\n",
      "Epoch:0103\n",
      "acc_train:0.9400 pre_train:0.9102 recall_train:0.9806 F1_train:0.9441 AUC_train:0.9778\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.8960\n",
      "Epoch:0104\n",
      "acc_train:0.9333 pre_train:0.9124 recall_train:0.9634 F1_train:0.9372 AUC_train:0.9699\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.8960\n",
      "Epoch:0105\n",
      "acc_train:0.9411 pre_train:0.9137 recall_train:0.9785 F1_train:0.9450 AUC_train:0.9791\n",
      "acc_val:0.8000 pre_val:0.7419 recall_val:0.9200 F1_val:0.821429 AUC_val:0.9068\n",
      "Epoch:0106\n",
      "acc_train:0.9556 pre_train:0.9346 recall_train:0.9828 F1_train:0.9581 AUC_train:0.9830\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9068\n",
      "Epoch:0107\n",
      "acc_train:0.9489 pre_train:0.9249 recall_train:0.9806 F1_train:0.9520 AUC_train:0.9814\n",
      "acc_val:0.8100 pre_val:0.7541 recall_val:0.9200 F1_val:0.828829 AUC_val:0.9156\n",
      "Epoch:0108\n",
      "acc_train:0.9656 pre_train:0.9465 recall_train:0.9892 F1_train:0.9674 AUC_train:0.9881\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0109\n",
      "acc_train:0.9456 pre_train:0.9245 recall_train:0.9742 F1_train:0.9487 AUC_train:0.9840\n",
      "acc_val:0.8600 pre_val:0.8333 recall_val:0.9000 F1_val:0.865385 AUC_val:0.9132\n",
      "Epoch:0110\n",
      "acc_train:0.9422 pre_train:0.9172 recall_train:0.9763 F1_train:0.9458 AUC_train:0.9834\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9148\n",
      "Epoch:0111\n",
      "acc_train:0.9522 pre_train:0.9289 recall_train:0.9828 F1_train:0.9551 AUC_train:0.9902\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9164\n",
      "Epoch:0112\n",
      "acc_train:0.9622 pre_train:0.9462 recall_train:0.9828 F1_train:0.9641 AUC_train:0.9859\n",
      "acc_val:0.8100 pre_val:0.7541 recall_val:0.9200 F1_val:0.828829 AUC_val:0.9200\n",
      "Epoch:0113\n",
      "acc_train:0.9633 pre_train:0.9500 recall_train:0.9806 F1_train:0.9651 AUC_train:0.9906\n",
      "acc_val:0.8000 pre_val:0.7419 recall_val:0.9200 F1_val:0.821429 AUC_val:0.9204\n",
      "Epoch:0114\n",
      "acc_train:0.9567 pre_train:0.9329 recall_train:0.9871 F1_train:0.9592 AUC_train:0.9901\n",
      "acc_val:0.8000 pre_val:0.7419 recall_val:0.9200 F1_val:0.821429 AUC_val:0.9164\n",
      "Epoch:0115\n",
      "acc_train:0.9656 pre_train:0.9447 recall_train:0.9914 F1_train:0.9675 AUC_train:0.9852\n",
      "acc_val:0.7900 pre_val:0.7302 recall_val:0.9200 F1_val:0.814159 AUC_val:0.9168\n",
      "Epoch:0116\n",
      "acc_train:0.9656 pre_train:0.9465 recall_train:0.9892 F1_train:0.9674 AUC_train:0.9877\n",
      "acc_val:0.7900 pre_val:0.7302 recall_val:0.9200 F1_val:0.814159 AUC_val:0.9164\n",
      "Epoch:0117\n",
      "acc_train:0.9656 pre_train:0.9411 recall_train:0.9957 F1_train:0.9676 AUC_train:0.9914\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9136\n",
      "Epoch:0118\n",
      "acc_train:0.9678 pre_train:0.9449 recall_train:0.9957 F1_train:0.9696 AUC_train:0.9915\n",
      "acc_val:0.8000 pre_val:0.7419 recall_val:0.9200 F1_val:0.821429 AUC_val:0.9116\n",
      "Epoch:0119\n",
      "acc_train:0.9611 pre_train:0.9479 recall_train:0.9785 F1_train:0.9630 AUC_train:0.9917\n",
      "acc_val:0.7900 pre_val:0.7377 recall_val:0.9000 F1_val:0.810811 AUC_val:0.9064\n",
      "Epoch:0120\n",
      "acc_train:0.9711 pre_train:0.9582 recall_train:0.9871 F1_train:0.9725 AUC_train:0.9919\n",
      "acc_val:0.7900 pre_val:0.7377 recall_val:0.9000 F1_val:0.810811 AUC_val:0.9044\n",
      "Epoch:0121\n",
      "acc_train:0.9622 pre_train:0.9499 recall_train:0.9785 F1_train:0.9640 AUC_train:0.9931\n",
      "acc_val:0.7900 pre_val:0.7377 recall_val:0.9000 F1_val:0.810811 AUC_val:0.9012\n",
      "Epoch:0122\n",
      "acc_train:0.9611 pre_train:0.9498 recall_train:0.9763 F1_train:0.9629 AUC_train:0.9885\n",
      "acc_val:0.7900 pre_val:0.7377 recall_val:0.9000 F1_val:0.810811 AUC_val:0.9016\n",
      "Epoch:0123\n",
      "acc_train:0.9644 pre_train:0.9501 recall_train:0.9828 F1_train:0.9662 AUC_train:0.9897\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9008\n",
      "Epoch:0124\n",
      "acc_train:0.9722 pre_train:0.9583 recall_train:0.9892 F1_train:0.9735 AUC_train:0.9953\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.9068\n",
      "Epoch:0125\n",
      "acc_train:0.9711 pre_train:0.9545 recall_train:0.9914 F1_train:0.9726 AUC_train:0.9954\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9072\n",
      "Epoch:0126\n",
      "acc_train:0.9656 pre_train:0.9465 recall_train:0.9892 F1_train:0.9674 AUC_train:0.9922\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9064\n",
      "Epoch:0127\n",
      "acc_train:0.9711 pre_train:0.9641 recall_train:0.9806 F1_train:0.9723 AUC_train:0.9941\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9060\n",
      "Epoch:0128\n",
      "acc_train:0.9800 pre_train:0.9647 recall_train:0.9978 F1_train:0.9810 AUC_train:0.9938\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9036\n",
      "Epoch:0129\n",
      "acc_train:0.9678 pre_train:0.9504 recall_train:0.9892 F1_train:0.9694 AUC_train:0.9925\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.8964\n",
      "Epoch:0130\n",
      "acc_train:0.9778 pre_train:0.9704 recall_train:0.9871 F1_train:0.9787 AUC_train:0.9945\n",
      "acc_val:0.7900 pre_val:0.7302 recall_val:0.9200 F1_val:0.814159 AUC_val:0.9010\n",
      "Early Stopping!!! epoch：129\n",
      " Starting the 1-4 Fold:：\n",
      "Fitting estimator with 131503 features.\n",
      "Fitting estimator with 121503 features.\n",
      "Fitting estimator with 111503 features.\n",
      "Fitting estimator with 101503 features.\n",
      "Fitting estimator with 91503 features.\n",
      "Fitting estimator with 81503 features.\n",
      "Fitting estimator with 71503 features.\n",
      "Fitting estimator with 61503 features.\n",
      "Fitting estimator with 51503 features.\n",
      "Fitting estimator with 41503 features.\n",
      "Fitting estimator with 31503 features.\n",
      "Fitting estimator with 21503 features.\n",
      "Fitting estimator with 11503 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n",
      "Epoch:0001\n",
      "acc_train:0.5133 pre_train:0.5421 recall_train:0.3742 F1_train:0.4427 AUC_train:0.5411\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6492\n",
      "Epoch:0002\n",
      "acc_train:0.5356 pre_train:0.5630 recall_train:0.4516 F1_train:0.5012 AUC_train:0.5615\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6596\n",
      "Epoch:0003\n",
      "acc_train:0.5944 pre_train:0.6366 recall_train:0.5011 F1_train:0.5608 AUC_train:0.6370\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6616\n",
      "Epoch:0004\n",
      "acc_train:0.5533 pre_train:0.5831 recall_train:0.4753 F1_train:0.5237 AUC_train:0.5710\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6844\n",
      "Epoch:0005\n",
      "acc_train:0.5889 pre_train:0.6331 recall_train:0.4860 F1_train:0.5499 AUC_train:0.6102\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6940\n",
      "Epoch:0006\n",
      "acc_train:0.5922 pre_train:0.6361 recall_train:0.4925 F1_train:0.5552 AUC_train:0.5971\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:1.0000 F1_val:0.666667 AUC_val:0.6896\n",
      "Epoch:0007\n",
      "acc_train:0.5911 pre_train:0.6228 recall_train:0.5290 F1_train:0.5721 AUC_train:0.6246\n",
      "acc_val:0.5500 pre_val:0.5287 recall_val:0.9200 F1_val:0.671533 AUC_val:0.6828\n",
      "Epoch:0008\n",
      "acc_train:0.6189 pre_train:0.6580 recall_train:0.5462 F1_train:0.5969 AUC_train:0.6482\n",
      "acc_val:0.6300 pre_val:0.6066 recall_val:0.7400 F1_val:0.666667 AUC_val:0.6820\n",
      "Epoch:0009\n",
      "acc_train:0.6267 pre_train:0.6525 recall_train:0.5935 F1_train:0.6216 AUC_train:0.6528\n",
      "acc_val:0.6500 pre_val:0.6316 recall_val:0.7200 F1_val:0.672897 AUC_val:0.6872\n",
      "Epoch:0010\n",
      "acc_train:0.6089 pre_train:0.6317 recall_train:0.5828 F1_train:0.6063 AUC_train:0.6465\n",
      "acc_val:0.6300 pre_val:0.6383 recall_val:0.6000 F1_val:0.618557 AUC_val:0.6848\n",
      "Epoch:0011\n",
      "acc_train:0.6300 pre_train:0.6473 recall_train:0.6237 F1_train:0.6353 AUC_train:0.6539\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6848\n",
      "Epoch:0012\n",
      "acc_train:0.6289 pre_train:0.6650 recall_train:0.5677 F1_train:0.6125 AUC_train:0.6456\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6824\n",
      "Epoch:0013\n",
      "acc_train:0.6144 pre_train:0.6245 recall_train:0.6366 F1_train:0.6305 AUC_train:0.6503\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6936\n",
      "Epoch:0014\n",
      "acc_train:0.6078 pre_train:0.6261 recall_train:0.5978 F1_train:0.6117 AUC_train:0.6424\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6972\n",
      "Epoch:0015\n",
      "acc_train:0.6500 pre_train:0.6856 recall_train:0.5957 F1_train:0.6375 AUC_train:0.6715\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.7020\n",
      "Epoch:0016\n",
      "acc_train:0.6389 pre_train:0.6591 recall_train:0.6237 F1_train:0.6409 AUC_train:0.6705\n",
      "acc_val:0.6500 pre_val:0.6829 recall_val:0.5600 F1_val:0.615385 AUC_val:0.7024\n",
      "Epoch:0017\n",
      "acc_train:0.6278 pre_train:0.6438 recall_train:0.6258 F1_train:0.6347 AUC_train:0.6653\n",
      "acc_val:0.6500 pre_val:0.6829 recall_val:0.5600 F1_val:0.615385 AUC_val:0.7052\n",
      "Epoch:0018\n",
      "acc_train:0.6156 pre_train:0.6291 recall_train:0.6237 F1_train:0.6263 AUC_train:0.6604\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.7112\n",
      "Epoch:0019\n",
      "acc_train:0.6633 pre_train:0.6919 recall_train:0.6280 F1_train:0.6584 AUC_train:0.6739\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7124\n",
      "Epoch:0020\n",
      "acc_train:0.6333 pre_train:0.6658 recall_train:0.5828 F1_train:0.6216 AUC_train:0.6680\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7132\n",
      "Epoch:0021\n",
      "acc_train:0.6278 pre_train:0.6305 recall_train:0.6753 F1_train:0.6521 AUC_train:0.6725\n",
      "acc_val:0.6500 pre_val:0.6829 recall_val:0.5600 F1_val:0.615385 AUC_val:0.7164\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0022\n",
      "acc_train:0.6322 pre_train:0.6642 recall_train:0.5828 F1_train:0.6208 AUC_train:0.6627\n",
      "acc_val:0.6500 pre_val:0.6923 recall_val:0.5400 F1_val:0.606742 AUC_val:0.7228\n",
      "Epoch:0023\n",
      "acc_train:0.6333 pre_train:0.6331 recall_train:0.6903 F1_train:0.6605 AUC_train:0.6736\n",
      "acc_val:0.6500 pre_val:0.6829 recall_val:0.5600 F1_val:0.615385 AUC_val:0.7288\n",
      "Epoch:0024\n",
      "acc_train:0.6344 pre_train:0.6518 recall_train:0.6280 F1_train:0.6396 AUC_train:0.6978\n",
      "acc_val:0.6500 pre_val:0.6829 recall_val:0.5600 F1_val:0.615385 AUC_val:0.7400\n",
      "Epoch:0025\n",
      "acc_train:0.6544 pre_train:0.6878 recall_train:0.6065 F1_train:0.6446 AUC_train:0.6946\n",
      "acc_val:0.6500 pre_val:0.6923 recall_val:0.5400 F1_val:0.606742 AUC_val:0.7488\n",
      "Epoch:0026\n",
      "acc_train:0.6211 pre_train:0.6325 recall_train:0.6366 F1_train:0.6345 AUC_train:0.6758\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7660\n",
      "Epoch:0027\n",
      "acc_train:0.6378 pre_train:0.6451 recall_train:0.6645 F1_train:0.6547 AUC_train:0.6967\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7708\n",
      "Epoch:0028\n",
      "acc_train:0.6389 pre_train:0.6628 recall_train:0.6129 F1_train:0.6369 AUC_train:0.6923\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7736\n",
      "Epoch:0029\n",
      "acc_train:0.6489 pre_train:0.6737 recall_train:0.6215 F1_train:0.6465 AUC_train:0.7261\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7828\n",
      "Epoch:0030\n",
      "acc_train:0.6444 pre_train:0.6808 recall_train:0.5871 F1_train:0.6305 AUC_train:0.7208\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7884\n",
      "Epoch:0031\n",
      "acc_train:0.6589 pre_train:0.6872 recall_train:0.6237 F1_train:0.6539 AUC_train:0.7123\n",
      "acc_val:0.6900 pre_val:0.7568 recall_val:0.5600 F1_val:0.643678 AUC_val:0.7904\n",
      "Epoch:0032\n",
      "acc_train:0.6844 pre_train:0.7062 recall_train:0.6667 F1_train:0.6858 AUC_train:0.7594\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7932\n",
      "Epoch:0033\n",
      "acc_train:0.6489 pre_train:0.6877 recall_train:0.5871 F1_train:0.6334 AUC_train:0.7444\n",
      "acc_val:0.6900 pre_val:0.7714 recall_val:0.5400 F1_val:0.635294 AUC_val:0.8016\n",
      "Epoch:0034\n",
      "acc_train:0.6622 pre_train:0.6868 recall_train:0.6366 F1_train:0.6607 AUC_train:0.7559\n",
      "acc_val:0.7100 pre_val:0.8000 recall_val:0.5600 F1_val:0.658824 AUC_val:0.8156\n",
      "Epoch:0035\n",
      "acc_train:0.6933 pre_train:0.7077 recall_train:0.6925 F1_train:0.7000 AUC_train:0.7811\n",
      "acc_val:0.7200 pre_val:0.8235 recall_val:0.5600 F1_val:0.666667 AUC_val:0.8296\n",
      "Epoch:0036\n",
      "acc_train:0.7133 pre_train:0.7413 recall_train:0.6839 F1_train:0.7114 AUC_train:0.8073\n",
      "acc_val:0.7200 pre_val:0.8235 recall_val:0.5600 F1_val:0.666667 AUC_val:0.8460\n",
      "Epoch:0037\n",
      "acc_train:0.7356 pre_train:0.7551 recall_train:0.7226 F1_train:0.7385 AUC_train:0.8261\n",
      "acc_val:0.7200 pre_val:0.8438 recall_val:0.5400 F1_val:0.658537 AUC_val:0.8460\n",
      "Epoch:0038\n",
      "acc_train:0.7411 pre_train:0.8053 recall_train:0.6581 F1_train:0.7243 AUC_train:0.8513\n",
      "acc_val:0.7200 pre_val:0.8438 recall_val:0.5400 F1_val:0.658537 AUC_val:0.8620\n",
      "Epoch:0039\n",
      "acc_train:0.7478 pre_train:0.7833 recall_train:0.7075 F1_train:0.7435 AUC_train:0.8508\n",
      "acc_val:0.7400 pre_val:0.8529 recall_val:0.5800 F1_val:0.690476 AUC_val:0.8596\n",
      "Epoch:0040\n",
      "acc_train:0.7644 pre_train:0.8355 recall_train:0.6774 F1_train:0.7482 AUC_train:0.8742\n",
      "acc_val:0.8300 pre_val:0.9231 recall_val:0.7200 F1_val:0.808989 AUC_val:0.8696\n",
      "Epoch:0041\n",
      "acc_train:0.7656 pre_train:0.8307 recall_train:0.6860 F1_train:0.7515 AUC_train:0.8830\n",
      "acc_val:0.7900 pre_val:0.8718 recall_val:0.6800 F1_val:0.764045 AUC_val:0.8536\n",
      "Epoch:0042\n",
      "acc_train:0.7856 pre_train:0.8434 recall_train:0.7183 F1_train:0.7758 AUC_train:0.8872\n",
      "acc_val:0.8000 pre_val:0.8750 recall_val:0.7000 F1_val:0.777778 AUC_val:0.8612\n",
      "Epoch:0043\n",
      "acc_train:0.8100 pre_train:0.8311 recall_train:0.7935 F1_train:0.8119 AUC_train:0.8836\n",
      "acc_val:0.7900 pre_val:0.8222 recall_val:0.7400 F1_val:0.778947 AUC_val:0.8576\n",
      "Epoch:0044\n",
      "acc_train:0.8067 pre_train:0.8628 recall_train:0.7441 F1_train:0.7991 AUC_train:0.9094\n",
      "acc_val:0.8400 pre_val:0.8542 recall_val:0.8200 F1_val:0.836735 AUC_val:0.8664\n",
      "Epoch:0045\n",
      "acc_train:0.8700 pre_train:0.8372 recall_train:0.9290 F1_train:0.8807 AUC_train:0.9356\n",
      "acc_val:0.8400 pre_val:0.8542 recall_val:0.8200 F1_val:0.836735 AUC_val:0.8696\n",
      "Epoch:0046\n",
      "acc_train:0.8700 pre_train:0.8783 recall_train:0.8688 F1_train:0.8735 AUC_train:0.9248\n",
      "acc_val:0.8300 pre_val:0.8367 recall_val:0.8200 F1_val:0.828283 AUC_val:0.8704\n",
      "Epoch:0047\n",
      "acc_train:0.8811 pre_train:0.8496 recall_train:0.9355 F1_train:0.8905 AUC_train:0.9479\n",
      "acc_val:0.8300 pre_val:0.8235 recall_val:0.8400 F1_val:0.831683 AUC_val:0.8820\n",
      "Epoch:0048\n",
      "acc_train:0.8767 pre_train:0.8444 recall_train:0.9333 F1_train:0.8866 AUC_train:0.9356\n",
      "acc_val:0.8200 pre_val:0.8077 recall_val:0.8400 F1_val:0.823529 AUC_val:0.8836\n",
      "Epoch:0049\n",
      "acc_train:0.9011 pre_train:0.8821 recall_train:0.9333 F1_train:0.9070 AUC_train:0.9518\n",
      "acc_val:0.8000 pre_val:0.7586 recall_val:0.8800 F1_val:0.814815 AUC_val:0.8932\n",
      "Epoch:0050\n",
      "acc_train:0.9033 pre_train:0.8765 recall_train:0.9462 F1_train:0.9100 AUC_train:0.9561\n",
      "acc_val:0.7900 pre_val:0.7458 recall_val:0.8800 F1_val:0.807339 AUC_val:0.8952\n",
      "Epoch:0051\n",
      "acc_train:0.9022 pre_train:0.8733 recall_train:0.9484 F1_train:0.9093 AUC_train:0.9580\n",
      "acc_val:0.8200 pre_val:0.7857 recall_val:0.8800 F1_val:0.830189 AUC_val:0.9012\n",
      "Epoch:0052\n",
      "acc_train:0.8989 pre_train:0.8528 recall_train:0.9720 F1_train:0.9085 AUC_train:0.9656\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.8984\n",
      "Epoch:0053\n",
      "acc_train:0.8967 pre_train:0.8536 recall_train:0.9656 F1_train:0.9062 AUC_train:0.9627\n",
      "acc_val:0.8400 pre_val:0.8148 recall_val:0.8800 F1_val:0.846154 AUC_val:0.9012\n",
      "Epoch:0054\n",
      "acc_train:0.9033 pre_train:0.8593 recall_train:0.9720 F1_train:0.9122 AUC_train:0.9723\n",
      "acc_val:0.8300 pre_val:0.8113 recall_val:0.8600 F1_val:0.834951 AUC_val:0.8968\n",
      "Epoch:0055\n",
      "acc_train:0.9267 pre_train:0.8950 recall_train:0.9720 F1_train:0.9320 AUC_train:0.9671\n",
      "acc_val:0.8400 pre_val:0.8269 recall_val:0.8600 F1_val:0.843137 AUC_val:0.8976\n",
      "Epoch:0056\n",
      "acc_train:0.9156 pre_train:0.8867 recall_train:0.9591 F1_train:0.9215 AUC_train:0.9658\n",
      "acc_val:0.8600 pre_val:0.8462 recall_val:0.8800 F1_val:0.862745 AUC_val:0.9008\n",
      "Epoch:0057\n",
      "acc_train:0.9100 pre_train:0.8721 recall_train:0.9677 F1_train:0.9174 AUC_train:0.9661\n",
      "acc_val:0.8600 pre_val:0.8462 recall_val:0.8800 F1_val:0.862745 AUC_val:0.9080\n",
      "Epoch:0058\n",
      "acc_train:0.9278 pre_train:0.8922 recall_train:0.9785 F1_train:0.9333 AUC_train:0.9791\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9116\n",
      "Epoch:0059\n",
      "acc_train:0.9333 pre_train:0.9010 recall_train:0.9785 F1_train:0.9381 AUC_train:0.9708\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9128\n",
      "Epoch:0060\n",
      "acc_train:0.9244 pre_train:0.8885 recall_train:0.9763 F1_train:0.9303 AUC_train:0.9784\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9180\n",
      "Epoch:0061\n",
      "acc_train:0.9367 pre_train:0.9080 recall_train:0.9763 F1_train:0.9409 AUC_train:0.9781\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9148\n",
      "Epoch:0062\n",
      "acc_train:0.9411 pre_train:0.9137 recall_train:0.9785 F1_train:0.9450 AUC_train:0.9817\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9124\n",
      "Epoch:0063\n",
      "acc_train:0.9356 pre_train:0.9062 recall_train:0.9763 F1_train:0.9400 AUC_train:0.9836\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9064\n",
      "Epoch:0064\n",
      "acc_train:0.9400 pre_train:0.9152 recall_train:0.9742 F1_train:0.9437 AUC_train:0.9838\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9092\n",
      "Epoch:0065\n",
      "acc_train:0.9467 pre_train:0.9246 recall_train:0.9763 F1_train:0.9498 AUC_train:0.9837\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9088\n",
      "Epoch:0066\n",
      "acc_train:0.9500 pre_train:0.9234 recall_train:0.9849 F1_train:0.9532 AUC_train:0.9837\n",
      "acc_val:0.8200 pre_val:0.7857 recall_val:0.8800 F1_val:0.830189 AUC_val:0.9112\n",
      "Epoch:0067\n",
      "acc_train:0.9433 pre_train:0.9207 recall_train:0.9742 F1_train:0.9467 AUC_train:0.9797\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9160\n",
      "Epoch:0068\n",
      "acc_train:0.9522 pre_train:0.9324 recall_train:0.9785 F1_train:0.9549 AUC_train:0.9858\n",
      "acc_val:0.8400 pre_val:0.7931 recall_val:0.9200 F1_val:0.851852 AUC_val:0.9204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0069\n",
      "acc_train:0.9522 pre_train:0.9306 recall_train:0.9806 F1_train:0.9550 AUC_train:0.9801\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9248\n",
      "Epoch:0070\n",
      "acc_train:0.9533 pre_train:0.9273 recall_train:0.9871 F1_train:0.9563 AUC_train:0.9871\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9224\n",
      "Epoch:0071\n",
      "acc_train:0.9544 pre_train:0.9274 recall_train:0.9892 F1_train:0.9573 AUC_train:0.9917\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9220\n",
      "Epoch:0072\n",
      "acc_train:0.9478 pre_train:0.9147 recall_train:0.9914 F1_train:0.9515 AUC_train:0.9896\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9228\n",
      "Epoch:0073\n",
      "acc_train:0.9578 pre_train:0.9366 recall_train:0.9849 F1_train:0.9602 AUC_train:0.9885\n",
      "acc_val:0.8400 pre_val:0.8148 recall_val:0.8800 F1_val:0.846154 AUC_val:0.9120\n",
      "Epoch:0074\n",
      "acc_train:0.9500 pre_train:0.9321 recall_train:0.9742 F1_train:0.9527 AUC_train:0.9831\n",
      "acc_val:0.8500 pre_val:0.8302 recall_val:0.8800 F1_val:0.854369 AUC_val:0.9184\n",
      "Epoch:0075\n",
      "acc_train:0.9511 pre_train:0.9322 recall_train:0.9763 F1_train:0.9538 AUC_train:0.9863\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9232\n",
      "Epoch:0076\n",
      "acc_train:0.9567 pre_train:0.9401 recall_train:0.9785 F1_train:0.9589 AUC_train:0.9895\n",
      "acc_val:0.8000 pre_val:0.7500 recall_val:0.9000 F1_val:0.818182 AUC_val:0.9204\n",
      "Epoch:0077\n",
      "acc_train:0.9533 pre_train:0.9308 recall_train:0.9828 F1_train:0.9561 AUC_train:0.9939\n",
      "acc_val:0.8000 pre_val:0.7419 recall_val:0.9200 F1_val:0.821429 AUC_val:0.9188\n",
      "Epoch:0078\n",
      "acc_train:0.9611 pre_train:0.9461 recall_train:0.9806 F1_train:0.9630 AUC_train:0.9938\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.9200\n",
      "Epoch:0079\n",
      "acc_train:0.9678 pre_train:0.9467 recall_train:0.9935 F1_train:0.9696 AUC_train:0.9927\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9136\n",
      "Epoch:0080\n",
      "acc_train:0.9544 pre_train:0.9291 recall_train:0.9871 F1_train:0.9572 AUC_train:0.9916\n",
      "acc_val:0.8100 pre_val:0.7719 recall_val:0.8800 F1_val:0.822430 AUC_val:0.9160\n",
      "Epoch:0081\n",
      "acc_train:0.9600 pre_train:0.9478 recall_train:0.9763 F1_train:0.9619 AUC_train:0.9893\n",
      "acc_val:0.7900 pre_val:0.7377 recall_val:0.9000 F1_val:0.810811 AUC_val:0.9156\n",
      "Epoch:0082\n",
      "acc_train:0.9711 pre_train:0.9526 recall_train:0.9935 F1_train:0.9726 AUC_train:0.9892\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.9132\n",
      "Epoch:0083\n",
      "acc_train:0.9633 pre_train:0.9408 recall_train:0.9914 F1_train:0.9654 AUC_train:0.9952\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.9112\n",
      "Epoch:0084\n",
      "acc_train:0.9733 pre_train:0.9565 recall_train:0.9935 F1_train:0.9747 AUC_train:0.9959\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.9112\n",
      "Epoch:0085\n",
      "acc_train:0.9667 pre_train:0.9598 recall_train:0.9763 F1_train:0.9680 AUC_train:0.9948\n",
      "acc_val:0.8200 pre_val:0.7667 recall_val:0.9200 F1_val:0.836364 AUC_val:0.9110\n",
      "Epoch:0086\n",
      "acc_train:0.9733 pre_train:0.9565 recall_train:0.9935 F1_train:0.9747 AUC_train:0.9918\n",
      "acc_val:0.8100 pre_val:0.7719 recall_val:0.8800 F1_val:0.822430 AUC_val:0.9088\n",
      "Epoch:0087\n",
      "acc_train:0.9789 pre_train:0.9665 recall_train:0.9935 F1_train:0.9799 AUC_train:0.9957\n",
      "acc_val:0.8100 pre_val:0.7627 recall_val:0.9000 F1_val:0.825688 AUC_val:0.9098\n",
      "Epoch:0088\n",
      "acc_train:0.9767 pre_train:0.9644 recall_train:0.9914 F1_train:0.9777 AUC_train:0.9963\n",
      "acc_val:0.8000 pre_val:0.7419 recall_val:0.9200 F1_val:0.821429 AUC_val:0.9098\n",
      "Epoch:0089\n",
      "acc_train:0.9756 pre_train:0.9624 recall_train:0.9914 F1_train:0.9767 AUC_train:0.9960\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.9060\n",
      "Epoch:0090\n",
      "acc_train:0.9789 pre_train:0.9745 recall_train:0.9849 F1_train:0.9797 AUC_train:0.9940\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9068\n",
      "Epoch:0091\n",
      "acc_train:0.9822 pre_train:0.9766 recall_train:0.9892 F1_train:0.9829 AUC_train:0.9986\n",
      "acc_val:0.7600 pre_val:0.6970 recall_val:0.9200 F1_val:0.793103 AUC_val:0.9070\n",
      "Epoch:0092\n",
      "acc_train:0.9744 pre_train:0.9623 recall_train:0.9892 F1_train:0.9756 AUC_train:0.9973\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.9136\n",
      "Epoch:0093\n",
      "acc_train:0.9789 pre_train:0.9745 recall_train:0.9849 F1_train:0.9797 AUC_train:0.9968\n",
      "acc_val:0.8000 pre_val:0.7500 recall_val:0.9000 F1_val:0.818182 AUC_val:0.9140\n",
      "Epoch:0094\n",
      "acc_train:0.9822 pre_train:0.9726 recall_train:0.9935 F1_train:0.9830 AUC_train:0.9971\n",
      "acc_val:0.8100 pre_val:0.7719 recall_val:0.8800 F1_val:0.822430 AUC_val:0.9112\n",
      "Epoch:0095\n",
      "acc_train:0.9833 pre_train:0.9787 recall_train:0.9892 F1_train:0.9840 AUC_train:0.9982\n",
      "acc_val:0.8100 pre_val:0.7719 recall_val:0.8800 F1_val:0.822430 AUC_val:0.9112\n",
      "Early Stopping!!! epoch：94\n",
      " Starting the 1-5 Fold:：\n",
      "Fitting estimator with 131503 features.\n",
      "Fitting estimator with 121503 features.\n",
      "Fitting estimator with 111503 features.\n",
      "Fitting estimator with 101503 features.\n",
      "Fitting estimator with 91503 features.\n",
      "Fitting estimator with 81503 features.\n",
      "Fitting estimator with 71503 features.\n",
      "Fitting estimator with 61503 features.\n",
      "Fitting estimator with 51503 features.\n",
      "Fitting estimator with 41503 features.\n",
      "Fitting estimator with 31503 features.\n",
      "Fitting estimator with 21503 features.\n",
      "Fitting estimator with 11503 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001\n",
      "acc_train:0.5122 pre_train:0.5253 recall_train:0.5806 F1_train:0.5516 AUC_train:0.5105\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.3324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0002\n",
      "acc_train:0.5344 pre_train:0.5464 recall_train:0.5828 F1_train:0.5640 AUC_train:0.5424\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.3520\n",
      "Epoch:0003\n",
      "acc_train:0.5944 pre_train:0.6202 recall_train:0.5548 F1_train:0.5857 AUC_train:0.6073\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.1200 F1_val:0.193548 AUC_val:0.3584\n",
      "Epoch:0004\n",
      "acc_train:0.5367 pre_train:0.5517 recall_train:0.5505 F1_train:0.5511 AUC_train:0.5573\n",
      "acc_val:0.5000 pre_val:0.5000 recall_val:0.2800 F1_val:0.358974 AUC_val:0.4352\n",
      "Epoch:0005\n",
      "acc_train:0.6022 pre_train:0.6354 recall_train:0.5398 F1_train:0.5837 AUC_train:0.6154\n",
      "acc_val:0.6300 pre_val:0.6275 recall_val:0.6400 F1_val:0.633663 AUC_val:0.6004\n",
      "Epoch:0006\n",
      "acc_train:0.5722 pre_train:0.5985 recall_train:0.5226 F1_train:0.5580 AUC_train:0.5949\n",
      "acc_val:0.6500 pre_val:0.6364 recall_val:0.7000 F1_val:0.666667 AUC_val:0.6584\n",
      "Epoch:0007\n",
      "acc_train:0.5956 pre_train:0.6499 recall_train:0.4710 F1_train:0.5461 AUC_train:0.6279\n",
      "acc_val:0.6400 pre_val:0.6296 recall_val:0.6800 F1_val:0.653846 AUC_val:0.6816\n",
      "Epoch:0008\n",
      "acc_train:0.5744 pre_train:0.6062 recall_train:0.5032 F1_train:0.5499 AUC_train:0.6031\n",
      "acc_val:0.6300 pre_val:0.6383 recall_val:0.6000 F1_val:0.618557 AUC_val:0.6828\n",
      "Epoch:0009\n",
      "acc_train:0.6011 pre_train:0.6380 recall_train:0.5269 F1_train:0.5771 AUC_train:0.6362\n",
      "acc_val:0.6400 pre_val:0.6522 recall_val:0.6000 F1_val:0.625000 AUC_val:0.6848\n",
      "Epoch:0010\n",
      "acc_train:0.6178 pre_train:0.6532 recall_train:0.5548 F1_train:0.6000 AUC_train:0.6347\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6864\n",
      "Epoch:0011\n",
      "acc_train:0.6044 pre_train:0.6446 recall_train:0.5226 F1_train:0.5772 AUC_train:0.6394\n",
      "acc_val:0.6400 pre_val:0.6591 recall_val:0.5800 F1_val:0.617021 AUC_val:0.6872\n",
      "Epoch:0012\n",
      "acc_train:0.5944 pre_train:0.6263 recall_train:0.5333 F1_train:0.5761 AUC_train:0.6206\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6876\n",
      "Epoch:0013\n",
      "acc_train:0.6233 pre_train:0.6529 recall_train:0.5785 F1_train:0.6135 AUC_train:0.6468\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6872\n",
      "Epoch:0014\n",
      "acc_train:0.6056 pre_train:0.6228 recall_train:0.6000 F1_train:0.6112 AUC_train:0.6401\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6872\n",
      "Epoch:0015\n",
      "acc_train:0.6200 pre_train:0.6526 recall_train:0.5656 F1_train:0.6060 AUC_train:0.6438\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6864\n",
      "Epoch:0016\n",
      "acc_train:0.6233 pre_train:0.6529 recall_train:0.5785 F1_train:0.6135 AUC_train:0.6562\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6876\n",
      "Epoch:0017\n",
      "acc_train:0.6156 pre_train:0.6337 recall_train:0.6065 F1_train:0.6198 AUC_train:0.6386\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6884\n",
      "Epoch:0018\n",
      "acc_train:0.5978 pre_train:0.6157 recall_train:0.5892 F1_train:0.6022 AUC_train:0.6348\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6896\n",
      "Epoch:0019\n",
      "acc_train:0.6244 pre_train:0.6530 recall_train:0.5828 F1_train:0.6159 AUC_train:0.6683\n",
      "acc_val:0.6400 pre_val:0.6591 recall_val:0.5800 F1_val:0.617021 AUC_val:0.6916\n",
      "Epoch:0020\n",
      "acc_train:0.6033 pre_train:0.6116 recall_train:0.6366 F1_train:0.6238 AUC_train:0.6511\n",
      "acc_val:0.6300 pre_val:0.6444 recall_val:0.5800 F1_val:0.610526 AUC_val:0.6940\n",
      "Epoch:0021\n",
      "acc_train:0.6089 pre_train:0.6253 recall_train:0.6065 F1_train:0.6157 AUC_train:0.6464\n",
      "acc_val:0.6200 pre_val:0.6304 recall_val:0.5800 F1_val:0.604167 AUC_val:0.6944\n",
      "Epoch:0022\n",
      "acc_train:0.6067 pre_train:0.6049 recall_train:0.6882 F1_train:0.6439 AUC_train:0.6534\n",
      "acc_val:0.6300 pre_val:0.6226 recall_val:0.6600 F1_val:0.640777 AUC_val:0.6992\n",
      "Epoch:0023\n",
      "acc_train:0.6144 pre_train:0.6089 recall_train:0.7097 F1_train:0.6554 AUC_train:0.6451\n",
      "acc_val:0.6300 pre_val:0.6226 recall_val:0.6600 F1_val:0.640777 AUC_val:0.7076\n",
      "Epoch:0024\n",
      "acc_train:0.6289 pre_train:0.6465 recall_train:0.6215 F1_train:0.6338 AUC_train:0.6757\n",
      "acc_val:0.6300 pre_val:0.6226 recall_val:0.6600 F1_val:0.640777 AUC_val:0.7228\n",
      "Epoch:0025\n",
      "acc_train:0.6333 pre_train:0.6369 recall_train:0.6753 F1_train:0.6555 AUC_train:0.6628\n",
      "acc_val:0.6500 pre_val:0.6471 recall_val:0.6600 F1_val:0.653465 AUC_val:0.7352\n",
      "Epoch:0026\n",
      "acc_train:0.6033 pre_train:0.6179 recall_train:0.6086 F1_train:0.6132 AUC_train:0.6469\n",
      "acc_val:0.6500 pre_val:0.6471 recall_val:0.6600 F1_val:0.653465 AUC_val:0.7360\n",
      "Epoch:0027\n",
      "acc_train:0.6044 pre_train:0.6101 recall_train:0.6495 F1_train:0.6292 AUC_train:0.6471\n",
      "acc_val:0.6400 pre_val:0.6346 recall_val:0.6600 F1_val:0.647059 AUC_val:0.7312\n",
      "Epoch:0028\n",
      "acc_train:0.6311 pre_train:0.6354 recall_train:0.6710 F1_train:0.6527 AUC_train:0.6734\n",
      "acc_val:0.6300 pre_val:0.6226 recall_val:0.6600 F1_val:0.640777 AUC_val:0.7156\n",
      "Epoch:0029\n",
      "acc_train:0.6156 pre_train:0.6183 recall_train:0.6688 F1_train:0.6426 AUC_train:0.6696\n",
      "acc_val:0.6200 pre_val:0.6200 recall_val:0.6200 F1_val:0.620000 AUC_val:0.6972\n",
      "Epoch:0030\n",
      "acc_train:0.6122 pre_train:0.6174 recall_train:0.6559 F1_train:0.6361 AUC_train:0.6590\n",
      "acc_val:0.6200 pre_val:0.6200 recall_val:0.6200 F1_val:0.620000 AUC_val:0.6908\n",
      "Epoch:0031\n",
      "acc_train:0.6111 pre_train:0.6292 recall_train:0.6022 F1_train:0.6154 AUC_train:0.6664\n",
      "acc_val:0.6200 pre_val:0.6250 recall_val:0.6000 F1_val:0.612245 AUC_val:0.6864\n",
      "Epoch:0032\n",
      "acc_train:0.6133 pre_train:0.6232 recall_train:0.6366 F1_train:0.6298 AUC_train:0.6569\n",
      "acc_val:0.6100 pre_val:0.6170 recall_val:0.5800 F1_val:0.597938 AUC_val:0.6852\n",
      "Epoch:0033\n",
      "acc_train:0.6322 pre_train:0.6463 recall_train:0.6366 F1_train:0.6414 AUC_train:0.6839\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.7000\n",
      "Epoch:0034\n",
      "acc_train:0.6500 pre_train:0.6659 recall_train:0.6473 F1_train:0.6565 AUC_train:0.6921\n",
      "acc_val:0.6400 pre_val:0.6591 recall_val:0.5800 F1_val:0.617021 AUC_val:0.7072\n",
      "Epoch:0035\n",
      "acc_train:0.6400 pre_train:0.6628 recall_train:0.6172 F1_train:0.6392 AUC_train:0.6698\n",
      "acc_val:0.6400 pre_val:0.6591 recall_val:0.5800 F1_val:0.617021 AUC_val:0.7172\n",
      "Epoch:0036\n",
      "acc_train:0.6433 pre_train:0.6706 recall_train:0.6086 F1_train:0.6381 AUC_train:0.6781\n",
      "acc_val:0.6500 pre_val:0.6744 recall_val:0.5800 F1_val:0.623656 AUC_val:0.7168\n",
      "Epoch:0037\n",
      "acc_train:0.6500 pre_train:0.6596 recall_train:0.6667 F1_train:0.6631 AUC_train:0.6930\n",
      "acc_val:0.6500 pre_val:0.6744 recall_val:0.5800 F1_val:0.623656 AUC_val:0.7180\n",
      "Epoch:0038\n",
      "acc_train:0.6278 pre_train:0.6555 recall_train:0.5892 F1_train:0.6206 AUC_train:0.6795\n",
      "acc_val:0.6500 pre_val:0.6744 recall_val:0.5800 F1_val:0.623656 AUC_val:0.7156\n",
      "Epoch:0039\n",
      "acc_train:0.6422 pre_train:0.6621 recall_train:0.6280 F1_train:0.6446 AUC_train:0.7095\n",
      "acc_val:0.6600 pre_val:0.6905 recall_val:0.5800 F1_val:0.630435 AUC_val:0.7120\n",
      "Epoch:0040\n",
      "acc_train:0.6456 pre_train:0.6730 recall_train:0.6108 F1_train:0.6404 AUC_train:0.7069\n",
      "acc_val:0.6600 pre_val:0.6905 recall_val:0.5800 F1_val:0.630435 AUC_val:0.7172\n",
      "Epoch:0041\n",
      "acc_train:0.6278 pre_train:0.6585 recall_train:0.5806 F1_train:0.6171 AUC_train:0.6868\n",
      "acc_val:0.6700 pre_val:0.7073 recall_val:0.5800 F1_val:0.637363 AUC_val:0.7160\n",
      "Epoch:0042\n",
      "acc_train:0.6511 pre_train:0.6720 recall_train:0.6344 F1_train:0.6527 AUC_train:0.7115\n",
      "acc_val:0.6600 pre_val:0.7000 recall_val:0.5600 F1_val:0.622222 AUC_val:0.7144\n",
      "Epoch:0043\n",
      "acc_train:0.6478 pre_train:0.6832 recall_train:0.5935 F1_train:0.6352 AUC_train:0.7157\n",
      "acc_val:0.6600 pre_val:0.7000 recall_val:0.5600 F1_val:0.622222 AUC_val:0.7160\n",
      "Epoch:0044\n",
      "acc_train:0.6522 pre_train:0.6751 recall_train:0.6301 F1_train:0.6518 AUC_train:0.7143\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.7284\n",
      "Epoch:0045\n",
      "acc_train:0.6556 pre_train:0.6972 recall_train:0.5892 F1_train:0.6387 AUC_train:0.7348\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.7364\n",
      "Epoch:0046\n",
      "acc_train:0.6500 pre_train:0.6659 recall_train:0.6473 F1_train:0.6565 AUC_train:0.7275\n",
      "acc_val:0.6500 pre_val:0.6829 recall_val:0.5600 F1_val:0.615385 AUC_val:0.7508\n",
      "Epoch:0047\n",
      "acc_train:0.6689 pre_train:0.6937 recall_train:0.6430 F1_train:0.6674 AUC_train:0.7376\n",
      "acc_val:0.6700 pre_val:0.7073 recall_val:0.5800 F1_val:0.637363 AUC_val:0.7616\n",
      "Epoch:0048\n",
      "acc_train:0.6978 pre_train:0.7249 recall_train:0.6688 F1_train:0.6957 AUC_train:0.7812\n",
      "acc_val:0.6800 pre_val:0.7250 recall_val:0.5800 F1_val:0.644444 AUC_val:0.7704\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0049\n",
      "acc_train:0.6900 pre_train:0.7039 recall_train:0.6903 F1_train:0.6971 AUC_train:0.7683\n",
      "acc_val:0.7200 pre_val:0.7500 recall_val:0.6600 F1_val:0.702128 AUC_val:0.7752\n",
      "Epoch:0050\n",
      "acc_train:0.7078 pre_train:0.7235 recall_train:0.7032 F1_train:0.7132 AUC_train:0.7657\n",
      "acc_val:0.7700 pre_val:0.7872 recall_val:0.7400 F1_val:0.762887 AUC_val:0.7888\n",
      "Epoch:0051\n",
      "acc_train:0.7000 pre_train:0.7349 recall_train:0.6559 F1_train:0.6932 AUC_train:0.7847\n",
      "acc_val:0.7800 pre_val:0.7692 recall_val:0.8000 F1_val:0.784314 AUC_val:0.8036\n",
      "Epoch:0052\n",
      "acc_train:0.7522 pre_train:0.7500 recall_train:0.7806 F1_train:0.7650 AUC_train:0.8180\n",
      "acc_val:0.7500 pre_val:0.7119 recall_val:0.8400 F1_val:0.770642 AUC_val:0.8104\n",
      "Epoch:0053\n",
      "acc_train:0.7500 pre_train:0.7381 recall_train:0.8000 F1_train:0.7678 AUC_train:0.8101\n",
      "acc_val:0.7300 pre_val:0.6825 recall_val:0.8600 F1_val:0.761062 AUC_val:0.8208\n",
      "Epoch:0054\n",
      "acc_train:0.7800 pre_train:0.7505 recall_train:0.8602 F1_train:0.8016 AUC_train:0.8308\n",
      "acc_val:0.7400 pre_val:0.6935 recall_val:0.8600 F1_val:0.767857 AUC_val:0.8240\n",
      "Epoch:0055\n",
      "acc_train:0.8011 pre_train:0.7967 recall_train:0.8258 F1_train:0.8110 AUC_train:0.8488\n",
      "acc_val:0.7600 pre_val:0.7241 recall_val:0.8400 F1_val:0.777778 AUC_val:0.8304\n",
      "Epoch:0056\n",
      "acc_train:0.7978 pre_train:0.7824 recall_train:0.8430 F1_train:0.8116 AUC_train:0.8490\n",
      "acc_val:0.7500 pre_val:0.7193 recall_val:0.8200 F1_val:0.766355 AUC_val:0.8328\n",
      "Epoch:0057\n",
      "acc_train:0.8167 pre_train:0.7885 recall_train:0.8817 F1_train:0.8325 AUC_train:0.8681\n",
      "acc_val:0.7500 pre_val:0.7193 recall_val:0.8200 F1_val:0.766355 AUC_val:0.8340\n",
      "Epoch:0058\n",
      "acc_train:0.8244 pre_train:0.7958 recall_train:0.8882 F1_train:0.8394 AUC_train:0.8783\n",
      "acc_val:0.7500 pre_val:0.7119 recall_val:0.8400 F1_val:0.770642 AUC_val:0.8280\n",
      "Epoch:0059\n",
      "acc_train:0.8111 pre_train:0.8172 recall_train:0.8172 F1_train:0.8172 AUC_train:0.8739\n",
      "acc_val:0.7800 pre_val:0.7333 recall_val:0.8800 F1_val:0.800000 AUC_val:0.8324\n",
      "Epoch:0060\n",
      "acc_train:0.8233 pre_train:0.7965 recall_train:0.8839 F1_train:0.8379 AUC_train:0.8759\n",
      "acc_val:0.7500 pre_val:0.6984 recall_val:0.8800 F1_val:0.778761 AUC_val:0.8420\n",
      "Epoch:0061\n",
      "acc_train:0.8567 pre_train:0.8360 recall_train:0.8989 F1_train:0.8663 AUC_train:0.9175\n",
      "acc_val:0.7200 pre_val:0.6618 recall_val:0.9000 F1_val:0.762712 AUC_val:0.8448\n",
      "Epoch:0062\n",
      "acc_train:0.8644 pre_train:0.8410 recall_train:0.9097 F1_train:0.8740 AUC_train:0.9036\n",
      "acc_val:0.7500 pre_val:0.6761 recall_val:0.9600 F1_val:0.793388 AUC_val:0.8500\n",
      "Epoch:0063\n",
      "acc_train:0.8500 pre_train:0.8149 recall_train:0.9183 F1_train:0.8635 AUC_train:0.8955\n",
      "acc_val:0.7600 pre_val:0.6857 recall_val:0.9600 F1_val:0.800000 AUC_val:0.8596\n",
      "Epoch:0064\n",
      "acc_train:0.8756 pre_train:0.8311 recall_train:0.9527 F1_train:0.8878 AUC_train:0.9252\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.8604\n",
      "Epoch:0065\n",
      "acc_train:0.8711 pre_train:0.8286 recall_train:0.9462 F1_train:0.8835 AUC_train:0.9175\n",
      "acc_val:0.7600 pre_val:0.7031 recall_val:0.9000 F1_val:0.789474 AUC_val:0.8596\n",
      "Epoch:0066\n",
      "acc_train:0.8822 pre_train:0.8459 recall_train:0.9441 F1_train:0.8923 AUC_train:0.9209\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.8632\n",
      "Epoch:0067\n",
      "acc_train:0.8911 pre_train:0.8577 recall_train:0.9462 F1_train:0.8998 AUC_train:0.9330\n",
      "acc_val:0.7500 pre_val:0.6812 recall_val:0.9400 F1_val:0.789916 AUC_val:0.8668\n",
      "Epoch:0068\n",
      "acc_train:0.8978 pre_train:0.8566 recall_train:0.9634 F1_train:0.9069 AUC_train:0.9301\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.8640\n",
      "Epoch:0069\n",
      "acc_train:0.8811 pre_train:0.8429 recall_train:0.9462 F1_train:0.8916 AUC_train:0.9259\n",
      "acc_val:0.7400 pre_val:0.6765 recall_val:0.9200 F1_val:0.779661 AUC_val:0.8684\n",
      "Epoch:0070\n",
      "acc_train:0.8911 pre_train:0.8456 recall_train:0.9656 F1_train:0.9016 AUC_train:0.9433\n",
      "acc_val:0.7400 pre_val:0.6667 recall_val:0.9600 F1_val:0.786885 AUC_val:0.8784\n",
      "Epoch:0071\n",
      "acc_train:0.9089 pre_train:0.8690 recall_train:0.9699 F1_train:0.9167 AUC_train:0.9461\n",
      "acc_val:0.6900 pre_val:0.6267 recall_val:0.9400 F1_val:0.752000 AUC_val:0.8760\n",
      "Epoch:0072\n",
      "acc_train:0.9122 pre_train:0.8712 recall_train:0.9742 F1_train:0.9198 AUC_train:0.9680\n",
      "acc_val:0.7200 pre_val:0.6486 recall_val:0.9600 F1_val:0.774194 AUC_val:0.8744\n",
      "Epoch:0073\n",
      "acc_train:0.9056 pre_train:0.8585 recall_train:0.9785 F1_train:0.9146 AUC_train:0.9598\n",
      "acc_val:0.7500 pre_val:0.6866 recall_val:0.9200 F1_val:0.786325 AUC_val:0.8828\n",
      "Epoch:0074\n",
      "acc_train:0.9022 pre_train:0.8632 recall_train:0.9634 F1_train:0.9106 AUC_train:0.9674\n",
      "acc_val:0.8100 pre_val:0.7541 recall_val:0.9200 F1_val:0.828829 AUC_val:0.8836\n",
      "Epoch:0075\n",
      "acc_train:0.9122 pre_train:0.8755 recall_train:0.9677 F1_train:0.9193 AUC_train:0.9596\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.8828\n",
      "Epoch:0076\n",
      "acc_train:0.9000 pre_train:0.8518 recall_train:0.9763 F1_train:0.9098 AUC_train:0.9620\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.8860\n",
      "Epoch:0077\n",
      "acc_train:0.9056 pre_train:0.8654 recall_train:0.9677 F1_train:0.9137 AUC_train:0.9625\n",
      "acc_val:0.8400 pre_val:0.7931 recall_val:0.9200 F1_val:0.851852 AUC_val:0.8908\n",
      "Epoch:0078\n",
      "acc_train:0.9278 pre_train:0.8922 recall_train:0.9785 F1_train:0.9333 AUC_train:0.9673\n",
      "acc_val:0.8400 pre_val:0.7931 recall_val:0.9200 F1_val:0.851852 AUC_val:0.8928\n",
      "Epoch:0079\n",
      "acc_train:0.9378 pre_train:0.9082 recall_train:0.9785 F1_train:0.9420 AUC_train:0.9780\n",
      "acc_val:0.8100 pre_val:0.7541 recall_val:0.9200 F1_val:0.828829 AUC_val:0.8952\n",
      "Epoch:0080\n",
      "acc_train:0.9356 pre_train:0.9030 recall_train:0.9806 F1_train:0.9402 AUC_train:0.9748\n",
      "acc_val:0.8200 pre_val:0.7667 recall_val:0.9200 F1_val:0.836364 AUC_val:0.8968\n",
      "Epoch:0081\n",
      "acc_train:0.9300 pre_train:0.8957 recall_train:0.9785 F1_train:0.9353 AUC_train:0.9735\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.8984\n",
      "Epoch:0082\n",
      "acc_train:0.9411 pre_train:0.9153 recall_train:0.9763 F1_train:0.9448 AUC_train:0.9731\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.8952\n",
      "Epoch:0083\n",
      "acc_train:0.9467 pre_train:0.9229 recall_train:0.9785 F1_train:0.9499 AUC_train:0.9759\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.8980\n",
      "Epoch:0084\n",
      "acc_train:0.9367 pre_train:0.9080 recall_train:0.9763 F1_train:0.9409 AUC_train:0.9729\n",
      "acc_val:0.8400 pre_val:0.7931 recall_val:0.9200 F1_val:0.851852 AUC_val:0.8992\n",
      "Epoch:0085\n",
      "acc_train:0.9433 pre_train:0.9242 recall_train:0.9699 F1_train:0.9465 AUC_train:0.9846\n",
      "acc_val:0.8000 pre_val:0.7419 recall_val:0.9200 F1_val:0.821429 AUC_val:0.9020\n",
      "Epoch:0086\n",
      "acc_train:0.9344 pre_train:0.9060 recall_train:0.9742 F1_train:0.9389 AUC_train:0.9803\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.9060\n",
      "Epoch:0087\n",
      "acc_train:0.9411 pre_train:0.9187 recall_train:0.9720 F1_train:0.9446 AUC_train:0.9804\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.9036\n",
      "Epoch:0088\n",
      "acc_train:0.9400 pre_train:0.9168 recall_train:0.9720 F1_train:0.9436 AUC_train:0.9785\n",
      "acc_val:0.8500 pre_val:0.8070 recall_val:0.9200 F1_val:0.859813 AUC_val:0.9024\n",
      "Epoch:0089\n",
      "acc_train:0.9478 pre_train:0.9300 recall_train:0.9720 F1_train:0.9506 AUC_train:0.9847\n",
      "acc_val:0.8000 pre_val:0.7419 recall_val:0.9200 F1_val:0.821429 AUC_val:0.9120\n",
      "Epoch:0090\n",
      "acc_train:0.9556 pre_train:0.9400 recall_train:0.9763 F1_train:0.9578 AUC_train:0.9820\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.9136\n",
      "Epoch:0091\n",
      "acc_train:0.9511 pre_train:0.9287 recall_train:0.9806 F1_train:0.9540 AUC_train:0.9858\n",
      "acc_val:0.7600 pre_val:0.6970 recall_val:0.9200 F1_val:0.793103 AUC_val:0.9120\n",
      "Epoch:0092\n",
      "acc_train:0.9500 pre_train:0.9251 recall_train:0.9828 F1_train:0.9531 AUC_train:0.9858\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.9128\n",
      "Epoch:0093\n",
      "acc_train:0.9489 pre_train:0.9302 recall_train:0.9742 F1_train:0.9517 AUC_train:0.9886\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9056\n",
      "Epoch:0094\n",
      "acc_train:0.9544 pre_train:0.9327 recall_train:0.9828 F1_train:0.9571 AUC_train:0.9919\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.8972\n",
      "Epoch:0095\n",
      "acc_train:0.9578 pre_train:0.9384 recall_train:0.9828 F1_train:0.9601 AUC_train:0.9895\n",
      "acc_val:0.7600 pre_val:0.6970 recall_val:0.9200 F1_val:0.793103 AUC_val:0.8944\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0096\n",
      "acc_train:0.9644 pre_train:0.9446 recall_train:0.9892 F1_train:0.9664 AUC_train:0.9912\n",
      "acc_val:0.7600 pre_val:0.6857 recall_val:0.9600 F1_val:0.800000 AUC_val:0.8820\n",
      "Epoch:0097\n",
      "acc_train:0.9633 pre_train:0.9426 recall_train:0.9892 F1_train:0.9654 AUC_train:0.9915\n",
      "acc_val:0.7300 pre_val:0.6533 recall_val:0.9800 F1_val:0.784000 AUC_val:0.8896\n",
      "Epoch:0098\n",
      "acc_train:0.9600 pre_train:0.9387 recall_train:0.9871 F1_train:0.9623 AUC_train:0.9940\n",
      "acc_val:0.7900 pre_val:0.7302 recall_val:0.9200 F1_val:0.814159 AUC_val:0.8876\n",
      "Epoch:0099\n",
      "acc_train:0.9622 pre_train:0.9480 recall_train:0.9806 F1_train:0.9641 AUC_train:0.9917\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.8760\n",
      "Epoch:0100\n",
      "acc_train:0.9567 pre_train:0.9383 recall_train:0.9806 F1_train:0.9590 AUC_train:0.9895\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.8924\n",
      "Epoch:0101\n",
      "acc_train:0.9689 pre_train:0.9487 recall_train:0.9935 F1_train:0.9706 AUC_train:0.9900\n",
      "acc_val:0.7800 pre_val:0.7258 recall_val:0.9000 F1_val:0.803571 AUC_val:0.9018\n",
      "Epoch:0102\n",
      "acc_train:0.9633 pre_train:0.9500 recall_train:0.9806 F1_train:0.9651 AUC_train:0.9905\n",
      "acc_val:0.7700 pre_val:0.7077 recall_val:0.9200 F1_val:0.800000 AUC_val:0.9038\n",
      "Epoch:0103\n",
      "acc_train:0.9644 pre_train:0.9482 recall_train:0.9849 F1_train:0.9662 AUC_train:0.9916\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9032\n",
      "Epoch:0104\n",
      "acc_train:0.9711 pre_train:0.9582 recall_train:0.9871 F1_train:0.9725 AUC_train:0.9931\n",
      "acc_val:0.8100 pre_val:0.7627 recall_val:0.9000 F1_val:0.825688 AUC_val:0.9008\n",
      "Epoch:0105\n",
      "acc_train:0.9578 pre_train:0.9533 recall_train:0.9656 F1_train:0.9594 AUC_train:0.9938\n",
      "acc_val:0.7900 pre_val:0.7377 recall_val:0.9000 F1_val:0.810811 AUC_val:0.9032\n",
      "Epoch:0106\n",
      "acc_train:0.9589 pre_train:0.9496 recall_train:0.9720 F1_train:0.9607 AUC_train:0.9905\n",
      "acc_val:0.7700 pre_val:0.7143 recall_val:0.9000 F1_val:0.796460 AUC_val:0.9012\n",
      "Epoch:0107\n",
      "acc_train:0.9756 pre_train:0.9663 recall_train:0.9871 F1_train:0.9766 AUC_train:0.9939\n",
      "acc_val:0.7600 pre_val:0.7031 recall_val:0.9000 F1_val:0.789474 AUC_val:0.8960\n",
      "Epoch:0108\n",
      "acc_train:0.9700 pre_train:0.9601 recall_train:0.9828 F1_train:0.9713 AUC_train:0.9937\n",
      "acc_val:0.7600 pre_val:0.7031 recall_val:0.9000 F1_val:0.789474 AUC_val:0.9032\n",
      "Epoch:0109\n",
      "acc_train:0.9644 pre_train:0.9482 recall_train:0.9849 F1_train:0.9662 AUC_train:0.9895\n",
      "acc_val:0.7600 pre_val:0.7031 recall_val:0.9000 F1_val:0.789474 AUC_val:0.9048\n",
      "Epoch:0110\n",
      "acc_train:0.9756 pre_train:0.9703 recall_train:0.9828 F1_train:0.9765 AUC_train:0.9967\n",
      "acc_val:0.7300 pre_val:0.6716 recall_val:0.9000 F1_val:0.769231 AUC_val:0.9020\n",
      "Epoch:0111\n",
      "acc_train:0.9733 pre_train:0.9642 recall_train:0.9849 F1_train:0.9745 AUC_train:0.9923\n",
      "acc_val:0.7400 pre_val:0.6818 recall_val:0.9000 F1_val:0.775862 AUC_val:0.9080\n",
      "Epoch:0112\n",
      "acc_train:0.9811 pre_train:0.9726 recall_train:0.9914 F1_train:0.9819 AUC_train:0.9981\n",
      "acc_val:0.7400 pre_val:0.6714 recall_val:0.9400 F1_val:0.783333 AUC_val:0.9072\n",
      "Epoch:0113\n",
      "acc_train:0.9756 pre_train:0.9663 recall_train:0.9871 F1_train:0.9766 AUC_train:0.9933\n",
      "acc_val:0.7500 pre_val:0.6866 recall_val:0.9200 F1_val:0.786325 AUC_val:0.9072\n",
      "Epoch:0114\n",
      "acc_train:0.9789 pre_train:0.9627 recall_train:0.9978 F1_train:0.9799 AUC_train:0.9912\n",
      "acc_val:0.7600 pre_val:0.7031 recall_val:0.9000 F1_val:0.789474 AUC_val:0.8948\n",
      "Epoch:0115\n",
      "acc_train:0.9778 pre_train:0.9684 recall_train:0.9892 F1_train:0.9787 AUC_train:0.9923\n",
      "acc_val:0.7900 pre_val:0.7377 recall_val:0.9000 F1_val:0.810811 AUC_val:0.8956\n",
      "Epoch:0116\n",
      "acc_train:0.9689 pre_train:0.9600 recall_train:0.9806 F1_train:0.9702 AUC_train:0.9958\n",
      "acc_val:0.7700 pre_val:0.7143 recall_val:0.9000 F1_val:0.796460 AUC_val:0.9004\n",
      "Epoch:0117\n",
      "acc_train:0.9778 pre_train:0.9724 recall_train:0.9849 F1_train:0.9786 AUC_train:0.9935\n",
      "acc_val:0.7600 pre_val:0.6970 recall_val:0.9200 F1_val:0.793103 AUC_val:0.9028\n",
      "Epoch:0118\n",
      "acc_train:0.9800 pre_train:0.9765 recall_train:0.9849 F1_train:0.9807 AUC_train:0.9952\n",
      "acc_val:0.7600 pre_val:0.6970 recall_val:0.9200 F1_val:0.793103 AUC_val:0.8984\n",
      "Epoch:0119\n",
      "acc_train:0.9733 pre_train:0.9642 recall_train:0.9849 F1_train:0.9745 AUC_train:0.9953\n",
      "acc_val:0.7700 pre_val:0.7143 recall_val:0.9000 F1_val:0.796460 AUC_val:0.9000\n",
      "Epoch:0120\n",
      "acc_train:0.9778 pre_train:0.9645 recall_train:0.9935 F1_train:0.9788 AUC_train:0.9930\n",
      "acc_val:0.7700 pre_val:0.7143 recall_val:0.9000 F1_val:0.796460 AUC_val:0.9060\n",
      "Epoch:0121\n",
      "acc_train:0.9711 pre_train:0.9621 recall_train:0.9828 F1_train:0.9723 AUC_train:0.9959\n",
      "acc_val:0.7800 pre_val:0.7258 recall_val:0.9000 F1_val:0.803571 AUC_val:0.9092\n",
      "Epoch:0122\n",
      "acc_train:0.9767 pre_train:0.9703 recall_train:0.9849 F1_train:0.9776 AUC_train:0.9913\n",
      "acc_val:0.7600 pre_val:0.7031 recall_val:0.9000 F1_val:0.789474 AUC_val:0.9180\n",
      "Epoch:0123\n",
      "acc_train:0.9733 pre_train:0.9623 recall_train:0.9871 F1_train:0.9745 AUC_train:0.9932\n",
      "acc_val:0.7500 pre_val:0.6923 recall_val:0.9000 F1_val:0.782609 AUC_val:0.9216\n",
      "Epoch:0124\n",
      "acc_train:0.9867 pre_train:0.9768 recall_train:0.9978 F1_train:0.9872 AUC_train:0.9957\n",
      "acc_val:0.7400 pre_val:0.6818 recall_val:0.9000 F1_val:0.775862 AUC_val:0.9220\n",
      "Early Stopping!!! epoch：123\n",
      " Starting the 1-6 Fold:：\n",
      "Fitting estimator with 131503 features.\n",
      "Fitting estimator with 121503 features.\n",
      "Fitting estimator with 111503 features.\n",
      "Fitting estimator with 101503 features.\n",
      "Fitting estimator with 91503 features.\n",
      "Fitting estimator with 81503 features.\n",
      "Fitting estimator with 71503 features.\n",
      "Fitting estimator with 61503 features.\n",
      "Fitting estimator with 51503 features.\n",
      "Fitting estimator with 41503 features.\n",
      "Fitting estimator with 31503 features.\n",
      "Fitting estimator with 21503 features.\n",
      "Fitting estimator with 11503 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001\n",
      "acc_train:0.4444 pre_train:0.4736 recall_train:0.6753 F1_train:0.5567 AUC_train:0.4203\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.3168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0002\n",
      "acc_train:0.4689 pre_train:0.4888 recall_train:0.6086 F1_train:0.5421 AUC_train:0.4467\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.3224\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KSB\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0003\n",
      "acc_train:0.4756 pre_train:0.4936 recall_train:0.5763 F1_train:0.5317 AUC_train:0.4796\n",
      "acc_val:0.5000 pre_val:0.0000 recall_val:0.0000 F1_val:0.000000 AUC_val:0.3261\n",
      "Epoch:0004\n",
      "acc_train:0.5333 pre_train:0.5506 recall_train:0.5269 F1_train:0.5385 AUC_train:0.5419\n",
      "acc_val:0.5100 pre_val:0.6000 recall_val:0.0600 F1_val:0.109091 AUC_val:0.3948\n",
      "Epoch:0005\n",
      "acc_train:0.4900 pre_train:0.5060 recall_train:0.5398 F1_train:0.5224 AUC_train:0.4933\n",
      "acc_val:0.6100 pre_val:0.7895 recall_val:0.3000 F1_val:0.434783 AUC_val:0.6032\n",
      "Epoch:0006\n",
      "acc_train:0.5433 pre_train:0.5634 recall_train:0.5161 F1_train:0.5387 AUC_train:0.5477\n",
      "acc_val:0.6200 pre_val:0.7500 recall_val:0.3600 F1_val:0.486486 AUC_val:0.6432\n",
      "Epoch:0007\n",
      "acc_train:0.5467 pre_train:0.5993 recall_train:0.3699 F1_train:0.4574 AUC_train:0.5364\n",
      "acc_val:0.6400 pre_val:0.7333 recall_val:0.4400 F1_val:0.550000 AUC_val:0.6596\n",
      "Epoch:0008\n",
      "acc_train:0.5422 pre_train:0.5869 recall_train:0.3849 F1_train:0.4649 AUC_train:0.5615\n",
      "acc_val:0.6400 pre_val:0.7333 recall_val:0.4400 F1_val:0.550000 AUC_val:0.6664\n",
      "Epoch:0009\n",
      "acc_train:0.5767 pre_train:0.6296 recall_train:0.4387 F1_train:0.5171 AUC_train:0.5706\n",
      "acc_val:0.6500 pre_val:0.7419 recall_val:0.4600 F1_val:0.567901 AUC_val:0.6688\n",
      "Epoch:0010\n",
      "acc_train:0.5856 pre_train:0.6361 recall_train:0.4624 F1_train:0.5355 AUC_train:0.6074\n",
      "acc_val:0.6500 pre_val:0.7419 recall_val:0.4600 F1_val:0.567901 AUC_val:0.6728\n",
      "Epoch:0011\n",
      "acc_train:0.5889 pre_train:0.6316 recall_train:0.4903 F1_train:0.5521 AUC_train:0.5954\n",
      "acc_val:0.6500 pre_val:0.7419 recall_val:0.4600 F1_val:0.567901 AUC_val:0.6740\n",
      "Epoch:0012\n",
      "acc_train:0.5722 pre_train:0.5901 recall_train:0.5634 F1_train:0.5765 AUC_train:0.6015\n",
      "acc_val:0.6600 pre_val:0.7353 recall_val:0.5000 F1_val:0.595238 AUC_val:0.6760\n",
      "Epoch:0013\n",
      "acc_train:0.5856 pre_train:0.6041 recall_train:0.5742 F1_train:0.5888 AUC_train:0.6170\n",
      "acc_val:0.6500 pre_val:0.6923 recall_val:0.5400 F1_val:0.606742 AUC_val:0.6752\n",
      "Epoch:0014\n",
      "acc_train:0.5922 pre_train:0.6124 recall_train:0.5742 F1_train:0.5927 AUC_train:0.6170\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6756\n",
      "Epoch:0015\n",
      "acc_train:0.5811 pre_train:0.6063 recall_train:0.5398 F1_train:0.5711 AUC_train:0.5958\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6764\n",
      "Epoch:0016\n",
      "acc_train:0.5767 pre_train:0.5882 recall_train:0.6022 F1_train:0.5951 AUC_train:0.5887\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.6768\n",
      "Epoch:0017\n",
      "acc_train:0.6000 pre_train:0.6115 recall_train:0.6194 F1_train:0.6154 AUC_train:0.6218\n",
      "acc_val:0.6100 pre_val:0.6222 recall_val:0.5600 F1_val:0.589474 AUC_val:0.6772\n",
      "Epoch:0018\n",
      "acc_train:0.6133 pre_train:0.6303 recall_train:0.6086 F1_train:0.6193 AUC_train:0.6444\n",
      "acc_val:0.6100 pre_val:0.6222 recall_val:0.5600 F1_val:0.589474 AUC_val:0.6808\n",
      "Epoch:0019\n",
      "acc_train:0.6244 pre_train:0.6427 recall_train:0.6151 F1_train:0.6286 AUC_train:0.6473\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.6808\n",
      "Epoch:0020\n",
      "acc_train:0.6022 pre_train:0.6208 recall_train:0.5914 F1_train:0.6057 AUC_train:0.6233\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.6780\n",
      "Epoch:0021\n",
      "acc_train:0.5933 pre_train:0.5992 recall_train:0.6430 F1_train:0.6203 AUC_train:0.6282\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.6768\n",
      "Epoch:0022\n",
      "acc_train:0.6100 pre_train:0.6228 recall_train:0.6215 F1_train:0.6222 AUC_train:0.6374\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.6776\n",
      "Epoch:0023\n",
      "acc_train:0.5933 pre_train:0.6012 recall_train:0.6323 F1_train:0.6164 AUC_train:0.6310\n",
      "acc_val:0.6000 pre_val:0.6087 recall_val:0.5600 F1_val:0.583333 AUC_val:0.6784\n",
      "Epoch:0024\n",
      "acc_train:0.5944 pre_train:0.5947 recall_train:0.6753 F1_train:0.6324 AUC_train:0.6405\n",
      "acc_val:0.6000 pre_val:0.6087 recall_val:0.5600 F1_val:0.583333 AUC_val:0.6816\n",
      "Epoch:0025\n",
      "acc_train:0.6133 pre_train:0.6370 recall_train:0.5849 F1_train:0.6099 AUC_train:0.6420\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.6836\n",
      "Epoch:0026\n",
      "acc_train:0.6167 pre_train:0.6376 recall_train:0.5978 F1_train:0.6171 AUC_train:0.6491\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.6840\n",
      "Epoch:0027\n",
      "acc_train:0.6267 pre_train:0.6335 recall_train:0.6581 F1_train:0.6456 AUC_train:0.6532\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.6848\n",
      "Epoch:0028\n",
      "acc_train:0.6089 pre_train:0.6270 recall_train:0.6000 F1_train:0.6132 AUC_train:0.6426\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.6844\n",
      "Epoch:0029\n",
      "acc_train:0.6056 pre_train:0.6160 recall_train:0.6280 F1_train:0.6219 AUC_train:0.6541\n",
      "acc_val:0.6300 pre_val:0.6585 recall_val:0.5400 F1_val:0.593407 AUC_val:0.6840\n",
      "Epoch:0030\n",
      "acc_train:0.6178 pre_train:0.6237 recall_train:0.6559 F1_train:0.6394 AUC_train:0.6678\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.6876\n",
      "Epoch:0031\n",
      "acc_train:0.6222 pre_train:0.6283 recall_train:0.6581 F1_train:0.6429 AUC_train:0.6600\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.6884\n",
      "Epoch:0032\n",
      "acc_train:0.6011 pre_train:0.6162 recall_train:0.6043 F1_train:0.6102 AUC_train:0.6488\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.6888\n",
      "Epoch:0033\n",
      "acc_train:0.6289 pre_train:0.6427 recall_train:0.6344 F1_train:0.6385 AUC_train:0.6684\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.6864\n",
      "Epoch:0034\n",
      "acc_train:0.6067 pre_train:0.6065 recall_train:0.6796 F1_train:0.6410 AUC_train:0.6475\n",
      "acc_val:0.6200 pre_val:0.6429 recall_val:0.5400 F1_val:0.586957 AUC_val:0.6808\n",
      "Epoch:0035\n",
      "acc_train:0.6122 pre_train:0.6189 recall_train:0.6495 F1_train:0.6338 AUC_train:0.6645\n",
      "acc_val:0.6000 pre_val:0.6136 recall_val:0.5400 F1_val:0.574468 AUC_val:0.6788\n",
      "Epoch:0036\n",
      "acc_train:0.6267 pre_train:0.6314 recall_train:0.6667 F1_train:0.6485 AUC_train:0.6517\n",
      "acc_val:0.6000 pre_val:0.6136 recall_val:0.5400 F1_val:0.574468 AUC_val:0.6840\n",
      "Epoch:0037\n",
      "acc_train:0.6156 pre_train:0.6413 recall_train:0.5806 F1_train:0.6095 AUC_train:0.6455\n",
      "acc_val:0.6000 pre_val:0.6136 recall_val:0.5400 F1_val:0.574468 AUC_val:0.6952\n",
      "Epoch:0038\n",
      "acc_train:0.6133 pre_train:0.6191 recall_train:0.6538 F1_train:0.6360 AUC_train:0.6453\n",
      "acc_val:0.6000 pre_val:0.6136 recall_val:0.5400 F1_val:0.574468 AUC_val:0.7008\n",
      "Epoch:0039\n",
      "acc_train:0.6189 pre_train:0.6230 recall_train:0.6645 F1_train:0.6431 AUC_train:0.6692\n",
      "acc_val:0.6100 pre_val:0.6222 recall_val:0.5600 F1_val:0.589474 AUC_val:0.7124\n",
      "Epoch:0040\n",
      "acc_train:0.5978 pre_train:0.6071 recall_train:0.6280 F1_train:0.6173 AUC_train:0.6436\n",
      "acc_val:0.6200 pre_val:0.6364 recall_val:0.5600 F1_val:0.595745 AUC_val:0.7156\n",
      "Epoch:0041\n",
      "acc_train:0.6044 pre_train:0.6097 recall_train:0.6516 F1_train:0.6299 AUC_train:0.6516\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.7156\n",
      "Epoch:0042\n",
      "acc_train:0.6089 pre_train:0.6253 recall_train:0.6065 F1_train:0.6157 AUC_train:0.6610\n",
      "acc_val:0.6300 pre_val:0.6512 recall_val:0.5600 F1_val:0.602151 AUC_val:0.7168\n",
      "Epoch:0043\n",
      "acc_train:0.6033 pre_train:0.6184 recall_train:0.6065 F1_train:0.6124 AUC_train:0.6402\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.7168\n",
      "Epoch:0044\n",
      "acc_train:0.6111 pre_train:0.6162 recall_train:0.6559 F1_train:0.6354 AUC_train:0.6453\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.7076\n",
      "Epoch:0045\n",
      "acc_train:0.6356 pre_train:0.6442 recall_train:0.6581 F1_train:0.6511 AUC_train:0.6646\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.7052\n",
      "Epoch:0046\n",
      "acc_train:0.5867 pre_train:0.5943 recall_train:0.6301 F1_train:0.6117 AUC_train:0.6377\n",
      "acc_val:0.6600 pre_val:0.7105 recall_val:0.5400 F1_val:0.613636 AUC_val:0.7032\n",
      "Epoch:0047\n",
      "acc_train:0.5967 pre_train:0.6032 recall_train:0.6409 F1_train:0.6215 AUC_train:0.6551\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7044\n",
      "Epoch:0048\n",
      "acc_train:0.6378 pre_train:0.6527 recall_train:0.6387 F1_train:0.6457 AUC_train:0.6745\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7084\n",
      "Epoch:0049\n",
      "acc_train:0.6400 pre_train:0.6921 recall_train:0.5462 F1_train:0.6106 AUC_train:0.6685\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0050\n",
      "acc_train:0.6456 pre_train:0.6763 recall_train:0.6022 F1_train:0.6371 AUC_train:0.6831\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7112\n",
      "Epoch:0051\n",
      "acc_train:0.6067 pre_train:0.6294 recall_train:0.5806 F1_train:0.6040 AUC_train:0.6600\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7148\n",
      "Epoch:0052\n",
      "acc_train:0.6311 pre_train:0.6462 recall_train:0.6323 F1_train:0.6391 AUC_train:0.6839\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7156\n",
      "Epoch:0053\n",
      "acc_train:0.6022 pre_train:0.6146 recall_train:0.6172 F1_train:0.6159 AUC_train:0.6288\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7208\n",
      "Epoch:0054\n",
      "acc_train:0.6522 pre_train:0.6784 recall_train:0.6215 F1_train:0.6487 AUC_train:0.7016\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7236\n",
      "Epoch:0055\n",
      "acc_train:0.6500 pre_train:0.6812 recall_train:0.6065 F1_train:0.6416 AUC_train:0.7044\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7260\n",
      "Epoch:0056\n",
      "acc_train:0.6422 pre_train:0.6748 recall_train:0.5935 F1_train:0.6316 AUC_train:0.7059\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7292\n",
      "Epoch:0057\n",
      "acc_train:0.6444 pre_train:0.6730 recall_train:0.6065 F1_train:0.6380 AUC_train:0.7006\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7296\n",
      "Epoch:0058\n",
      "acc_train:0.6200 pre_train:0.6475 recall_train:0.5806 F1_train:0.6122 AUC_train:0.6809\n",
      "acc_val:0.6800 pre_val:0.7500 recall_val:0.5400 F1_val:0.627907 AUC_val:0.7264\n",
      "Epoch:0059\n",
      "acc_train:0.6544 pre_train:0.6652 recall_train:0.6667 F1_train:0.6660 AUC_train:0.7096\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7284\n",
      "Epoch:0060\n",
      "acc_train:0.6322 pre_train:0.6565 recall_train:0.6043 F1_train:0.6293 AUC_train:0.7095\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7296\n",
      "Epoch:0061\n",
      "acc_train:0.6411 pre_train:0.6707 recall_train:0.6000 F1_train:0.6334 AUC_train:0.7081\n",
      "acc_val:0.6700 pre_val:0.7297 recall_val:0.5400 F1_val:0.620690 AUC_val:0.7328\n",
      "Epoch:0062\n",
      "acc_train:0.6611 pre_train:0.6770 recall_train:0.6581 F1_train:0.6674 AUC_train:0.7178\n",
      "acc_val:0.6600 pre_val:0.7105 recall_val:0.5400 F1_val:0.613636 AUC_val:0.7352\n",
      "Epoch:0063\n",
      "acc_train:0.6333 pre_train:0.6386 recall_train:0.6688 F1_train:0.6534 AUC_train:0.7030\n",
      "acc_val:0.6600 pre_val:0.7105 recall_val:0.5400 F1_val:0.613636 AUC_val:0.7404\n",
      "Epoch:0064\n",
      "acc_train:0.6433 pre_train:0.6667 recall_train:0.6194 F1_train:0.6421 AUC_train:0.7239\n",
      "acc_val:0.6600 pre_val:0.7105 recall_val:0.5400 F1_val:0.613636 AUC_val:0.7428\n",
      "Epoch:0065\n",
      "acc_train:0.6644 pre_train:0.6807 recall_train:0.6602 F1_train:0.6703 AUC_train:0.7362\n",
      "acc_val:0.6500 pre_val:0.6923 recall_val:0.5400 F1_val:0.606742 AUC_val:0.7476\n",
      "Epoch:0066\n",
      "acc_train:0.6822 pre_train:0.7002 recall_train:0.6731 F1_train:0.6864 AUC_train:0.7631\n",
      "acc_val:0.6400 pre_val:0.6842 recall_val:0.5200 F1_val:0.590909 AUC_val:0.7492\n",
      "Epoch:0067\n",
      "acc_train:0.6733 pre_train:0.6763 recall_train:0.7054 F1_train:0.6905 AUC_train:0.7421\n",
      "acc_val:0.6600 pre_val:0.7000 recall_val:0.5600 F1_val:0.622222 AUC_val:0.7652\n",
      "Epoch:0068\n",
      "acc_train:0.6800 pre_train:0.6679 recall_train:0.7570 F1_train:0.7097 AUC_train:0.7356\n",
      "acc_val:0.6700 pre_val:0.7073 recall_val:0.5800 F1_val:0.637363 AUC_val:0.7820\n",
      "Epoch:0069\n",
      "acc_train:0.6967 pre_train:0.7043 recall_train:0.7118 F1_train:0.7080 AUC_train:0.7732\n",
      "acc_val:0.6900 pre_val:0.7436 recall_val:0.5800 F1_val:0.651685 AUC_val:0.7944\n",
      "Epoch:0070\n",
      "acc_train:0.7067 pre_train:0.7180 recall_train:0.7118 F1_train:0.7149 AUC_train:0.7868\n",
      "acc_val:0.7100 pre_val:0.7561 recall_val:0.6200 F1_val:0.681319 AUC_val:0.8072\n",
      "Epoch:0071\n",
      "acc_train:0.7511 pre_train:0.7464 recall_train:0.7849 F1_train:0.7652 AUC_train:0.7983\n",
      "acc_val:0.7600 pre_val:0.7955 recall_val:0.7000 F1_val:0.744681 AUC_val:0.8172\n",
      "Epoch:0072\n",
      "acc_train:0.7222 pre_train:0.7312 recall_train:0.7312 F1_train:0.7312 AUC_train:0.7954\n",
      "acc_val:0.7800 pre_val:0.7800 recall_val:0.7800 F1_val:0.780000 AUC_val:0.8260\n",
      "Epoch:0073\n",
      "acc_train:0.7656 pre_train:0.7520 recall_train:0.8151 F1_train:0.7822 AUC_train:0.8359\n",
      "acc_val:0.7900 pre_val:0.7843 recall_val:0.8000 F1_val:0.792079 AUC_val:0.8184\n",
      "Epoch:0074\n",
      "acc_train:0.7856 pre_train:0.7764 recall_train:0.8215 F1_train:0.7983 AUC_train:0.8373\n",
      "acc_val:0.7700 pre_val:0.7647 recall_val:0.7800 F1_val:0.772277 AUC_val:0.8096\n",
      "Epoch:0075\n",
      "acc_train:0.7800 pre_train:0.7562 recall_train:0.8473 F1_train:0.7992 AUC_train:0.8461\n",
      "acc_val:0.7600 pre_val:0.7500 recall_val:0.7800 F1_val:0.764706 AUC_val:0.8016\n",
      "Epoch:0076\n",
      "acc_train:0.7711 pre_train:0.7524 recall_train:0.8301 F1_train:0.7894 AUC_train:0.8355\n",
      "acc_val:0.7800 pre_val:0.7800 recall_val:0.7800 F1_val:0.780000 AUC_val:0.8096\n",
      "Epoch:0077\n",
      "acc_train:0.7933 pre_train:0.7579 recall_train:0.8817 F1_train:0.8151 AUC_train:0.8504\n",
      "acc_val:0.7800 pre_val:0.7692 recall_val:0.8000 F1_val:0.784314 AUC_val:0.8284\n",
      "Epoch:0078\n",
      "acc_train:0.8011 pre_train:0.7688 recall_train:0.8796 F1_train:0.8205 AUC_train:0.8606\n",
      "acc_val:0.7700 pre_val:0.7288 recall_val:0.8600 F1_val:0.788991 AUC_val:0.8436\n",
      "Epoch:0079\n",
      "acc_train:0.8456 pre_train:0.8052 recall_train:0.9247 F1_train:0.8609 AUC_train:0.8966\n",
      "acc_val:0.7500 pre_val:0.6984 recall_val:0.8800 F1_val:0.778761 AUC_val:0.8540\n",
      "Epoch:0080\n",
      "acc_train:0.8500 pre_train:0.8067 recall_train:0.9333 F1_train:0.8654 AUC_train:0.9151\n",
      "acc_val:0.8100 pre_val:0.7818 recall_val:0.8600 F1_val:0.819048 AUC_val:0.8600\n",
      "Epoch:0081\n",
      "acc_train:0.8678 pre_train:0.8327 recall_train:0.9312 F1_train:0.8792 AUC_train:0.9137\n",
      "acc_val:0.8200 pre_val:0.8200 recall_val:0.8200 F1_val:0.820000 AUC_val:0.8568\n",
      "Epoch:0082\n",
      "acc_train:0.8467 pre_train:0.7978 recall_train:0.9419 F1_train:0.8639 AUC_train:0.9159\n",
      "acc_val:0.8200 pre_val:0.8200 recall_val:0.8200 F1_val:0.820000 AUC_val:0.8556\n",
      "Epoch:0083\n",
      "acc_train:0.8322 pre_train:0.7886 recall_train:0.9226 F1_train:0.8503 AUC_train:0.8947\n",
      "acc_val:0.8300 pre_val:0.8235 recall_val:0.8400 F1_val:0.831683 AUC_val:0.8568\n",
      "Epoch:0084\n",
      "acc_train:0.8633 pre_train:0.8276 recall_train:0.9290 F1_train:0.8754 AUC_train:0.9149\n",
      "acc_val:0.8300 pre_val:0.8113 recall_val:0.8600 F1_val:0.834951 AUC_val:0.8680\n",
      "Epoch:0085\n",
      "acc_train:0.8522 pre_train:0.7954 recall_train:0.9613 F1_train:0.8705 AUC_train:0.9311\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.8748\n",
      "Epoch:0086\n",
      "acc_train:0.8767 pre_train:0.8242 recall_train:0.9677 F1_train:0.8902 AUC_train:0.9363\n",
      "acc_val:0.8200 pre_val:0.7667 recall_val:0.9200 F1_val:0.836364 AUC_val:0.8748\n",
      "Epoch:0087\n",
      "acc_train:0.8867 pre_train:0.8306 recall_train:0.9806 F1_train:0.8994 AUC_train:0.9563\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.8740\n",
      "Epoch:0088\n",
      "acc_train:0.8956 pre_train:0.8507 recall_train:0.9677 F1_train:0.9054 AUC_train:0.9460\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.8696\n",
      "Epoch:0089\n",
      "acc_train:0.8967 pre_train:0.8523 recall_train:0.9677 F1_train:0.9063 AUC_train:0.9325\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.8756\n",
      "Epoch:0090\n",
      "acc_train:0.8944 pre_train:0.8451 recall_train:0.9742 F1_train:0.9051 AUC_train:0.9464\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.8828\n",
      "Epoch:0091\n",
      "acc_train:0.9100 pre_train:0.8650 recall_train:0.9785 F1_train:0.9183 AUC_train:0.9643\n",
      "acc_val:0.8500 pre_val:0.8182 recall_val:0.9000 F1_val:0.857143 AUC_val:0.8796\n",
      "Epoch:0092\n",
      "acc_train:0.9222 pre_train:0.8776 recall_train:0.9871 F1_train:0.9291 AUC_train:0.9598\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.8772\n",
      "Epoch:0093\n",
      "acc_train:0.8989 pre_train:0.8515 recall_train:0.9742 F1_train:0.9087 AUC_train:0.9604\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.8788\n",
      "Epoch:0094\n",
      "acc_train:0.9167 pre_train:0.8693 recall_train:0.9871 F1_train:0.9245 AUC_train:0.9614\n",
      "acc_val:0.8300 pre_val:0.7797 recall_val:0.9200 F1_val:0.844037 AUC_val:0.8800\n",
      "Epoch:0095\n",
      "acc_train:0.9200 pre_train:0.8816 recall_train:0.9763 F1_train:0.9265 AUC_train:0.9559\n",
      "acc_val:0.8600 pre_val:0.8214 recall_val:0.9200 F1_val:0.867925 AUC_val:0.8924\n",
      "Epoch:0096\n",
      "acc_train:0.9189 pre_train:0.8755 recall_train:0.9828 F1_train:0.9260 AUC_train:0.9535\n",
      "acc_val:0.8600 pre_val:0.8333 recall_val:0.9000 F1_val:0.865385 AUC_val:0.8916\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0097\n",
      "acc_train:0.9178 pre_train:0.8752 recall_train:0.9806 F1_train:0.9249 AUC_train:0.9698\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.8884\n",
      "Epoch:0098\n",
      "acc_train:0.9333 pre_train:0.8947 recall_train:0.9871 F1_train:0.9387 AUC_train:0.9803\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.8844\n",
      "Epoch:0099\n",
      "acc_train:0.9267 pre_train:0.8874 recall_train:0.9828 F1_train:0.9327 AUC_train:0.9716\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.8872\n",
      "Epoch:0100\n",
      "acc_train:0.9378 pre_train:0.9066 recall_train:0.9806 F1_train:0.9421 AUC_train:0.9650\n",
      "acc_val:0.8400 pre_val:0.8036 recall_val:0.9000 F1_val:0.849057 AUC_val:0.8968\n",
      "Epoch:0101\n",
      "acc_train:0.9278 pre_train:0.8846 recall_train:0.9892 F1_train:0.9340 AUC_train:0.9717\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.8992\n",
      "Epoch:0102\n",
      "acc_train:0.9356 pre_train:0.8951 recall_train:0.9914 F1_train:0.9408 AUC_train:0.9772\n",
      "acc_val:0.8300 pre_val:0.8000 recall_val:0.8800 F1_val:0.838095 AUC_val:0.9000\n",
      "Epoch:0103\n",
      "acc_train:0.9322 pre_train:0.8976 recall_train:0.9806 F1_train:0.9373 AUC_train:0.9625\n",
      "acc_val:0.8100 pre_val:0.7541 recall_val:0.9200 F1_val:0.828829 AUC_val:0.9012\n",
      "Epoch:0104\n",
      "acc_train:0.9344 pre_train:0.9060 recall_train:0.9742 F1_train:0.9389 AUC_train:0.9812\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.8864\n",
      "Epoch:0105\n",
      "acc_train:0.9311 pre_train:0.9006 recall_train:0.9742 F1_train:0.9360 AUC_train:0.9809\n",
      "acc_val:0.7800 pre_val:0.7121 recall_val:0.9400 F1_val:0.810345 AUC_val:0.8892\n",
      "Epoch:0106\n",
      "acc_train:0.9422 pre_train:0.9122 recall_train:0.9828 F1_train:0.9462 AUC_train:0.9826\n",
      "acc_val:0.8300 pre_val:0.7895 recall_val:0.9000 F1_val:0.841121 AUC_val:0.9020\n",
      "Epoch:0107\n",
      "acc_train:0.9467 pre_train:0.9212 recall_train:0.9806 F1_train:0.9500 AUC_train:0.9858\n",
      "acc_val:0.8300 pre_val:0.8113 recall_val:0.8600 F1_val:0.834951 AUC_val:0.8968\n",
      "Epoch:0108\n",
      "acc_train:0.9489 pre_train:0.9302 recall_train:0.9742 F1_train:0.9517 AUC_train:0.9850\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.9028\n",
      "Epoch:0109\n",
      "acc_train:0.9400 pre_train:0.9135 recall_train:0.9763 F1_train:0.9439 AUC_train:0.9824\n",
      "acc_val:0.7800 pre_val:0.7258 recall_val:0.9000 F1_val:0.803571 AUC_val:0.9144\n",
      "Epoch:0110\n",
      "acc_train:0.9444 pre_train:0.9261 recall_train:0.9699 F1_train:0.9475 AUC_train:0.9857\n",
      "acc_val:0.7500 pre_val:0.6866 recall_val:0.9200 F1_val:0.786325 AUC_val:0.9128\n",
      "Epoch:0111\n",
      "acc_train:0.9478 pre_train:0.9248 recall_train:0.9785 F1_train:0.9509 AUC_train:0.9811\n",
      "acc_val:0.7500 pre_val:0.6812 recall_val:0.9400 F1_val:0.789916 AUC_val:0.9116\n",
      "Epoch:0112\n",
      "acc_train:0.9667 pre_train:0.9522 recall_train:0.9849 F1_train:0.9683 AUC_train:0.9874\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9144\n",
      "Epoch:0113\n",
      "acc_train:0.9544 pre_train:0.9223 recall_train:0.9957 F1_train:0.9576 AUC_train:0.9894\n",
      "acc_val:0.7800 pre_val:0.7258 recall_val:0.9000 F1_val:0.803571 AUC_val:0.9040\n",
      "Epoch:0114\n",
      "acc_train:0.9556 pre_train:0.9328 recall_train:0.9849 F1_train:0.9582 AUC_train:0.9836\n",
      "acc_val:0.7700 pre_val:0.7143 recall_val:0.9000 F1_val:0.796460 AUC_val:0.9068\n",
      "Epoch:0115\n",
      "acc_train:0.9533 pre_train:0.9343 recall_train:0.9785 F1_train:0.9559 AUC_train:0.9850\n",
      "acc_val:0.7600 pre_val:0.6970 recall_val:0.9200 F1_val:0.793103 AUC_val:0.9124\n",
      "Epoch:0116\n",
      "acc_train:0.9622 pre_train:0.9371 recall_train:0.9935 F1_train:0.9645 AUC_train:0.9898\n",
      "acc_val:0.7400 pre_val:0.6765 recall_val:0.9200 F1_val:0.779661 AUC_val:0.9124\n",
      "Epoch:0117\n",
      "acc_train:0.9600 pre_train:0.9405 recall_train:0.9849 F1_train:0.9622 AUC_train:0.9898\n",
      "acc_val:0.7600 pre_val:0.6970 recall_val:0.9200 F1_val:0.793103 AUC_val:0.9080\n",
      "Epoch:0118\n",
      "acc_train:0.9667 pre_train:0.9579 recall_train:0.9785 F1_train:0.9681 AUC_train:0.9890\n",
      "acc_val:0.8100 pre_val:0.7627 recall_val:0.9000 F1_val:0.825688 AUC_val:0.9068\n",
      "Epoch:0119\n",
      "acc_train:0.9656 pre_train:0.9483 recall_train:0.9871 F1_train:0.9673 AUC_train:0.9916\n",
      "acc_val:0.8200 pre_val:0.7759 recall_val:0.9000 F1_val:0.833333 AUC_val:0.9044\n",
      "Epoch:0120\n",
      "acc_train:0.9578 pre_train:0.9348 recall_train:0.9871 F1_train:0.9603 AUC_train:0.9850\n",
      "acc_val:0.8200 pre_val:0.7857 recall_val:0.8800 F1_val:0.830189 AUC_val:0.9040\n",
      "Epoch:0121\n",
      "acc_train:0.9578 pre_train:0.9331 recall_train:0.9892 F1_train:0.9603 AUC_train:0.9863\n",
      "acc_val:0.7900 pre_val:0.7377 recall_val:0.9000 F1_val:0.810811 AUC_val:0.9052\n",
      "Epoch:0122\n",
      "acc_train:0.9533 pre_train:0.9308 recall_train:0.9828 F1_train:0.9561 AUC_train:0.9838\n",
      "acc_val:0.7400 pre_val:0.6714 recall_val:0.9400 F1_val:0.783333 AUC_val:0.9056\n",
      "Epoch:0123\n",
      "acc_train:0.9711 pre_train:0.9545 recall_train:0.9914 F1_train:0.9726 AUC_train:0.9872\n",
      "acc_val:0.7200 pre_val:0.6486 recall_val:0.9600 F1_val:0.774194 AUC_val:0.9060\n",
      "Epoch:0124\n",
      "acc_train:0.9667 pre_train:0.9503 recall_train:0.9871 F1_train:0.9684 AUC_train:0.9918\n",
      "acc_val:0.7200 pre_val:0.6486 recall_val:0.9600 F1_val:0.774194 AUC_val:0.9052\n",
      "Epoch:0125\n",
      "acc_train:0.9711 pre_train:0.9507 recall_train:0.9957 F1_train:0.9727 AUC_train:0.9932\n",
      "acc_val:0.7400 pre_val:0.6818 recall_val:0.9000 F1_val:0.775862 AUC_val:0.9034\n",
      "Epoch:0126\n",
      "acc_train:0.9756 pre_train:0.9605 recall_train:0.9935 F1_train:0.9767 AUC_train:0.9921\n",
      "acc_val:0.7800 pre_val:0.7258 recall_val:0.9000 F1_val:0.803571 AUC_val:0.8988\n",
      "Epoch:0127\n",
      "acc_train:0.9678 pre_train:0.9467 recall_train:0.9935 F1_train:0.9696 AUC_train:0.9884\n",
      "acc_val:0.7700 pre_val:0.7213 recall_val:0.8800 F1_val:0.792793 AUC_val:0.8952\n",
      "Epoch:0128\n",
      "acc_train:0.9711 pre_train:0.9507 recall_train:0.9957 F1_train:0.9727 AUC_train:0.9924\n",
      "acc_val:0.7500 pre_val:0.6984 recall_val:0.8800 F1_val:0.778761 AUC_val:0.8964\n",
      "Epoch:0129\n",
      "acc_train:0.9689 pre_train:0.9487 recall_train:0.9935 F1_train:0.9706 AUC_train:0.9928\n",
      "acc_val:0.7200 pre_val:0.6571 recall_val:0.9200 F1_val:0.766667 AUC_val:0.8954\n",
      "Epoch:0130\n",
      "acc_train:0.9744 pre_train:0.9604 recall_train:0.9914 F1_train:0.9757 AUC_train:0.9922\n",
      "acc_val:0.7300 pre_val:0.6575 recall_val:0.9600 F1_val:0.780488 AUC_val:0.8910\n",
      "Epoch:0131\n",
      "acc_train:0.9700 pre_train:0.9525 recall_train:0.9914 F1_train:0.9715 AUC_train:0.9921\n",
      "acc_val:0.7200 pre_val:0.6528 recall_val:0.9400 F1_val:0.770492 AUC_val:0.8982\n",
      "Epoch:0132\n",
      "acc_train:0.9767 pre_train:0.9664 recall_train:0.9892 F1_train:0.9777 AUC_train:0.9963\n",
      "acc_val:0.7700 pre_val:0.7015 recall_val:0.9400 F1_val:0.803419 AUC_val:0.9012\n",
      "Epoch:0133\n",
      "acc_train:0.9678 pre_train:0.9486 recall_train:0.9914 F1_train:0.9695 AUC_train:0.9901\n",
      "acc_val:0.7800 pre_val:0.7258 recall_val:0.9000 F1_val:0.803571 AUC_val:0.9020\n",
      "Epoch:0134\n",
      "acc_train:0.9656 pre_train:0.9559 recall_train:0.9785 F1_train:0.9671 AUC_train:0.9890\n",
      "acc_val:0.7800 pre_val:0.7188 recall_val:0.9200 F1_val:0.807018 AUC_val:0.9004\n",
      "Epoch:0135\n",
      "acc_train:0.9689 pre_train:0.9468 recall_train:0.9957 F1_train:0.9706 AUC_train:0.9958\n",
      "acc_val:0.7500 pre_val:0.6923 recall_val:0.9000 F1_val:0.782609 AUC_val:0.9016\n",
      "Epoch:0136\n",
      "acc_train:0.9733 pre_train:0.9584 recall_train:0.9914 F1_train:0.9746 AUC_train:0.9919\n",
      "acc_val:0.7300 pre_val:0.6667 recall_val:0.9200 F1_val:0.773109 AUC_val:0.9012\n",
      "Epoch:0137\n",
      "acc_train:0.9744 pre_train:0.9682 recall_train:0.9828 F1_train:0.9755 AUC_train:0.9925\n",
      "acc_val:0.7300 pre_val:0.6667 recall_val:0.9200 F1_val:0.773109 AUC_val:0.8982\n",
      "Epoch:0138\n",
      "acc_train:0.9711 pre_train:0.9641 recall_train:0.9806 F1_train:0.9723 AUC_train:0.9917\n",
      "acc_val:0.7300 pre_val:0.6667 recall_val:0.9200 F1_val:0.773109 AUC_val:0.8946\n",
      "Epoch:0139\n",
      "acc_train:0.9833 pre_train:0.9747 recall_train:0.9935 F1_train:0.9840 AUC_train:0.9904\n",
      "acc_val:0.7100 pre_val:0.6522 recall_val:0.9000 F1_val:0.756303 AUC_val:0.8972\n",
      "Early Stopping!!! epoch：138\n",
      " Starting the 1-7 Fold:：\n",
      "Fitting estimator with 131503 features.\n",
      "Fitting estimator with 121503 features.\n",
      "Fitting estimator with 111503 features.\n",
      "Fitting estimator with 101503 features.\n",
      "Fitting estimator with 91503 features.\n",
      "Fitting estimator with 81503 features.\n",
      "Fitting estimator with 71503 features.\n",
      "Fitting estimator with 61503 features.\n",
      "Fitting estimator with 51503 features.\n",
      "Fitting estimator with 41503 features.\n",
      "Fitting estimator with 31503 features.\n",
      "Fitting estimator with 21503 features.\n",
      "Fitting estimator with 11503 features.\n",
      "Number of labeled samples 900\n",
      "Number of features selected 2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:0001\n",
      "acc_train:0.4967 pre_train:0.5121 recall_train:0.5462 F1_train:0.5286 AUC_train:0.4980\n",
      "acc_val:0.5100 pre_val:1.0000 recall_val:0.0200 F1_val:0.039216 AUC_val:0.5488\n",
      "Epoch:0002\n",
      "acc_train:0.5489 pre_train:0.5707 recall_train:0.5118 F1_train:0.5397 AUC_train:0.5618\n",
      "acc_val:0.5700 pre_val:0.7059 recall_val:0.2400 F1_val:0.358209 AUC_val:0.6396\n",
      "Epoch:0003\n",
      "acc_train:0.5622 pre_train:0.5913 recall_train:0.4946 F1_train:0.5386 AUC_train:0.5898\n",
      "acc_val:0.6500 pre_val:0.7027 recall_val:0.5200 F1_val:0.597701 AUC_val:0.7048\n",
      "Epoch:0004\n",
      "acc_train:0.5733 pre_train:0.5935 recall_train:0.5527 F1_train:0.5724 AUC_train:0.5995\n",
      "acc_val:0.6500 pre_val:0.6667 recall_val:0.6000 F1_val:0.631579 AUC_val:0.7064\n",
      "Epoch:0005\n",
      "acc_train:0.6044 pre_train:0.6394 recall_train:0.5376 F1_train:0.5841 AUC_train:0.6389\n",
      "acc_val:0.6500 pre_val:0.6596 recall_val:0.6200 F1_val:0.639175 AUC_val:0.7236\n",
      "Epoch:0006\n",
      "acc_train:0.6189 pre_train:0.6533 recall_train:0.5591 F1_train:0.6025 AUC_train:0.6436\n",
      "acc_val:0.6700 pre_val:0.6667 recall_val:0.6800 F1_val:0.673267 AUC_val:0.7276\n",
      "Epoch:0007\n",
      "acc_train:0.6022 pre_train:0.6302 recall_train:0.5570 F1_train:0.5913 AUC_train:0.6183\n",
      "acc_val:0.6000 pre_val:0.6042 recall_val:0.5800 F1_val:0.591837 AUC_val:0.6888\n",
      "Epoch:0008\n",
      "acc_train:0.6267 pre_train:0.6562 recall_train:0.5828 F1_train:0.6173 AUC_train:0.6417\n",
      "acc_val:0.6200 pre_val:0.6304 recall_val:0.5800 F1_val:0.604167 AUC_val:0.6896\n",
      "Epoch:0009\n",
      "acc_train:0.6133 pre_train:0.6519 recall_train:0.5398 F1_train:0.5906 AUC_train:0.6344\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.6836\n",
      "Epoch:0010\n",
      "acc_train:0.6211 pre_train:0.6590 recall_train:0.5527 F1_train:0.6012 AUC_train:0.6439\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.6848\n",
      "Epoch:0011\n",
      "acc_train:0.6200 pre_train:0.6468 recall_train:0.5828 F1_train:0.6131 AUC_train:0.6485\n",
      "acc_val:0.6400 pre_val:0.6667 recall_val:0.5600 F1_val:0.608696 AUC_val:0.6880\n",
      "Epoch:0012\n",
      "acc_train:0.6367 pre_train:0.6691 recall_train:0.5871 F1_train:0.6254 AUC_train:0.6788\n",
      "acc_val:0.6500 pre_val:0.6829 recall_val:0.5600 F1_val:0.615385 AUC_val:0.6900\n",
      "Epoch:0013\n",
      "acc_train:0.6078 pre_train:0.6327 recall_train:0.5742 F1_train:0.6020 AUC_train:0.6409\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.6876\n",
      "Epoch:0014\n",
      "acc_train:0.6167 pre_train:0.6271 recall_train:0.6366 F1_train:0.6318 AUC_train:0.6471\n",
      "acc_val:0.6400 pre_val:0.6750 recall_val:0.5400 F1_val:0.600000 AUC_val:0.6776\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "import io\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "# from dataloader import dataloader\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import auc\n",
    "\n",
    "if hasattr(sys.stdout, 'buffer'):\n",
    "    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8')\n",
    "\n",
    "\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.no_cuda = False\n",
    "        self.seed = 46\n",
    "        self.epochs = 200\n",
    "        self.lr = 0.001\n",
    "        self.weight_decay = 5e-5\n",
    "        self.hidden = 32\n",
    "        self.dropout = 0.2\n",
    "        self.atlas = 'ez'\n",
    "        self.num_features = 2000\n",
    "        self.folds = 10\n",
    "        self.connectivity = 'correlation'\n",
    "        self.max_degree = 3\n",
    "        self.ngl = 8\n",
    "        self.edropout = 0.3\n",
    "        self.train = 1\n",
    "        self.ckpt_path = '../folds/rois_seven_32_pth'\n",
    "        self.early_stopping = True\n",
    "        self.early_stopping_patience = 20\n",
    "\n",
    "# Instantiate Args class\n",
    "args = Args()\n",
    "\n",
    "# Check if CUDA is available\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "# Set random seeds\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "if args.cuda:\n",
    "    torch.cuda.manual_seed(args.seed)\n",
    "\n",
    "# Create params dictionary\n",
    "params = vars(args)\n",
    "\n",
    "# Print Hyperparameters\n",
    "print('Hyperparameters:')\n",
    "for key, value in params.items():\n",
    "    print(key + \":\", value)\n",
    "\n",
    "corrects = np.zeros(args.folds, dtype=np.int32) \n",
    "accs = np.zeros(args.folds, dtype=np.float32) \n",
    "aucs = np.zeros(args.folds, dtype=np.float32)\n",
    "prfs = np.zeros([args.folds,3], dtype=np.float32) # Save Precision, Recall, F1\n",
    "test_num = np.zeros(args.folds, dtype=np.float32)\n",
    "\n",
    "\n",
    "print('  Loading dataset ...')\n",
    "dataloader = dataloader()\n",
    "raw_features, y, nonimg = dataloader.load_data(params) \n",
    "cv_splits = dataloader.data_split(args.folds)\n",
    "features=raw_features\n",
    "\n",
    "t1 = time.time()\n",
    "count=1;\n",
    "for i in range(args.folds):\n",
    "    \n",
    "    \n",
    "    \n",
    "    t_start = time.time()\n",
    "    train_ind, test_ind = cv_splits[i]\n",
    "\n",
    "    train_ind, valid_ind = train_test_split(train_ind, test_size=0.1, random_state = 24)\n",
    "    \n",
    "    cv_splits[i] = (train_ind, valid_ind)\n",
    "    cv_splits[i] = cv_splits[i] + (test_ind,)\n",
    "    print('Size of the {}-fold Training, Validation, and Test Sets:{},{},{}' .format(i+1, len(cv_splits[i][0]), len(cv_splits[i][1]), len(cv_splits[i][2])))\n",
    "\n",
    "    if args.train == 1:\n",
    "        for j in range(args.folds):\n",
    "            print(' Starting the {}-{} Fold:：'.format(i+1,j+1))\n",
    "            node_ftr = dataloader.get_node_features(train_ind)\n",
    "            edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "            edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "            \n",
    "            model = GCN(input_dim = args.num_features,\n",
    "                        nhid = args.hidden, \n",
    "                        num_classes = 2, \n",
    "                        ngl = args.ngl, \n",
    "                        dropout = args.dropout, \n",
    "                        edge_dropout = args.edropout, \n",
    "                        edgenet_input_dim = 2*nonimg.shape[1])\n",
    "            optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "            \n",
    "#             if args.cuda:\n",
    "            model\n",
    "            features = torch.tensor(node_ftr, dtype=torch.float32)\n",
    "            edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "            edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "            labels = torch.tensor(y, dtype=torch.long)\n",
    "            fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "                \n",
    "            acc = 0\n",
    "            best_val_loss = float('inf') # early stoppping: Initialized to positive infinity\n",
    "            current_patience = 0 # early stopping: Used to record the epochs of the current early stopping\n",
    "            \n",
    "            epoch_store = []\n",
    "            acc_train_store =[]        \n",
    "            pre_train_store =[]\n",
    "            recall_train_store =[]\n",
    "            F1_train_store =[]\n",
    "            AUC_train_store =[]\n",
    "            acc_val_store=[]\n",
    "            pre_val_store=[]\n",
    "            recall_val_store=[]\n",
    "            F1_val_store=[]\n",
    "            AUC_val_store=[]\n",
    "            \n",
    "            for epoch in range(args.epochs):\n",
    "                # train\n",
    "                model.train()\n",
    "                with torch.set_grad_enabled(True):\n",
    "                    optimizer.zero_grad()\n",
    "                    output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "                    loss_train = torch.nn.CrossEntropyLoss()(output[train_ind], labels[train_ind])\n",
    "                    loss_train.backward()\n",
    "                    optimizer.step()\n",
    "                acc_train = torchmetrics_accuracy(output[train_ind], labels[train_ind])\n",
    "                auc_train = torchmetrics_auc(output[train_ind], labels[train_ind])\n",
    "                logits_train = output[train_ind].detach().cpu().numpy()\n",
    "                prf_train = prf(logits_train, y[train_ind])\n",
    "\n",
    "                \n",
    "                # valid\n",
    "                model.eval()\n",
    "                with torch.set_grad_enabled(False):\n",
    "                    output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "                loss_val = torch.nn.CrossEntropyLoss()(output[valid_ind], labels[valid_ind])\n",
    "                acc_val = torchmetrics_accuracy(output[valid_ind], labels[valid_ind])\n",
    "                auc_val = torchmetrics_auc(output[valid_ind], labels[valid_ind])\n",
    "                logits_val = output[valid_ind].detach().cpu().numpy()\n",
    "                prf_val = prf(logits_val, y[valid_ind])\n",
    "\n",
    "                \n",
    "                print('Epoch:{:04d}'.format(epoch+1))\n",
    "                print('acc_train:{:.4f}'.format(acc_train),\n",
    "                      'pre_train:{:.4f}'.format(prf_train[0]),\n",
    "                      'recall_train:{:.4f}'.format(prf_train[1]),\n",
    "                      'F1_train:{:.4f}'.format(prf_train[2]),\n",
    "                      'AUC_train:{:.4f}'.format(auc_train))\n",
    "                print('acc_val:{:.4f}'.format(acc_val),\n",
    "                      'pre_val:{:.4f}'.format(prf_val[0]),\n",
    "                      'recall_val:{:.4f}'.format(prf_val[1]),\n",
    "                      'F1_val:{:4f}'.format(prf_val[2]),\n",
    "                      'AUC_val:{:.4f}'.format(auc_val))\n",
    "                \n",
    "                epoch_store.append(epoch+1)\n",
    "                acc_train_store.append(acc_train)       \n",
    "                pre_train_store.append(prf_train[0])\n",
    "                recall_train_store.append(prf_train[1])\n",
    "                F1_train_store.append(prf_train[2])\n",
    "                AUC_train_store.append(auc_train)\n",
    "                acc_val_store.append(acc_val)\n",
    "                pre_val_store.append(prf_val[0])\n",
    "                recall_val_store.append(prf_val[1])\n",
    "                F1_val_store.append(prf_val[2])\n",
    "                AUC_val_store.append(auc_val)\n",
    "                \n",
    "                # save pth\n",
    "                if acc_val > acc and epoch > 50:\n",
    "                    acc = acc_val\n",
    "                    if args.ckpt_path != '':\n",
    "                        if not os.path.exists(args.ckpt_path):\n",
    "                            os.makedirs(args.ckpt_path)\n",
    "                        torch.save(model.state_dict(), fold_model_path)\n",
    "                \n",
    "                # Early Stopping\n",
    "                if epoch > 50 and args.early_stopping == True:\n",
    "                    if loss_val < best_val_loss:\n",
    "                        best_val_loss = loss_val\n",
    "                        current_patience = 0\n",
    "                    else:\n",
    "                        current_patience += 1\n",
    "                    if current_patience >= args.early_stopping_patience:\n",
    "                        print('Early Stopping!!! epoch：{}'.format(epoch))\n",
    "                        break\n",
    "        print(\"===================================================================\",i,\"_\",j)\n",
    "        data  = { \n",
    "              \"epoch\" : epoch_store ,\n",
    "              \"acc_train\" : acc_train_store ,        \n",
    "              \"pre_train\" : pre_train_store ,\n",
    "              \"recall_train\" : recall_train_store ,\n",
    "              \"F1_train\" : F1_train_store ,\n",
    "              \"AUC_train\" : AUC_train_store ,\n",
    "              \"acc_val\" : acc_val_store,\n",
    "               \"pre_val\" : pre_val_store ,\n",
    "              \"recall_val\" : recall_val_store ,\n",
    "              \"F1_val\" : F1_val_store ,\n",
    "              \"AUC_val\" : AUC_val_store  \n",
    "        }\n",
    "        \n",
    "        \n",
    "        epoch_file_path =  f'../files/rois_seven_32/file_{i}_{j}_{count}.csv'\n",
    "        data_file = pd.DataFrame(data);\n",
    "        data_file.to_csv(epoch_file_path , index=False);\n",
    "        count=count+1;\n",
    "        # test\n",
    "        print(\"Loading the Model for the {}-th Fold:... ...\".format(i+1),\n",
    "              \"Size of samples in the test set:{}\".format(len(test_ind)))\n",
    "#         model.load_state_dict(torch.load(fold_model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "        acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "        auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "        logits_test = output[test_ind].detach().cpu().numpy()\n",
    "        correct_test = correct_num(logits_test, y[test_ind])\n",
    "        prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "        t_end = time.time()\n",
    "        t = t_end - t_start\n",
    "        print('Fold {} Results:'.format(i+1),\n",
    "              'test acc:{:.4f}'.format(acc_test),\n",
    "              'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "              'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "              'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "              'test_AUC:{:.4f}'.format(auc_test),\n",
    "              'time:{:.3f}s'.format(t))\n",
    "        \n",
    "        correct = correct_test\n",
    "        aucs[i] = auc_test\n",
    "        prfs[i] = prf_test\n",
    "        corrects[i] = correct\n",
    "        test_num[i] = len(test_ind)\n",
    "    \n",
    "    \n",
    "    if args.train == 0:\n",
    "        node_ftr = dataloader.get_node_features(train_ind)\n",
    "        edge_index, edgenet_input = dataloader.get_WL_inputs(nonimg)\n",
    "        edgenet_input = (edgenet_input - edgenet_input.mean(axis=0)) / edgenet_input.std(axis=0)\n",
    "        \n",
    "        model = GCN(input_dim = args.num_features,\n",
    "                    nhid = args.hidden, \n",
    "                    num_classes = 2, \n",
    "                    ngl = args.ngl, \n",
    "                    dropout = args.dropout, \n",
    "                    edge_dropout = args.edropout, \n",
    "                    edgenet_input_dim = 2*nonimg.shape[1])\n",
    "        optimizer = optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "        \n",
    "#         if args.cuda\n",
    "        model\n",
    "        features = torch.tensor(node_ftr, dtype=torch.float)\n",
    "        edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
    "        edgenet_input = torch.tensor(edgenet_input, dtype=torch.float32)\n",
    "        labels = torch.tensor(y, dtype=torch.long)\n",
    "        fold_model_path = args.ckpt_path + \"/fold{}.pth\".format(i+1)\n",
    "        \n",
    "        model.load_state_dict(torch.load(fold_model_path))\n",
    "        model.eval()\n",
    "        \n",
    "        with torch.set_grad_enabled(False):\n",
    "            output, edge_weights = model(features, edge_index, edgenet_input)\n",
    "        acc_test = torchmetrics_accuracy(output[test_ind], labels[test_ind])\n",
    "        auc_test = torchmetrics_auc(output[test_ind], labels[test_ind])\n",
    "        logits_test = output[test_ind].detach().cpu().numpy()\n",
    "        correct_test = correct_num(logits_test, y[test_ind])\n",
    "        prf_test =  prf(logits_test, y[test_ind])\n",
    "        \n",
    "        t_end = time.time()\n",
    "        t = t_end - t_start\n",
    "        print('Fold {} Results:'.format(i+1),\n",
    "              'test acc:{:.4f}'.format(acc_test),\n",
    "              'test_pre:{:.4f}'.format(prf_test[0]),\n",
    "              'test_recall:{:.4f}'.format(prf_test[1]),\n",
    "              'test_F1:{:.4f}'.format(prf_test[2]),\n",
    "              'test_AUC:{:.4f}'.format(auc_test),\n",
    "              'time:{:.3f}s'.format(t))\n",
    "        \n",
    "        correct = correct_test\n",
    "        aucs[i] = auc_test\n",
    "        prfs[i] = prf_test\n",
    "        corrects[i] = correct\n",
    "        test_num[i] = len(test_ind)\n",
    "\n",
    "t2 = time.time()\n",
    "\n",
    "print('\\r\\n======Finish Results for Nested 10-fold cross-validation======')\n",
    "Nested10kCV_acc = np.sum(corrects) / np.sum(test_num)\n",
    "Nested10kCV_auc = np.mean(aucs)\n",
    "Nested10kCV_precision, Nested10kCV_recall, Nested10kCV_F1 = np.mean(prfs, axis=0)\n",
    "print('Test:',\n",
    "      'acc:{}'.format(Nested10kCV_acc),\n",
    "      'precision:{}'.format(Nested10kCV_precision),\n",
    "      'recall:{}'.format(Nested10kCV_recall),\n",
    "      'F1:{}'.format(Nested10kCV_F1),\n",
    "      'AUC:{}'.format(Nested10kCV_auc))\n",
    "print('Total duration:{}'.format(t2 - t1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d845c726",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03838a8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
